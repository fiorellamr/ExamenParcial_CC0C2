{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTnWIB4JMNkh"
      },
      "source": [
        "Nombres y apellidos: Fiorella Meza Rodriguez <br>\n",
        "Código: 20192730G"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHnMc8PYxtA5"
      },
      "source": [
        "# Parte 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6qTZKVNM-k3"
      },
      "source": [
        "## Ejercicio 0.1.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1g7hBPURCO6"
      },
      "source": [
        "Dadas tres oraciones \"all models are wrong\", a model is wrong, y some models are useful y un vocabulario [\"<s>\", \"</s>\", 'a', 'all', 'are', 'model', 'models',\n",
        "               'some', 'useful', 'wrong']. En codigo responda las siguientes preguntas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yLShdilRWa0"
      },
      "source": [
        "### a.Calcule las probabilidades de todos los bigramas sin suavizado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aflN8zojW4gx"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "\n",
        "corpus = [ \"all models are wrong\", \"a model is wrong\",\n",
        "            \"some models are useful\"]\n",
        "\n",
        "vocabulario = ['<s>', '</s>', 'a', 'all', 'are', 'model', 'models',\n",
        "               'some', 'useful', 'wrong']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6s2Tth3W_B-",
        "outputId": "0f37bebd-089f-43ff-edca-47cf74fc2437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probabilidades de bigramas sin suavizado:\n",
            "P('models' | 'all') = 1.000\n",
            "P('are' | 'models') = 1.000\n",
            "P('wrong' | 'are') = 0.500\n",
            "P('model' | 'a') = 1.000\n",
            "P('is' | 'model') = 1.000\n",
            "P('wrong' | 'is') = 1.000\n",
            "P('models' | 'some') = 1.000\n",
            "P('useful' | 'are') = 0.500\n"
          ]
        }
      ],
      "source": [
        "# Inicializamos los conteos\n",
        "bigram_counts = defaultdict(int)\n",
        "unigram_counts = defaultdict(int)\n",
        "\n",
        "# Poblamos los conteos\n",
        "for sentence in corpus:\n",
        "    words = sentence.split()\n",
        "    for i in range(len(words) - 1):\n",
        "        bigram_counts[(words[i], words[i+1])] += 1\n",
        "        unigram_counts[words[i]] += 1\n",
        "    unigram_counts[words[-1]] += 1\n",
        "\n",
        "\n",
        "## Probabilidad de los bigramas\n",
        "def bigram_probability(w1, w2):\n",
        "    return bigram_counts[(w1, w2)] / unigram_counts[w1] if unigram_counts[w1] > 0 else 0\n",
        "\n",
        "\n",
        "print(\"Probabilidades de bigramas sin suavizado:\")\n",
        "for i in bigram_counts:\n",
        "  prob = bigram_probability(i[0],i[1])\n",
        "  print(f\"P('{i[1]}' | '{i[0]}') = {prob:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deJJ9B5JSEVF"
      },
      "source": [
        "### b. Calcule todas las probabilidades de todos los bigramas y el brigrama no visto 'a models' con suavizado add-one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQGzul5hm794"
      },
      "source": [
        "Referencia: https://github.com/kapumota/Actividades-CC0C2/blob/3d0cde28776954bfc6367f2b94725dffcfaf8f38/Cuadernos-CC0C2/Clase2/Counts-backoff-suavizado.ipynb#L113"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dOqHI6jPO2z2"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "bigram_counts = defaultdict(int)\n",
        "unigram_counts = defaultdict(int)\n",
        "\n",
        "corpus = [\"all models are wrong\", \"a model is wrong\",\n",
        "            \"some models are useful\"]\n",
        "\n",
        "# Poblamos los conteos\n",
        "for sentence in corpus:\n",
        "    words = sentence.split()\n",
        "    for i in range(len(words) - 1):\n",
        "        bigram_counts[(words[i], words[i+1])] += 1\n",
        "        unigram_counts[words[i]] += 1\n",
        "    unigram_counts[words[-1]] += 1\n",
        "\n",
        "\n",
        "beta = 1\n",
        "vocab_size = len(vocabulario)\n",
        "\n",
        "def laplace_smoothing_bigram_probability(w1, w2, beta, vocab_size):\n",
        "    bigram_prob = (bigram_counts[(w1, w2)] + beta) / (unigram_counts[w1] + beta * vocab_size)\n",
        "    return bigram_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63rBn8FBYeiX"
      },
      "source": [
        "Ahora calcularemos de todos los bigramas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChdckeNQYg_N",
        "outputId": "d1cb0601-d72c-4bda-a23d-57eba618d447"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La probabilidad de los bigramas con suavizado de laplace:\n",
            "P('models' | 'all') = 0.1818\n",
            "P('are' | 'models') = 0.2500\n",
            "P('wrong' | 'are') = 0.1667\n",
            "P('model' | 'a') = 0.1818\n",
            "P('is' | 'model') = 0.1818\n",
            "P('wrong' | 'is') = 0.1818\n",
            "P('models' | 'some') = 0.1818\n",
            "P('useful' | 'are') = 0.1667\n"
          ]
        }
      ],
      "source": [
        "print(f\"La probabilidad de los bigramas con suavizado de laplace:\")\n",
        "for bigram in bigram_counts:\n",
        "  prob = laplace_smoothing_bigram_probability(bigram[0],bigram[1], beta, vocab_size)\n",
        "  print(f\"P('{bigram[1]}' | '{bigram[0]}') = {prob:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bH-hirVfy9Rr",
        "outputId": "24ee2aba-6022-46d6-dfe3-38779f2b123a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La probabilidad del bigrama 'a models' con suavizado de Laplace es:\n",
            "P('models' | 'a') = 0.0909\n"
          ]
        }
      ],
      "source": [
        "prob = laplace_smoothing_bigram_probability(\"a\", \"models\", beta, vocab_size)\n",
        "print(f\"La probabilidad del bigrama 'a models' con suavizado de Laplace es:\")\n",
        "print(f\"P('models' | 'a') = {prob:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WotfA-QRTSmk"
      },
      "source": [
        "### C. Calcule la probabilidad de todos los bigramas y el bigrama no visto \"a models\" con suavizado add-k. Pruebe con k=00.5 y k=0.15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGnBsvLjnD2t"
      },
      "source": [
        "Referencia: https://github.com/kapumota/Actividades-CC0C2/blob/3d0cde28776954bfc6367f2b94725dffcfaf8f38/Cuadernos-CC0C2/Clase2/Modelos-lenguaje2.ipynb#L12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8h_c06APThMr",
        "outputId": "7fede1c9-66f9-4d3a-c519-6f2eb5e0d4f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Probabilidades de bigramas suavizadas con Add-k (k=0.05):\n",
            "P(models | all) = 0.7241\n",
            "P(are | models) = 0.8367\n",
            "P(wrong | are) = 0.4286\n",
            "P(model | a) = 0.7241\n",
            "P(is | model) = 0.7241\n",
            "P(wrong | is) = 0.7241\n",
            "P(models | some) = 0.7241\n",
            "P(useful | are) = 0.4286\n",
            "P(<UNK> | <UNK>) = 0.0006\n",
            "\n",
            "Probabilidades de bigramas suavizadas con Add-k (k=0.15):\n",
            "P(models | all) = 0.4894\n",
            "P(are | models) = 0.6418\n",
            "P(wrong | are) = 0.3433\n",
            "P(model | a) = 0.4894\n",
            "P(is | model) = 0.4894\n",
            "P(wrong | is) = 0.4894\n",
            "P(models | some) = 0.4894\n",
            "P(useful | are) = 0.3433\n",
            "P(<UNK> | <UNK>) = 0.0018\n"
          ]
        }
      ],
      "source": [
        "bigram_counts = defaultdict(int)\n",
        "unigram_counts = defaultdict(int)\n",
        "\n",
        "#Suavizado add-k para bigrama\n",
        "def add_k_smoothing_bigram(corpus, k):\n",
        "    # Conteo de bigramas y unigrams\n",
        "    bigram_counts = {}\n",
        "    unigram_counts = {}\n",
        "\n",
        "    # Construir bigramas y contar unigramas\n",
        "    for sentence in corpus:\n",
        "        words = sentence.split()\n",
        "        for i in range(len(words)-1):\n",
        "            bigram = (words[i], words[i + 1])\n",
        "            unigram = words[i]\n",
        "\n",
        "            # Conteo de bigramas\n",
        "            if bigram in bigram_counts:\n",
        "                bigram_counts[bigram] += 1\n",
        "            else:\n",
        "                bigram_counts[bigram] = 1\n",
        "\n",
        "            # Conteo de unigrams\n",
        "            if unigram in unigram_counts:\n",
        "                unigram_counts[unigram] += 1\n",
        "            else:\n",
        "                unigram_counts[unigram] = 1\n",
        "\n",
        "        # Contar el último unigrama de la oración\n",
        "        last_word = words[-1]\n",
        "        if last_word in unigram_counts:\n",
        "            unigram_counts[last_word] += 1\n",
        "        else:\n",
        "            unigram_counts[last_word] = 1\n",
        "\n",
        "    # Tamaño del vocabulario\n",
        "    V = len(unigram_counts)\n",
        "\n",
        "    # Cálculo de las probabilidades suavizadas para bigramas\n",
        "    add_k_probabilities = {}\n",
        "    for bigram, bigram_count in bigram_counts.items():\n",
        "        w_n_1 = bigram[0]  # w_{n-1}\n",
        "        # Aplicando la ecuación P_Add-k(w_n | w_{n-1}) = (C(w_{n-1}w_n) + k) / (C(w_{n-1}) + kV)\n",
        "        add_k_probabilities[bigram] = (bigram_count + k) / (unigram_counts[w_n_1] + k * V)\n",
        "\n",
        "    # Probabilidad para un bigrama no visto\n",
        "    add_k_probabilities[('<UNK>', '<UNK>')] = k / (V * (V + k))\n",
        "\n",
        "    return add_k_probabilities\n",
        "\n",
        "\n",
        "k = 0.05\n",
        "add_k_prob_bigrams = add_k_smoothing_bigram(corpus, k)\n",
        "print(\"\\nProbabilidades de bigramas suavizadas con Add-k (k=0.05):\")\n",
        "for bigram, prob in add_k_prob_bigrams.items():\n",
        "    print(f\"P({bigram[1]} | {bigram[0]}) = {prob:.4f}\")\n",
        "\n",
        "k = 0.15\n",
        "add_k_prob_bigrams = add_k_smoothing_bigram(corpus, k)\n",
        "print(\"\\nProbabilidades de bigramas suavizadas con Add-k (k=0.15):\")\n",
        "for bigram, prob in add_k_prob_bigrams.items():\n",
        "    print(f\"P({bigram[1]} | {bigram[0]}) = {prob:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZEGdjAkuV3w"
      },
      "source": [
        "En la salida anterior se observa que para la probabilidad para k= 0.05, el valor de P( models | a) es 0.0006 y para k=0.15 el valor de\n",
        "P(models | a) = 0.0018."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUW6VE3lZ5RS"
      },
      "source": [
        "### d. Calcule las probabilidades de todos los bigramas y el bigrama no visto a models con back-off y stupid-off:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyrtijcTofkG"
      },
      "source": [
        "Referencia: https://github.com/kapumota/Actividades-CC0C2/blob/3d0cde28776954bfc6367f2b94725dffcfaf8f38/Cuadernos-CC0C2/Clase2/Counts-backoff-suavizado.ipynb#L171"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLlAzbPgzmZ4",
        "outputId": "a15ad7b3-8ded-4b65-e6a0-8f18655126c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probabilidades de bigramas con back-off:\n",
            "P('wrong' | 'are') = 0.5000\n",
            "P('model' | 'a') = 1.0000\n",
            "P('</s>' | 'useful') = 1.0000\n",
            "P('are' | 'models') = 1.0000\n",
            "P('wrong' | 'is') = 1.0000\n",
            "P('some' | '<s>') = 0.3333\n",
            "P('a' | '<s>') = 0.3333\n",
            "P('useful' | 'are') = 0.5000\n",
            "P('is' | 'model') = 1.0000\n",
            "P('all' | '<s>') = 0.3333\n",
            "P('models' | 'some') = 1.0000\n",
            "P('models' | 'all') = 1.0000\n",
            "P('</s>' | 'wrong') = 1.0000\n",
            "\n",
            "P('models' | 'a') con back-off = 0.1333\n",
            "\n",
            "Probabilidades de bigramas con stupid-backoff:\n",
            "P('wrong' | 'are') = 0.5000\n",
            "P('model' | 'a') = 1.0000\n",
            "P('</s>' | 'useful') = 1.0000\n",
            "P('are' | 'models') = 1.0000\n",
            "P('wrong' | 'is') = 1.0000\n",
            "P('some' | '<s>') = 0.3333\n",
            "P('a' | '<s>') = 0.3333\n",
            "P('useful' | 'are') = 0.5000\n",
            "P('is' | 'model') = 1.0000\n",
            "P('all' | '<s>') = 0.3333\n",
            "P('models' | 'some') = 1.0000\n",
            "P('models' | 'all') = 1.0000\n",
            "P('</s>' | 'wrong') = 1.0000\n",
            "\n",
            "P('models' | 'a') con stupid-backoff = 0.0533\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import collections\n",
        "from typing import List, Tuple\n",
        "\n",
        "# El vocabulario y el corpus proporcionados\n",
        "vocabulario = ['<s>', '</s>', 'a', 'all', 'are', 'model', 'models', 'some', 'useful', 'wrong']\n",
        "corpus = [\"all models are wrong\", \"a model is wrong\", \"some models are useful\"]\n",
        "\n",
        "# Convertimos el corpus en una lista de listas de tokens\n",
        "train_corpus = [sentence.split() for sentence in corpus]\n",
        "\n",
        "# Implementación de modelos N-grama\n",
        "class NGramModel:\n",
        "    def __init__(self, n: int):\n",
        "        self.n = n\n",
        "        self.ngram_counts = collections.Counter()\n",
        "        self.context_counts = collections.Counter()\n",
        "        self.vocab = set()\n",
        "        self.total_ngrams = 0\n",
        "\n",
        "    def train(self, corpus: List[List[str]]):\n",
        "        for document in corpus:\n",
        "            tokens = ['<s>'] * (self.n - 1) + document + ['</s>']\n",
        "            self.vocab.update(tokens)\n",
        "            for i in range(len(tokens) - self.n + 1):\n",
        "                ngram = tuple(tokens[i:i + self.n])\n",
        "                context = tuple(tokens[i:i + self.n - 1])\n",
        "                self.ngram_counts[ngram] += 1\n",
        "                self.context_counts[context] += 1\n",
        "                self.total_ngrams += 1\n",
        "\n",
        "    def get_ngram_prob(self, ngram: Tuple[str, ...]) -> float:\n",
        "        count = self.ngram_counts.get(ngram, 0)\n",
        "        context = ngram[:-1]\n",
        "        context_count = self.context_counts.get(context, 0)\n",
        "        if context_count == 0:\n",
        "            return 0.0\n",
        "        else:\n",
        "            return count / context_count\n",
        "\n",
        "# Implementación de Backoff Estándar\n",
        "class BackoffNGramModel(NGramModel):\n",
        "    def __init__(self, n: int, models: List[NGramModel]):\n",
        "        super().__init__(n)\n",
        "        self.models = models  # Lista de modelos de diferentes órdenes, ordenados de mayor a menor\n",
        "        # Actualizamos self.vocab con la unión de los vocabularios de los modelos\n",
        "        self.vocab = set()\n",
        "        for model in self.models:\n",
        "            self.vocab.update(model.vocab)\n",
        "\n",
        "    def get_ngram_prob(self, ngram: Tuple[str, ...]) -> float:\n",
        "        for model in self.models:\n",
        "            ngram_adjusted = ngram[-model.n:]\n",
        "            prob = model.get_ngram_prob(ngram_adjusted)\n",
        "            if prob > 0:\n",
        "                return prob\n",
        "        # Si ningún modelo tiene el n-grama, retrocedemos al unigrama\n",
        "        return 0.0\n",
        "\n",
        "# Implementación del Stupid Backoff\n",
        "class StupidBackoffNGramModel(NGramModel):\n",
        "    def __init__(self, n: int, models: List[NGramModel], alpha: float = 0.4):\n",
        "        super().__init__(n)\n",
        "        self.models = models  # Lista de modelos de diferentes órdenes, ordenados de mayor a menor\n",
        "        self.alpha = alpha    # Factor de escala fijo\n",
        "        # Actualizamos self.vocab con la unión de los vocabularios de los modelos\n",
        "        self.vocab = set()\n",
        "        for model in self.models:\n",
        "            self.vocab.update(model.vocab)\n",
        "\n",
        "    def get_ngram_prob(self, ngram: Tuple[str, ...]) -> float:\n",
        "        for i, model in enumerate(self.models):\n",
        "            ngram_adjusted = ngram[-model.n:]\n",
        "            prob = model.get_ngram_prob(ngram_adjusted)\n",
        "            if prob > 0:\n",
        "                return (self.alpha ** i) * prob\n",
        "        # Si ningún modelo tiene el n-grama, asignamos probabilidad cero\n",
        "        return 0.0\n",
        "\n",
        "# Entrenamiento de los modelos\n",
        "# Entrenamos modelos base\n",
        "unigram_model = NGramModel(n=1)\n",
        "bigram_model = NGramModel(n=2)\n",
        "\n",
        "unigram_model.train(train_corpus)\n",
        "bigram_model.train(train_corpus)\n",
        "\n",
        "# Modelos para backoff estándar\n",
        "backoff_model = BackoffNGramModel(n=2, models=[bigram_model, unigram_model])\n",
        "\n",
        "# Modelo Stupid Backoff\n",
        "stupid_backoff_model = StupidBackoffNGramModel(n=2, models=[bigram_model, unigram_model], alpha=0.4)\n",
        "\n",
        "# Lista de bigramas del corpus\n",
        "bigrams_in_corpus = set()\n",
        "for sentence in train_corpus:\n",
        "    tokens = ['<s>'] + sentence + ['</s>']\n",
        "    for i in range(len(tokens) - 1):\n",
        "        bigr = (tokens[i], tokens[i + 1])\n",
        "        bigrams_in_corpus.add(bigr)\n",
        "\n",
        "# Cálculo de probabilidades con back-off para los bigramas del corpus\n",
        "print(\"Probabilidades de bigramas con back-off:\")\n",
        "for bigram in bigrams_in_corpus:\n",
        "    prob = backoff_model.get_ngram_prob(bigram)\n",
        "    print(f\"P('{bigram[1]}' | '{bigram[0]}') = {prob:.4f}\")\n",
        "\n",
        "# Probabilidad del bigrama no visto 'a models' con back-off\n",
        "prob_a_models_backoff = backoff_model.get_ngram_prob(('a', 'models'))\n",
        "print(f\"\\nP('models' | 'a') con back-off = {prob_a_models_backoff:.4f}\")\n",
        "\n",
        "# Cálculo de probabilidades con stupid-backoff para los bigramas del corpus\n",
        "print(\"\\nProbabilidades de bigramas con stupid-backoff:\")\n",
        "for bigram in bigrams_in_corpus:\n",
        "    prob = stupid_backoff_model.get_ngram_prob(bigram)\n",
        "    print(f\"P('{bigram[1]}' | '{bigram[0]}') = {prob:.4f}\")\n",
        "\n",
        "# Probabilidad del bigrama no visto 'a models' con stupid-backoff\n",
        "prob_a_models_stupid = stupid_backoff_model.get_ngram_prob(('a', 'models'))\n",
        "print(f\"\\nP('models' | 'a') con stupid-backoff = {prob_a_models_stupid:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrEJ30ZGcy97"
      },
      "source": [
        "# Parte 2:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XycIxfpyAmTx"
      },
      "source": [
        "Referencia: https://github.com/kapumota/Actividades-CC0C2/blob/3d0cde28776954bfc6367f2b94725dffcfaf8f38/Cuadernos-CC0C2/Clase2/Modelos-lenguaje2.ipynb#L913"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cpnxPBMVBid"
      },
      "source": [
        "Referencia: https://github.com/kapumota/Actividades-CC0C2/blob/3d0cde28776954bfc6367f2b94725dffcfaf8f38/Cuadernos-CC0C2/Clase2/Topicos-avanzados.ipynb#L501"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z4ClZgQc4f4"
      },
      "source": [
        "### e. El suavizado de Good-Turing reasigna la masa de probabilidad de los n-gramas ricos a los n-gramas pobres. Dado un corpus D, supongamos que tratamos todas las unigramas desconocidas como ⟨UNK⟩ por lo tanto, el vocabulario es {w:w∈D}∪{⟨UNK⟩} y N0=1. Calcula r, Nr, para todas las unigramas de la parte 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "o9OnsViqhZ_i"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import math\n",
        "from typing import List, Tuple, Dict\n",
        "import numpy as np\n",
        "\n",
        "corpus = [ \"all models are wrong\", \"a model is wrong\",\n",
        "            \"some models are useful\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zhVp8eWLAFHe"
      },
      "outputs": [],
      "source": [
        "def tokenize_corpus(corpus: List[str]) -> List[List[str]]:\n",
        "    tokenized_corpus = []\n",
        "    for sentence in corpus:\n",
        "        tokens = sentence.lower().split()\n",
        "        tokens = ['<s>'] + tokens + ['</s>']  # Añadimos tokens de inicio y fin de oración\n",
        "        tokenized_corpus.append(tokens)\n",
        "    return tokenized_corpus\n",
        "\n",
        "\n",
        "tokenized_corpus = tokenize_corpus(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2u46GW6mltA7",
        "outputId": "1fe6dd2c-6737-4688-aba1-8e9a65a77011"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Conteos de Unigramas:\n",
            "('<s>',): 3\n",
            "('all',): 1\n",
            "('models',): 2\n",
            "('are',): 2\n",
            "('wrong',): 2\n",
            "('</s>',): 3\n",
            "('a',): 1\n",
            "('model',): 1\n",
            "('is',): 1\n",
            "('some',): 1\n",
            "('useful',): 1\n"
          ]
        }
      ],
      "source": [
        "# Conteo de N-gramas\n",
        "def count_ngrams(tokenized_corpus: List[List[str]], n: int) -> Dict[Tuple[str, ...], int]:\n",
        "    ngram_counts = collections.Counter()\n",
        "    for tokens in tokenized_corpus:\n",
        "        for i in range(len(tokens) - n + 1):\n",
        "            ngram = tuple(tokens[i:i + n])\n",
        "            ngram_counts[ngram] += 1\n",
        "    return ngram_counts\n",
        "\n",
        "# Contamos unigramas e incluimos <UNK>\n",
        "unigram_counts = count_ngrams(tokenized_corpus, 1)\n",
        "unigram_counts[('<UNK>',)] = 0  # Para representar unigramas desconocidos\n",
        "\n",
        "unigram_counts = count_ngrams(tokenized_corpus, 1)\n",
        "print(\"\\nConteos de Unigramas:\")\n",
        "for unigram, count in unigram_counts.items():\n",
        "    print(f\"{unigram}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Mq0I6s4lAReZ"
      },
      "outputs": [],
      "source": [
        "bigram_counts = count_ngrams(tokenized_corpus, 2)\n",
        "trigram_counts = count_ngrams(tokenized_corpus, 3)\n",
        "\n",
        "#  Cálculo de N(C)\n",
        "def calculate_NC(ngram_counts: Dict[Tuple[str, ...], int]) -> Dict[int, int]:\n",
        "    count_of_counts = collections.Counter()\n",
        "    for count in ngram_counts.values():\n",
        "        count_of_counts[count] += 1\n",
        "    return count_of_counts\n",
        "\n",
        "def sort_NC(NC: Dict[int, int]) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    counts = np.array(list(NC.keys()))\n",
        "    frequencies = np.array([NC[count] for count in counts])\n",
        "    sorted_indices = np.argsort(counts)\n",
        "    return counts[sorted_indices], frequencies[sorted_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tLTd1zHSnX4g"
      },
      "outputs": [],
      "source": [
        "# Calculamos N(C) para bigramas (puede aplicarse a unigramas y trigramas de manera similar)\n",
        "NC_bigram = calculate_NC(bigram_counts)\n",
        "counts_bigram, frequencies_bigram = sort_NC(NC_bigram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "J0LOitM9na9R"
      },
      "outputs": [],
      "source": [
        "# Suavizado Good-Turing\n",
        "def good_turing_discounting(ngram_counts: Dict[Tuple[str, ...], int]) -> Dict[Tuple[str, ...], float]:\n",
        "    # Calculamos N(C)\n",
        "    NC = calculate_NC(ngram_counts)\n",
        "    counts, frequencies = sort_NC(NC)\n",
        "\n",
        "    # Ajuste de conteos\n",
        "    total_ngrams = sum(ngram_counts.values())\n",
        "    max_count = max(counts)\n",
        "    adjusted_counts = {}\n",
        "\n",
        "    for ngram, count in ngram_counts.items():\n",
        "        if count < max_count:\n",
        "            Nc = NC[count]\n",
        "            Nc1 = NC.get(count + 1, 0)\n",
        "            if Nc > 0:\n",
        "                C_star = (count + 1) * (Nc1 / Nc)\n",
        "                adjusted_counts[ngram] = C_star\n",
        "            else:\n",
        "                adjusted_counts[ngram] = count\n",
        "        else:\n",
        "            adjusted_counts[ngram] = count  # Para conteos máximos, no ajustamos\n",
        "    return adjusted_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ifznnUoAo68K"
      },
      "outputs": [],
      "source": [
        "#Cálculo de probabilidades ajustadas\n",
        "def calculate_probabilities(adjusted_counts: Dict[Tuple[str, ...], float],\n",
        "                            n_minus1_counts: Dict[Tuple[str, ...], int]) -> Dict[Tuple[str, ...], float]:\n",
        "    probabilities = {}\n",
        "    for ngram, adjusted_count in adjusted_counts.items():\n",
        "        context = ngram[:-1]\n",
        "        context_count = n_minus1_counts.get(context, sum(n_minus1_counts.values()))\n",
        "        probability = adjusted_count / context_count if context_count > 0 else 0.0\n",
        "        probabilities[ngram] = probability\n",
        "    return probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kzqanCIh1XPQ"
      },
      "outputs": [],
      "source": [
        "adjusted_bigram_counts = good_turing_discounting(bigram_counts)\n",
        "\n",
        "adjusted_unigram_counts = good_turing_discounting(unigram_counts)\n",
        "\n",
        "# Calculamos las probabilidades ajustadas para bigramas\n",
        "unigram_total_count = sum(unigram_counts.values())\n",
        "bigram_probabilities = calculate_probabilities(adjusted_bigram_counts, unigram_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FAJSzk3C2v9Z"
      },
      "outputs": [],
      "source": [
        "# Asignación de probabilidad a N-gramas no observados\n",
        "def probability_of_unseen(NC: Dict[int, int], total_ngrams: int) -> float:\n",
        "    N1 = NC.get(1, 0)\n",
        "    return N1 / total_ngrams if total_ngrams > 0 else 0.0\n",
        "\n",
        "# Probabilidad de n-gramas no observados para bigramas\n",
        "total_bigrams = sum(bigram_counts.values())\n",
        "P_unseen_bigram = probability_of_unseen(NC_bigram, total_bigrams)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2paoc320pGFU"
      },
      "outputs": [],
      "source": [
        "#Uso del modelo para calcular la probabilidad de una oración\n",
        "def sentence_probability(sentence: str, bigram_probabilities: Dict[Tuple[str, str], float], P_unseen: float) -> float:\n",
        "    tokens = ['<s>'] + sentence.lower().split() + ['</s>']\n",
        "    probability_log_sum = 0.0\n",
        "    for i in range(len(tokens) - 1):\n",
        "        bigram = (tokens[i], tokens[i + 1])\n",
        "        prob = bigram_probabilities.get(bigram, P_unseen)\n",
        "        probability_log_sum += math.log(prob) if prob > 0 else float('-inf')\n",
        "    return math.exp(probability_log_sum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdthLr8vDP3X",
        "outputId": "7467c929-1e94-4841-814d-0b101980d64b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " r (veces que ocurre un unigrama):\n",
            "\n",
            "('<s>',): 3\n",
            "('all',): 1\n",
            "('models',): 2\n",
            "('are',): 2\n",
            "('wrong',): 2\n",
            "('</s>',): 3\n",
            "('a',): 1\n",
            "('model',): 1\n",
            "('is',): 1\n",
            "('some',): 1\n",
            "('useful',): 1\n",
            "\n",
            " Nr (número de unigramas que ocurren exactamente r veces):\n",
            "\n",
            "Counter({1: 6, 2: 3, 3: 2, 0: 1})\n"
          ]
        }
      ],
      "source": [
        "NC = calculate_NC(unigram_counts)\n",
        "NC[0] = 1  # Agregamos N0 = 1 para el caso <UNK>\n",
        "counts_unigram, frequencies_unigram  = sort_NC(NC)\n",
        "\n",
        "print(\"\\n r (veces que ocurre un unigrama):\\n\")\n",
        "for unigram, count in unigram_counts.items():\n",
        "    print(f\"{unigram}: {count}\")\n",
        "\n",
        "print(\"\\n Nr (número de unigramas que ocurren exactamente r veces):\\n\")\n",
        "print(NC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgzcRy37DxIt"
      },
      "source": [
        "### f. Para r<3, calcula cr y las probabilidades de todas las unigramas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeMkFnAS_q_1",
        "outputId": "87b66c43-38e1-4dde-fda3-fbe8b6ecde5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Probabilidades ajustadas para r < 3:\n",
            "C('all',): 1.0\n",
            "C('models',): 2.0\n",
            "C('are',): 2.0\n",
            "C('wrong',): 2.0\n",
            "C('a',): 1.0\n",
            "C('model',): 1.0\n",
            "C('is',): 1.0\n",
            "C('some',): 1.0\n",
            "C('useful',): 1.0\n"
          ]
        }
      ],
      "source": [
        "# Mostramos las probabilidades para r < 3\n",
        "print(\"\\nProbabilidades ajustadas para r < 3:\")\n",
        "for unigram, count in adjusted_unigram_counts.items():\n",
        "    if unigram_counts[unigram] < 3:\n",
        "        print(f\"C{unigram}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PQE5JqVffRf",
        "outputId": "4c07d110-28af-4e4a-acc8-6ba690ba8ee2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Probabilidades ajustadas para r < 3:\n",
            "P('all',): 0.05555555555555555\n",
            "P('models',): 0.1111111111111111\n",
            "P('are',): 0.1111111111111111\n",
            "P('wrong',): 0.1111111111111111\n",
            "P('a',): 0.05555555555555555\n",
            "P('model',): 0.05555555555555555\n",
            "P('is',): 0.05555555555555555\n",
            "P('some',): 0.05555555555555555\n",
            "P('useful',): 0.05555555555555555\n"
          ]
        }
      ],
      "source": [
        "unigram_probabilities = calculate_probabilities(adjusted_unigram_counts, {'<s>': unigram_total_count})\n",
        "\n",
        "# Mostramos las probabilidades para r < 3\n",
        "print(\"\\nProbabilidades ajustadas para r < 3:\")\n",
        "for unigram, prob in unigram_probabilities.items():\n",
        "    if unigram_counts[unigram] < 3:\n",
        "        print(f\"P{unigram}: {prob}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2THL2vknuukW"
      },
      "source": [
        "### g. Para el valor máximo de r, Nr+1=0. En este caso, la probabilidad P(w: #w=r) aún puede estimarse mediante MLE (Máxima Verosimilitud). Calcula la   probabilidad de las unigramas de la parte 1 que aparecen con mayor frecuencia, es decir, r=3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "junsGpFaP9gy",
        "outputId": "7c81dc3b-5530-4cf5-bd94-704254c87383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Probabilidades para unigramas con r = 3 usando MLE:\n",
            "C('<s>',): 3, P(w) = 0.1667\n",
            "C('</s>',): 3, P(w) = 0.1667\n"
          ]
        }
      ],
      "source": [
        "total_tokens = sum(unigram_counts.values())\n",
        "probabilities_g = {}\n",
        "print(\"\\nProbabilidades para unigramas con r = 3 usando MLE:\")\n",
        "for unigram, count in unigram_counts.items():\n",
        "    if count == 3:\n",
        "        probability = count / total_tokens\n",
        "        unigram_probabilities[unigram] = probability\n",
        "        print(f\"C{unigram}: {count}, P(w) = {probability:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Funq9Pg2Hh98"
      },
      "source": [
        "### h. Muestra que la suma de las probabilidades de todas las unigramas dadas en (f) y (g) no es 1. Intenta normalizar las probabilidades."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7UUA1EX5LDio"
      },
      "outputs": [],
      "source": [
        "NC_unigram = calculate_NC(unigram_counts)\n",
        "total_unigrams = sum(unigram_counts.values())\n",
        "P_unseen_unigram = probability_of_unseen(NC_unigram, total_unigrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfvShEl7F-ro",
        "outputId": "14dc0fed-786a-42c6-d3e4-1b64252c0a3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Probabilidades normalizadas:\n",
            "('<s>',): P(w) = 0.1250\n",
            "('all',): P(w) = 0.0417\n",
            "('models',): P(w) = 0.0833\n",
            "('are',): P(w) = 0.0833\n",
            "('wrong',): P(w) = 0.0833\n",
            "('</s>',): P(w) = 0.1250\n",
            "('a',): P(w) = 0.0417\n",
            "('model',): P(w) = 0.0417\n",
            "('is',): P(w) = 0.0417\n",
            "('some',): P(w) = 0.0417\n",
            "('useful',): P(w) = 0.0417\n"
          ]
        }
      ],
      "source": [
        "probabilities_normalized = {w: p / (sum(unigram_probabilities.values())+P_unseen_unigram) for w, p in unigram_probabilities.items()}\n",
        "\n",
        "suma_prob = 0\n",
        "print(\"\\nProbabilidades normalizadas:\")\n",
        "for unigram, prob in probabilities_normalized.items():\n",
        "    print(f\"{unigram}: P(w) = {prob:.4f}\")\n",
        "    suma_prob += prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Fpa3rG1Ot3D"
      },
      "source": [
        "A continuación se observa que la suma de las probabilidades vistas en f y g no suman 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2Acn1SLLwLm",
        "outputId": "5156e203-2d8b-44fc-c22e-6eba3ca78c0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7499999999999998\n"
          ]
        }
      ],
      "source": [
        "print(suma_prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Hobd5SzNAPc"
      },
      "source": [
        "### i. En un corpus grande, Nr puede ser cero para valores grandes de r. Esto puede ser problemático, ya que conduce a que los valores estimados de P(w:#w=r) sean cero. Una forma de resolver este problema es usar una línea suavizada para ajustar aproximadamente la distribución de los valores conocidos. Supongamos que cambiamos la segunda oración de ejemplo de la parte 1 a ‘a model is wrong wrong wrong wrong’ de modo que N4=1, pero S_5=0. Adivina un buen valor suavizado de N_5. Usa el valor aproximado de N_5 y el valor original de los otros recuentos de frecuencia para calcular las probabilidades de todas las unigramas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-S8bE1tOs8H",
        "outputId": "9dcce103-d990-4d1d-de0a-224e3a149a26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "r y N_r:\n",
            "r = 1, N_r = 6\n",
            "r = 2, N_r = 2\n",
            "r = 3, N_r = 2\n",
            "r = 5, N_r = 1\n"
          ]
        }
      ],
      "source": [
        "# Actualizamos el corpus\n",
        "corpus_i = [\n",
        "    \"all models are wrong\",\n",
        "    \"a model is wrong wrong wrong wrong\",\n",
        "    \"some models are useful\"\n",
        "]\n",
        "\n",
        "# Tokenizamos y contamos nuevamente\n",
        "tokenized_corpus_i = tokenize_corpus(corpus_i)\n",
        "unigram_counts_i = count_ngrams(tokenized_corpus_i, 1)\n",
        "NC_unigram_i = calculate_NC(unigram_counts_i)\n",
        "\n",
        "# Mostramos los nuevos valores de N_r\n",
        "print(\"\\nr y N_r:\")\n",
        "for r, Nr in sorted(NC_unigram_i.items()):\n",
        "    print(f\"r = {r}, N_r = {Nr}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhuBpdIFPAQ4",
        "outputId": "34b213c0-a213-4233-c490-6fb66aec523c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Probabilidades ajustadas tras el suavizado:\n",
            "('<s>',): 0.05882352941176472\n",
            "('all',): 0.03921568627450981\n",
            "('models',): 0.17647058823529416\n",
            "('are',): 0.17647058823529416\n",
            "('wrong',): 0.2941176470588236\n",
            "('</s>',): 0.05882352941176472\n",
            "('a',): 0.03921568627450981\n",
            "('model',): 0.03921568627450981\n",
            "('is',): 0.03921568627450981\n",
            "('some',): 0.03921568627450981\n",
            "('useful',): 0.03921568627450981\n"
          ]
        }
      ],
      "source": [
        "# Función de suavizado Good-Turing con valor estimado de N_r\n",
        "def good_turing_discounting_unigrams_smoothed(unigram_counts: Dict[Tuple[str, ...], int], N4_estimate: float) -> Dict[Tuple[str, ...], float]:\n",
        "    NC = calculate_NC(unigram_counts)\n",
        "    # Estimamos N_4\n",
        "    NC[4] = N4_estimate\n",
        "    adjusted_counts = {}\n",
        "    counts = sorted(NC.keys())\n",
        "    max_r = max(counts)\n",
        "\n",
        "    for unigram, count in unigram_counts.items():\n",
        "        r = count\n",
        "        Nr = NC.get(r, 0)\n",
        "        Nr1 = NC.get(r + 1, 0)\n",
        "        if Nr > 0 and Nr1 > 0:\n",
        "            c_star = (r + 1) * (Nr1 / Nr)\n",
        "        else:\n",
        "            c_star = r  # Si Nr o Nr1 es 0, no ajustamos\n",
        "        adjusted_counts[unigram] = c_star\n",
        "    return adjusted_counts\n",
        "\n",
        "# Aplicamos el suavizado con N4_estimate = 0.5\n",
        "adjusted_unigram_counts_i = good_turing_discounting_unigrams_smoothed(unigram_counts_i, N4_estimate=0.5)\n",
        "\n",
        "# Calculamos las probabilidades ajustadas\n",
        "unigram_probabilities_i = calculate_probabilities(adjusted_unigram_counts_i, {'<s>': unigram_total_count})\n",
        "\n",
        "# Normalizamos las probabilidades\n",
        "total_probability_i = sum(unigram_probabilities_i.values())\n",
        "if total_probability_i != 1.0:\n",
        "    for unigram in unigram_probabilities_i:\n",
        "        unigram_probabilities_i[unigram] /= total_probability_i\n",
        "\n",
        "# Mostramos las probabilidades finales\n",
        "print(\"\\nProbabilidades ajustadas tras el suavizado:\")\n",
        "for unigram, prob in unigram_probabilities_i.items():\n",
        "    print(f\"{unigram}: {prob}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ9h4pMfrOXj"
      },
      "source": [
        "## Parte 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOx2Io1TrP43"
      },
      "source": [
        "## Implementación inicial del Brown Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAJ4B4K9gqwn",
        "outputId": "2a311b8e-0b0d-4cf7-a4cc-e3781a2cac6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Descargando el artículo: Inteligencia artificial\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import json\n",
        "\n",
        "articulos = [\n",
        "    \"Inteligencia artificial\", \"Aprendizaje automático\", \"Biotecnología\"\n",
        "]\n",
        "\n",
        "# URL de la API de Wikipedia para obtener el extracto en formato JSON\n",
        "base_url = 'https://es.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&explaintext&titles={}'\n",
        "\n",
        "# Archivo para guardar el corpus\n",
        "filename = 'corpus.txt'\n",
        "\n",
        "with open(filename, 'w', encoding='utf-8') as file:\n",
        "    for title in articulos:\n",
        "        \"\"\"\n",
        "        Descarga el contenido de artículos de Wikipedia y los guarda en un archivo de texto.\n",
        "\n",
        "        Args:\n",
        "            title (str): El título del artículo de Wikipedia a descargar.\n",
        "\n",
        "        Funcionalidad:\n",
        "            - Codifica el título del artículo para incluirlo en una URL de solicitud a la API de Wikipedia.\n",
        "            - Descarga el contenido del artículo en formato JSON y extrae el campo 'extract', que contiene el texto del artículo.\n",
        "            - Si el extracto no está vacío, lo escribe en el archivo especificado y confirma la descarga en consola.\n",
        "            - Si el extracto está vacío, indica que el artículo no se pudo descargar.\n",
        "\n",
        "        Output:\n",
        "            Archivo 'corpus.txt' con el contenido de todos los artículos descargados.\n",
        "        \"\"\"\n",
        "\n",
        "        # Codificar el título para URL\n",
        "        encoded_title = urllib.request.quote(title)\n",
        "        url = base_url.format(encoded_title)\n",
        "        print(f\"Descargando el artículo: {title}\")\n",
        "        response = urllib.request.urlopen(url)\n",
        "        data = response.read().decode('utf-8')\n",
        "\n",
        "        # Extraer el texto del JSON\n",
        "        json_data = json.loads(data)\n",
        "        pages = json_data['query']['pages']\n",
        "        page = next(iter(pages.values()))\n",
        "        extract = page.get('extract', '')\n",
        "\n",
        "        # Escribir el texto en el archivo si el extracto no está vacío\n",
        "        if extract:\n",
        "            file.write(extract + '\\n')\n",
        "            print(f\"Artículo '{title}' descargado.\")\n",
        "        else:\n",
        "            print(f\"No se pudo descargar el artículo '{title}'.\")\n",
        "\n",
        "print(f\"Todos los artículos han sido descargados y guardados en '{filename}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNUg4NpIrKVm",
        "outputId": "2b3a99c9-14e8-4e3c-a8aa-194ccf1502e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iniciando el preprocesamiento del corpus\n",
            "Tamaño del vocabulario final: 477 palabras\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Convierte el texto en minúsculas, remueve la puntuación y lo tokeniza en palabras.\n",
        "\n",
        "    Args:\n",
        "        text (str): Texto a procesar.\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de palabras (tokens) extraídas del texto.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "def stem(word):\n",
        "    \"\"\"\n",
        "    Aplica una reducción simple de sufijos comunes en español para simular el stemming.\n",
        "\n",
        "    Args:\n",
        "        word (str): Palabra a la que se le aplicará el stemming.\n",
        "\n",
        "    Returns:\n",
        "        str: La palabra sin sufijos comunes.\n",
        "    \"\"\"\n",
        "    suffixes = ['aciones', 'imientos', 'amiento', 'ición', 'adora', 'ación', 'adoras', 'adores', 'ante',\n",
        "                'ancia', 'mente', 'idad', 'ivas', 'ivos', 'anza', 'icos', 'icas', 'ico', 'ica',\n",
        "                'oso', 'osa', 'osos', 'osas', 'ismo', 'ismos', 'able', 'ables', 'ible', 'ibles',\n",
        "                'ente', 'entes', 'mente']\n",
        "    for suffix in suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            return word[:-len(suffix)]\n",
        "    return word\n",
        "\n",
        "stopwords = set([\n",
        "     'vuestro', 'vuestros', 'y', 'ya', 'yo', 'él', 'éramos','a', 'al', 'algo', 'algunas', 'algunos',\n",
        "     'ante', 'antes', 'como', 'con', 'contra', 'de', 'del', 'desde', 'donde', 'durante', 'e', 'el',\n",
        "     'ella', 'ellas', 'ellos', 'en', 'entre', 'era', 'erais', 'eran', 'eras', 'eres', 'es',\n",
        "     'esa', 'esas', 'ese', 'eso', 'esos', 'esta', 'estaba', 'estabais', 'estaban', 'estabas',\n",
        "    'vosotras', 'vosotros', 'vuestra', 'vuestras'\n",
        "])\n",
        "\n",
        "\n",
        "def preprocess_corpus(filename):\n",
        "    \"\"\"\n",
        "    Lee un archivo de texto, realiza tokenización, stemming, remueve stopwords y filtra palabras raras.\n",
        "\n",
        "    Args:\n",
        "        filename (str): Ruta del archivo de texto a procesar.\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de tokens procesados que cumplen con un umbral de frecuencia.\n",
        "    \"\"\"\n",
        "    print(\"Iniciando el preprocesamiento del corpus\")\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        word_counter = Counter()\n",
        "        final_tokens = []\n",
        "        for line in file:\n",
        "            tokens = tokenize(line)\n",
        "            stems = [stem(token) for token in tokens]\n",
        "            tokens_filtered = [word for word in stems if word not in stopwords]\n",
        "            word_counter.update(tokens_filtered)\n",
        "            final_tokens.extend(tokens_filtered)\n",
        "            \n",
        "    threshold = 5\n",
        "    frequent_words = {word for word, count in word_counter.items() if count >= threshold}\n",
        "    final_tokens = [word for word in final_tokens if word in frequent_words]\n",
        "\n",
        "    print(f\"Tamaño del vocabulario final: {len(set(final_tokens))} palabras\")\n",
        "    return final_tokens\n",
        "\n",
        "final_tokens = preprocess_corpus('corpus.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "TsqVWvb02mHI"
      },
      "outputs": [],
      "source": [
        "def brown_clustering_initial(final_tokens):\n",
        "    \"\"\"\n",
        "    Brown Clustering.\n",
        "\n",
        "    Pasos:\n",
        "    1. Inicialización: asigne cada palabra a su propio grupo.\n",
        "\n",
        "    2. Cálculos de probabilidad: estime P(c) y P(c_i, c_j).\n",
        "\n",
        "    3. Búsqueda de combinación óptima: evalúe ΔI(c_i, c_j) para cada par.\n",
        "\n",
        "    4. Combinación de grupos: combine los grupos que minimicen ΔI.\n",
        "\n",
        "    5. Repita hasta alcanzar la cantidad deseada de grupos.\n",
        "\n",
        "    \"\"\"\n",
        "    # Inicialización\n",
        "    vocabulary = set(final_tokens)\n",
        "    clusters = {}  # cluster_id: conjunto de palabras\n",
        "    word_to_cluster = {}  # word: cluster_id\n",
        "\n",
        "    for i, word in enumerate(vocabulary):\n",
        "        clusters[i] = set([word])\n",
        "        word_to_cluster[word] = i\n",
        "\n",
        "    # Cáculo de probabilidades\n",
        "    from collections import defaultdict\n",
        "    import math\n",
        "\n",
        "    # Cuenta los unigramas y bigramas\n",
        "    bigram_counts = defaultdict(int)\n",
        "    unigram_counts = defaultdict(int)\n",
        "\n",
        "    for i in range(len(final_tokens) - 1):\n",
        "        w1 = final_tokens[i]\n",
        "        w2 = final_tokens[i + 1]\n",
        "        bigram_counts[(w1, w2)] += 1\n",
        "        unigram_counts[w1] += 1\n",
        "    unigram_counts[final_tokens[-1]] += 1  # última palabra\n",
        "\n",
        "    total_bigrams = sum(bigram_counts.values())\n",
        "    total_words = sum(unigram_counts.values())\n",
        "\n",
        "    cluster_counts = defaultdict(int) \n",
        "    cluster_bigram_counts = defaultdict(int) \n",
        "\n",
        "    for word, count in unigram_counts.items():\n",
        "        c = word_to_cluster[word]\n",
        "        cluster_counts[c] += count\n",
        "\n",
        "    for (w1, w2), count in bigram_counts.items():\n",
        "        c1 = word_to_cluster[w1]\n",
        "        c2 = word_to_cluster[w2]\n",
        "        cluster_bigram_counts[(c1, c2)] += count\n",
        "\n",
        "    # Caclula la información mutua\n",
        "    def compute_mutual_information(cluster_counts, cluster_bigram_counts, total_bigrams, total_words):\n",
        "        I = 0.0\n",
        "        for (c_i, c_j), count in cluster_bigram_counts.items():\n",
        "            p_ci_cj = count / total_bigrams\n",
        "            p_ci = cluster_counts[c_i] / total_words\n",
        "            p_cj = cluster_counts[c_j] / total_words\n",
        "            if p_ci_cj > 0 and p_ci > 0 and p_cj > 0:\n",
        "                I += p_ci_cj * math.log(p_ci_cj / (p_ci * p_cj))\n",
        "        return I\n",
        "\n",
        "    I_current = compute_mutual_information(cluster_counts, cluster_bigram_counts, total_bigrams, total_words)\n",
        "\n",
        "    # Loop \n",
        "    desired_num_clusters = 100 \n",
        "    num_clusters = len(clusters)\n",
        "\n",
        "    while num_clusters > desired_num_clusters:\n",
        "        min_delta_I = None\n",
        "        best_pair = None\n",
        "        best_cluster_counts = None\n",
        "        best_cluster_bigram_counts = None\n",
        "        best_clusters = None\n",
        "        best_word_to_cluster = None\n",
        "\n",
        "        cluster_ids = list(clusters.keys())\n",
        "\n",
        "        for idx_i in range(len(cluster_ids)):\n",
        "            c_i = cluster_ids[idx_i]\n",
        "            for idx_j in range(idx_i + 1, len(cluster_ids)):\n",
        "                c_j = cluster_ids[idx_j]\n",
        "\n",
        "                # Crea clusters temporales\n",
        "                clusters_temp = clusters.copy()\n",
        "                clusters_temp[c_i] = clusters[c_i].union(clusters[c_j])\n",
        "                del clusters_temp[c_j]\n",
        "\n",
        "                # Actualiza word_to_cluster\n",
        "                word_to_cluster_temp = word_to_cluster.copy()\n",
        "                for word in clusters[c_j]:\n",
        "                    word_to_cluster_temp[word] = c_i\n",
        "\n",
        "                # Actualiza cluster_counts_temp\n",
        "                cluster_counts_temp = cluster_counts.copy()\n",
        "                cluster_counts_temp[c_i] += cluster_counts_temp[c_j]\n",
        "                del cluster_counts_temp[c_j]\n",
        "\n",
        "                # Actualiza cluster_bigram_counts_temp\n",
        "                cluster_bigram_counts_temp = defaultdict(int)\n",
        "                for (ci, cj), count in cluster_bigram_counts.items():\n",
        "                    ci_new = ci\n",
        "                    cj_new = cj\n",
        "                    if ci == c_j:\n",
        "                        ci_new = c_i\n",
        "                    if cj == c_j:\n",
        "                        cj_new = c_i\n",
        "                    cluster_bigram_counts_temp[(ci_new, cj_new)] += count\n",
        "\n",
        "                # Calula la información mutua\n",
        "                I_merge = compute_mutual_information(cluster_counts_temp, cluster_bigram_counts_temp, total_bigrams, total_words)\n",
        "\n",
        "                delta_I = I_merge - I_current\n",
        "\n",
        "                if min_delta_I is None or delta_I < min_delta_I:\n",
        "                    min_delta_I = delta_I\n",
        "                    best_pair = (c_i, c_j)\n",
        "                    best_cluster_counts = cluster_counts_temp\n",
        "                    best_cluster_bigram_counts = cluster_bigram_counts_temp\n",
        "                    best_clusters = clusters_temp\n",
        "                    best_word_to_cluster = word_to_cluster_temp\n",
        "\n",
        "        # Une los mejores pares\n",
        "        c_i, c_j = best_pair\n",
        "        clusters = best_clusters\n",
        "        cluster_counts = best_cluster_counts\n",
        "        cluster_bigram_counts = best_cluster_bigram_counts\n",
        "        word_to_cluster = best_word_to_cluster\n",
        "        I_current += min_delta_I\n",
        "        num_clusters -= 1\n",
        "        print(f\"Se unieron los clústeres {c_i} y {c_j}; número de clústeres: {num_clusters}\")\n",
        "\n",
        "    return clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "DF2H8FuM2nXZ"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[37], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      6\u001b[0m tracemalloc\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m----> 8\u001b[0m clusters_initial \u001b[38;5;241m=\u001b[39m \u001b[43mbrown_clustering_initial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m current, peak \u001b[38;5;241m=\u001b[39m tracemalloc\u001b[38;5;241m.\u001b[39mget_traced_memory()\n\u001b[0;32m     11\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
            "Cell \u001b[1;32mIn[36], line 110\u001b[0m, in \u001b[0;36mbrown_clustering_initial\u001b[1;34m(final_tokens)\u001b[0m\n\u001b[0;32m    108\u001b[0m ci_new \u001b[38;5;241m=\u001b[39m ci\n\u001b[0;32m    109\u001b[0m cj_new \u001b[38;5;241m=\u001b[39m cj\n\u001b[1;32m--> 110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ci \u001b[38;5;241m==\u001b[39m c_j:\n\u001b[0;32m    111\u001b[0m     ci_new \u001b[38;5;241m=\u001b[39m c_i\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cj \u001b[38;5;241m==\u001b[39m c_j:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import time\n",
        "import tracemalloc\n",
        "\n",
        "start_time = time.time()\n",
        "tracemalloc.start()\n",
        "\n",
        "clusters_initial = brown_clustering_initial(final_tokens)\n",
        "\n",
        "current, peak = tracemalloc.get_traced_memory()\n",
        "end_time = time.time()\n",
        "\n",
        "tracemalloc.stop()\n",
        "\n",
        "print(f\"Tiempo de ejecución de la implementación inicial: {end_time - start_time} segundos\")\n",
        "print(f\"Uso de memoria de la implementación inicial: Actual={current / 10**6} MB; Pico={peak / 10**6} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyD04ftm4Y8v"
      },
      "outputs": [],
      "source": [
        "def calculate_silhouette_coefficient(clusters, word_to_cluster, bigram_counts):\n",
        "    \"\"\"\n",
        "    Calcula el coeficiente de silhouette para evaluar la calidad del clustering\n",
        "    \"\"\"\n",
        "    from collections import defaultdict\n",
        "\n",
        "    # Calcular la distancia media entre grupos a(i)\n",
        "    a_values = {}\n",
        "    for cluster_id, words in clusters.items():\n",
        "        for word in words:\n",
        "            # Calcular la distancia promedio a otras palabras en el mismo grupo\n",
        "            intra_distances = []\n",
        "            for other_word in words:\n",
        "                if word != other_word:\n",
        "                    distance = 1 - (bigram_counts.get((word, other_word), 0) + bigram_counts.get((other_word, word), 0))\n",
        "                    intra_distances.append(distance)\n",
        "            a_values[word] = sum(intra_distances) / (len(words) - 1) if len(words) > 1 else 0\n",
        "\n",
        "    # Calcular la distancia promedio del grupo más cercano b(i)\n",
        "    b_values = {}\n",
        "    for word in word_to_cluster:\n",
        "        cluster_id = word_to_cluster[word]\n",
        "        min_distance = None\n",
        "        for other_cluster_id, words in clusters.items():\n",
        "            if other_cluster_id != cluster_id:\n",
        "                inter_distances = []\n",
        "                for other_word in words:\n",
        "                    distance = 1 - (bigram_counts.get((word, other_word), 0) + bigram_counts.get((other_word, word), 0))\n",
        "                    inter_distances.append(distance)\n",
        "                average_distance = sum(inter_distances) / len(inter_distances)\n",
        "                if min_distance is None or average_distance < min_distance:\n",
        "                    min_distance = average_distance\n",
        "        b_values[word] = min_distance\n",
        "\n",
        "    # Calcular puntuaciones de silueta\n",
        "    s_values = {}\n",
        "    for word in word_to_cluster:\n",
        "        a = a_values[word]\n",
        "        b = b_values[word]\n",
        "        s = (b - a) / max(a, b) if max(a, b) > 0 else 0\n",
        "        s_values[word] = s\n",
        "\n",
        "    # Devuelve la puntuación media de silueta\n",
        "    overall_silhouette = sum(s_values.values()) / len(s_values)\n",
        "    return overall_silhouette"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YBzbe483CKf"
      },
      "outputs": [],
      "source": [
        "def brown_clustering_improved(final_tokens):\n",
        "    \"\"\"\n",
        "    Implementación mejorada de Brown Clustering.\n",
        "\n",
        "    Mejoras:\n",
        "    - Utiliza estructuras de datos eficientes: utiliza colas de prioridad para gestionar pares de clústeres y su ΔI.\n",
        "    - Implementa técnicas de poda: limita las fusiones de candidatos mediante clústeres vecinos.\n",
        "\n",
        "    \"\"\"\n",
        "    # Inicialización\n",
        "    vocabulary = set(final_tokens)\n",
        "    clusters = {}  \n",
        "    word_to_cluster = {}  \n",
        "\n",
        "    for i, word in enumerate(vocabulary):\n",
        "        clusters[i] = set([word])\n",
        "        word_to_cluster[word] = i\n",
        "\n",
        "    # Cálculo de probabilidades\n",
        "    from collections import defaultdict\n",
        "    import math\n",
        "    import heapq\n",
        "\n",
        "    # Contar bigramas y unigramas\n",
        "    bigram_counts = defaultdict(int)\n",
        "    unigram_counts = defaultdict(int)\n",
        "\n",
        "    for i in range(len(final_tokens) - 1):\n",
        "        w1 = final_tokens[i]\n",
        "        w2 = final_tokens[i + 1]\n",
        "        bigram_counts[(w1, w2)] += 1\n",
        "        unigram_counts[w1] += 1\n",
        "    unigram_counts[final_tokens[-1]] += 1  # última palabra\n",
        "\n",
        "    total_bigrams = sum(bigram_counts.values())\n",
        "    total_words = sum(unigram_counts.values())\n",
        "\n",
        "    # Inicializar cluster_counts y cluster_bigram_counts\n",
        "    cluster_counts = defaultdict(int)  \n",
        "    cluster_bigram_counts = defaultdict(int) \n",
        "\n",
        "    for word, count in unigram_counts.items():\n",
        "        c = word_to_cluster[word]\n",
        "        cluster_counts[c] += count\n",
        "\n",
        "    for (w1, w2), count in bigram_counts.items():\n",
        "        c1 = word_to_cluster[w1]\n",
        "        c2 = word_to_cluster[w2]\n",
        "        cluster_bigram_counts[(c1, c2)] += count\n",
        "\n",
        "    # Inicializar clústeres vecinos\n",
        "    neighbors = defaultdict(set)  \n",
        "    for (c1, c2) in cluster_bigram_counts.keys():\n",
        "        if c1 != c2:\n",
        "            neighbors[c1].add(c2)\n",
        "            neighbors[c2].add(c1)\n",
        "\n",
        "    # Inicializar el montón con ΔI para cada par de vecinos\n",
        "    heap = []\n",
        "\n",
        "    def compute_delta_I(c_i, c_j):\n",
        "        # Cálculo eficiente de ΔI basado en recuentos de clústeres y recuentos de bigramas\n",
        "        delta_I = 0.0\n",
        "        # Calcular P(c_i), P(c_j), P(c_i, c_j)\n",
        "        p_ci = cluster_counts[c_i] / total_words\n",
        "        p_cj = cluster_counts[c_j] / total_words\n",
        "        p_ci_cj = cluster_bigram_counts.get((c_i, c_j), 0) / total_bigrams\n",
        "\n",
        "        if p_ci_cj > 0 and p_ci > 0 and p_cj > 0:\n",
        "            delta_I = p_ci_cj * math.log(p_ci_cj / (p_ci * p_cj))\n",
        "        else:\n",
        "            delta_I = 0.0\n",
        "        return delta_I\n",
        "\n",
        "    for c_i in clusters.keys():\n",
        "        for c_j in neighbors[c_i]:\n",
        "            if c_i < c_j:  # Evitar duplicados\n",
        "                delta_I = compute_delta_I(c_i, c_j)\n",
        "                heapq.heappush(heap, (delta_I, c_i, c_j))\n",
        "\n",
        "    desired_num_clusters = 100  \n",
        "    num_clusters = len(clusters)\n",
        "\n",
        "    while num_clusters > desired_num_clusters and heap:\n",
        "        delta_I, c_i, c_j = heapq.heappop(heap)\n",
        "\n",
        "        # Verificar si los clústeres se han fusionado\n",
        "        if c_i not in clusters or c_j not in clusters:\n",
        "            continue  # Omitir si los clústeres ya no existen\n",
        "\n",
        "        # Fusionar los clústeres c_i y c_j en c_new\n",
        "        c_new = c_i  # Utilice c_i como el nuevo ID del clúster\n",
        "\n",
        "        clusters[c_new].update(clusters[c_j])\n",
        "        del clusters[c_j]\n",
        "\n",
        "        # Actualiza word_to_cluster\n",
        "        for word in clusters[c_new]:\n",
        "            word_to_cluster[word] = c_new\n",
        "\n",
        "        # Actualiza cluster_counts\n",
        "        cluster_counts[c_new] += cluster_counts[c_j]\n",
        "        del cluster_counts[c_j]\n",
        "\n",
        "        # Actualiza cluster_bigram_counts\n",
        "        cluster_bigram_counts_updated = defaultdict(int)\n",
        "        for (ci, cj), count in cluster_bigram_counts.items():\n",
        "            ci_new = ci\n",
        "            cj_new = cj\n",
        "            if ci == c_j:\n",
        "                ci_new = c_new\n",
        "            if cj == c_j:\n",
        "                cj_new = c_new\n",
        "            cluster_bigram_counts_updated[(ci_new, cj_new)] += count\n",
        "        cluster_bigram_counts = cluster_bigram_counts_updated\n",
        "\n",
        "        # Actualiza neighbors\n",
        "        neighbors[c_new].update(neighbors[c_j])\n",
        "        neighbors[c_new].discard(c_new)\n",
        "        del neighbors[c_j]\n",
        "        for neighbor in neighbors:\n",
        "            if c_j in neighbors[neighbor]:\n",
        "                neighbors[neighbor].discard(c_j)\n",
        "                neighbors[neighbor].add(c_new)\n",
        "\n",
        "        # Actualiza heap\n",
        "        for c_k in neighbors[c_new]:\n",
        "            if c_k != c_new:\n",
        "                delta_I = compute_delta_I(c_new, c_k)\n",
        "                heapq.heappush(heap, (delta_I, c_new, c_k))\n",
        "\n",
        "        num_clusters -= 1\n",
        "        print(f\"Se unieron los clústeres {c_i} y {c_j}; número de clústeres: {num_clusters}\")\n",
        "\n",
        "    # Llamar a la función calculate_silhouette_coefficient\n",
        "    silhouette_score = calculate_silhouette_coefficient(clusters, word_to_cluster, bigram_counts)\n",
        "    print(f\"Coeficiente de Silhouette para la implementación con mejoras: {silhouette_score}\")\n",
        "\n",
        "    return clusters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIVEGHfl3tEk"
      },
      "outputs": [],
      "source": [
        "inicio = time.time()\n",
        "tracemalloc.start()\n",
        "\n",
        "clusters_improved = brown_clustering_improved(final_tokens)\n",
        "\n",
        "current, peak = tracemalloc.get_traced_memory()\n",
        "fin = time.time()\n",
        "\n",
        "tracemalloc.stop()\n",
        "\n",
        "print(f\"Tiempo de implementación con mejoras: {fin - inicio} segundos\")\n",
        "print(f\"Uso mejorado de la memoria de implementación: Actual={current / 10**6} MB; Pico={peak / 10**6} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vlQlyeA6tos"
      },
      "source": [
        "# LSA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYMa0DePQh7_"
      },
      "source": [
        "## Pre procesamiento del corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def get_random_wikipedia_articles(num_articles):\n",
        "    S = requests.Session()\n",
        "    URL = \"https://es.wikipedia.org/w/api.php\"\n",
        "    articles = []\n",
        "\n",
        "    while len(articles) < num_articles:\n",
        "        PARAMS = {\n",
        "            \"format\": \"json\",\n",
        "            \"action\": \"query\",\n",
        "            \"list\": \"random\",\n",
        "            \"rnnamespace\": 0,  \n",
        "            \"rnlimit\": min(500, num_articles - len(articles)),  \n",
        "        }\n",
        "\n",
        "        response = S.get(url=URL, params=PARAMS)\n",
        "        data = response.json()\n",
        "        random_articles = data['query']['random']\n",
        "        articles.extend([item['title'] for item in random_articles])\n",
        "\n",
        "    return articles\n",
        "\n",
        "# Obtener 1000 artículos aleatorios\n",
        "articulos_lsa= get_random_wikipedia_articles(100000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "Hi4GDPZFUST1"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Convierte el texto en minúsculas, remueve la puntuación y lo tokeniza en palabras.\n",
        "\n",
        "    Args:\n",
        "        text (str): Texto a procesar.\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de palabras (tokens) extraídas del texto.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "def stem(word):\n",
        "    \"\"\"\n",
        "    Aplica una reducción simple de sufijos comunes en español para simular el stemming.\n",
        "\n",
        "    Args:\n",
        "        word (str): Palabra a la que se le aplicará el stemming.\n",
        "\n",
        "    Returns:\n",
        "        str: La palabra sin sufijos comunes.\n",
        "    \"\"\"\n",
        "    suffixes = ['aciones', 'imientos', 'amiento', 'ición', 'adora', 'ación', 'adoras', 'adores', 'ante',\n",
        "                'ancia', 'mente', 'idad', 'ivas', 'ivos', 'anza', 'icos', 'icas', 'ico', 'ica',\n",
        "                'oso', 'osa', 'osos', 'osas', 'ismo', 'ismos', 'able', 'ables', 'ible', 'ibles',\n",
        "                'ente', 'entes', 'mente']\n",
        "    for suffix in suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            return word[:-len(suffix)]\n",
        "    return word\n",
        "\n",
        "stopwords = set([\n",
        "    'vuestro', 'vuestros', 'y', 'ya', 'yo', 'él', 'éramos', 'a', 'al', 'algo', 'algunas', 'algunos',\n",
        "    'ante', 'antes', 'como', 'con', 'contra', 'de', 'del', 'desde', 'donde', 'durante', 'e', 'el',\n",
        "    'ella', 'ellas', 'ellos', 'en', 'entre', 'era', 'erais', 'eran', 'eras', 'eres', 'es',\n",
        "    'esa', 'esas', 'ese', 'eso', 'esos', 'esta', 'estaba', 'estabais', 'estaban', 'estabas',\n",
        "    'vosotras', 'vosotros', 'vuestra', 'vuestras'\n",
        "])\n",
        "\n",
        "def preprocess_corpus(filename, articulos):\n",
        "    \"\"\"\n",
        "    Lee un archivo de texto, realiza tokenización, stemming, remueve stopwords y filtra palabras raras.\n",
        "    También separa el corpus en documentos individuales basados en los títulos de los artículos.\n",
        "\n",
        "    Args:\n",
        "        filename (str): Ruta del archivo de texto a procesar.\n",
        "        articulos (list): Lista de títulos de los artículos.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (documents, labels)\n",
        "            - documents: Lista de documentos, cada uno es una lista de tokens procesados.\n",
        "            - labels: Lista de etiquetas correspondientes a cada documento.\n",
        "    \"\"\"\n",
        "    print(\"Iniciando el preprocesamiento del corpus\")\n",
        "    documents = []\n",
        "    labels = []\n",
        "    word_counter = Counter()\n",
        "\n",
        "    # Leer todo el contenido del archivo\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    # Dividir el contenido en documentos utilizando doble salto de línea como separador\n",
        "    raw_documents = content.strip().split('\\n\\n')  # Ajustar según el formato real del archivo\n",
        "\n",
        "    # Asegurar que el número de documentos coincide con el número de títulos\n",
        "    min_length = min(len(raw_documents), len(articulos))\n",
        "    raw_documents = raw_documents[:min_length]\n",
        "    \n",
        "    articulos = articulos[:min_length]\n",
        "\n",
        "    for idx, text in enumerate(raw_documents):\n",
        "        # Tokenización y preprocesamiento\n",
        "        tokens = tokenize(text)\n",
        "        stems = [stem(token) for token in tokens]\n",
        "        tokens_filtered = [word for word in stems if word not in stopwords]\n",
        "        word_counter.update(tokens_filtered)\n",
        "        documents.append(tokens_filtered)\n",
        "        labels.append(articulos[idx])\n",
        "\n",
        "    # Filtrado de palabras raras\n",
        "    threshold = 3\n",
        "    frequent_words = {word for word, count in word_counter.items() if count >= threshold}\n",
        "\n",
        "    # Actualizar los documentos con las palabras frecuentes\n",
        "    final_documents = []\n",
        "    for doc in documents:\n",
        "        doc_filtered = [word for word in doc if word in frequent_words]\n",
        "        final_documents.append(doc_filtered)\n",
        "\n",
        "    print(f\"Tamaño del vocabulario final: {len(frequent_words)} palabras\")\n",
        "    return final_documents, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "3CEVk834-Lng"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iniciando el preprocesamiento del corpus\n",
            "Tamaño del vocabulario final: 16821 palabras\n"
          ]
        }
      ],
      "source": [
        "# Preprocesar el corpus y obtener documentos y etiquetas\n",
        "documents, labels = preprocess_corpus(filename, articulos_lsa)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhkqiftyQvFm"
      },
      "source": [
        "## Implementación inicial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "BBGRpcSi-SpD"
      },
      "outputs": [],
      "source": [
        "def lsa_initial(documents, k):\n",
        "    \"\"\"\n",
        "    Análisis Semántico Latente (LSA) implementación inicial.\n",
        "\n",
        "    Pasos:\n",
        "    1. Construir la matriz término-documento X con pesos TF-IDF.\n",
        "    2. Implementar SVD para descomponer X en U, Σ, V^T.\n",
        "    3. Reducción de Dimensionalidad: Seleccionar los k valores singulares más grandes y sus vectores correspondientes.\n",
        "    4. Proyección: Representar términos y documentos en el espacio reducido.\n",
        "\n",
        "    Parámetros:\n",
        "    - documents: Lista de documentos, cada documento es una lista de tokens.\n",
        "    - k: Número de dimensiones para reducir.\n",
        "\n",
        "    Retorna:\n",
        "    - term_vectors: Vectores de términos en dimensionalidad reducida.\n",
        "    - document_vectors: Vectores de documentos en dimensionalidad reducida.\n",
        "    - term_index: Diccionario que mapea términos a índices.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    import math\n",
        "    from collections import defaultdict\n",
        "\n",
        "    # Construir la matriz término-documento X con pesos TF-IDF.\n",
        "    # Construir vocabulario\n",
        "    vocabulary = set()\n",
        "    for doc in documents:\n",
        "        vocabulary.update(doc)\n",
        "    vocabulary = list(vocabulary)\n",
        "    term_index = {term: idx for idx, term in enumerate(vocabulary)}\n",
        "    num_terms = len(vocabulary)\n",
        "    num_docs = len(documents)\n",
        "\n",
        "    # Inicializar matriz término-documento\n",
        "    X = np.zeros((num_terms, num_docs))\n",
        "\n",
        "    # Calcular frecuencias de términos y frecuencias de documentos\n",
        "    df = defaultdict(int)  # Frecuencia de documentos\n",
        "    for idx, doc in enumerate(documents):\n",
        "        term_counts = defaultdict(int)\n",
        "        for term in doc:\n",
        "            term_counts[term] += 1\n",
        "        for term, count in term_counts.items():\n",
        "            term_idx = term_index[term]\n",
        "            X[term_idx, idx] = count  # Frecuencia de término\n",
        "        for term in set(doc):\n",
        "            df[term] += 1\n",
        "\n",
        "    # Calcular IDF y aplicar ponderación TF-IDF\n",
        "    idf = {}\n",
        "    for term in vocabulary:\n",
        "        idf[term] = math.log(num_docs / (1 + df[term]))\n",
        "    for term, idx in term_index.items():\n",
        "        X[idx, :] = X[idx, :] * idf[term]\n",
        "\n",
        "    # Implementar SVD\n",
        "    U, Sigma, VT = np.linalg.svd(X, full_matrices=False)\n",
        "\n",
        "    # Reducción de Dimensionalidad\n",
        "    U_k = U[:, :k]\n",
        "    Sigma_k = np.diag(Sigma[:k])\n",
        "    VT_k = VT[:k, :]\n",
        "\n",
        "    # Proyección\n",
        "    term_vectors = np.dot(U_k, Sigma_k)\n",
        "    document_vectors = np.dot(Sigma_k, VT_k)\n",
        "\n",
        "    return term_vectors, document_vectors.T, term_index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "455Zig2R-X5E"
      },
      "outputs": [],
      "source": [
        "def lsa_improved(documents, k, iterations=5):\n",
        "    \"\"\"\n",
        "    Análisis Semántico Latente (LSA) implementación mejorada usando método iterativo.\n",
        "\n",
        "    Mejoras:\n",
        "    - Implementar SVD truncado usando el método de potencia para calcular los k valores y vectores singulares más significativos.\n",
        "    - Optimizar operaciones usando matrices dispersas.\n",
        "\n",
        "    Parámetros:\n",
        "    - documents: Lista de documentos, cada documento es una lista de tokens.\n",
        "    - k: Número de dimensiones para reducir.\n",
        "    - iterations: Número de iteraciones para el método de potencia.\n",
        "\n",
        "    Retorna:\n",
        "    - term_vectors: Vectores de términos en dimensionalidad reducida.\n",
        "    - document_vectors: Vectores de documentos en dimensionalidad reducida.\n",
        "    - term_index: Diccionario que mapea términos a índices.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    import math\n",
        "    from collections import defaultdict\n",
        "    from scipy.sparse import csc_matrix\n",
        "\n",
        "    # Construir la matriz término-documento X con pesos TF-IDF.\n",
        "    # Construir vocabulario\n",
        "    vocabulary = set()\n",
        "    for doc in documents:\n",
        "        vocabulary.update(doc)\n",
        "    vocabulary = list(vocabulary)\n",
        "    term_index = {term: idx for idx, term in enumerate(vocabulary)}\n",
        "    num_terms = len(vocabulary)\n",
        "    num_docs = len(documents)\n",
        "\n",
        "    # Inicializar datos para matriz dispersa\n",
        "    data = []\n",
        "    rows = []\n",
        "    cols = []\n",
        "\n",
        "    # Calcular frecuencias de términos y frecuencias de documentos\n",
        "    df = defaultdict(int)  # Frecuencia de documentos\n",
        "    for idx, doc in enumerate(documents):\n",
        "        term_counts = defaultdict(int)\n",
        "        for term in doc:\n",
        "            term_counts[term] += 1\n",
        "        for term, count in term_counts.items():\n",
        "            term_idx = term_index[term]\n",
        "            data.append(count)\n",
        "            rows.append(term_idx)\n",
        "            cols.append(idx)\n",
        "        for term in set(doc):\n",
        "            df[term] += 1\n",
        "\n",
        "    # Crear matriz término-documento dispersa\n",
        "    X = csc_matrix((data, (rows, cols)), shape=(num_terms, num_docs))\n",
        "\n",
        "    # Calcular IDF y aplicar ponderación TF-IDF\n",
        "    idf = {}\n",
        "    for term in vocabulary:\n",
        "        idf[term] = math.log(num_docs / (1 + df[term]))\n",
        "    for term, idx in term_index.items():\n",
        "        X[idx, :] = X[idx, :].multiply(idf[term])\n",
        "\n",
        "    # Implementar SVD truncado usando el método de potencia\n",
        "    def truncated_svd(X, k, iterations):\n",
        "        n, m = X.shape\n",
        "        V = np.random.rand(m, k)\n",
        "        for _ in range(iterations):\n",
        "            U = X.dot(V)\n",
        "            U, _ = np.linalg.qr(U)\n",
        "            V = X.T.dot(U)\n",
        "            V, _ = np.linalg.qr(V)\n",
        "        Sigma = np.diag(np.linalg.norm(X.dot(V), axis=0))\n",
        "        return U, Sigma, V.T\n",
        "\n",
        "    U_k, Sigma_k, VT_k = truncated_svd(X, k, iterations)\n",
        "\n",
        "    # Proyección\n",
        "    term_vectors = U_k.dot(Sigma_k)\n",
        "    document_vectors = VT_k.T.dot(Sigma_k)\n",
        "\n",
        "    return term_vectors, document_vectors, term_index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "iOFW4b0X-sAK"
      },
      "outputs": [],
      "source": [
        "def classify_documents(document_vectors_train, labels_train, document_vectors_test, k=3):\n",
        "    \"\"\"\n",
        "    Clasificar documentos usando el algoritmo k-NN.\n",
        "\n",
        "    Parámetros:\n",
        "    - document_vectors_train: Array de vectores de documentos de entrenamiento.\n",
        "    - labels_train: Lista de etiquetas para los documentos de entrenamiento.\n",
        "    - document_vectors_test: Array de vectores de documentos a clasificar.\n",
        "    - k: Número de vecinos a considerar.\n",
        "\n",
        "    Retorna:\n",
        "    - predictions: Etiquetas predichas para los documentos de prueba.\n",
        "    \"\"\"\n",
        "    from collections import Counter\n",
        "    import numpy as np\n",
        "\n",
        "    predictions = []\n",
        "    for test_vec in document_vectors_test:\n",
        "        # Calcular distancias a todos los documentos de entrenamiento\n",
        "        distances = np.linalg.norm(document_vectors_train - test_vec, axis=1)\n",
        "        # Obtener índices de los k vecinos más cercanos\n",
        "        neighbor_indices = distances.argsort()[:k]\n",
        "        # Obtener etiquetas de los vecinos más cercanos\n",
        "        neighbor_labels = [labels_train[idx] for idx in neighbor_indices]\n",
        "        # Predecir la etiqueta más común\n",
        "        most_common = Counter(neighbor_labels).most_common(1)[0][0]\n",
        "        predictions.append(most_common)\n",
        "\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "bQ9ZH17u-rGl"
      },
      "outputs": [],
      "source": [
        "def evaluate_classification(true_labels, predicted_labels):\n",
        "    \"\"\"\n",
        "    Evaluar la exactitud de la clasificación.\n",
        "\n",
        "    Parámetros:\n",
        "    - true_labels: Lista de etiquetas verdaderas.\n",
        "    - predicted_labels: Lista de etiquetas predichas.\n",
        "\n",
        "    Retorna:\n",
        "    - accuracy: Exactitud de la clasificación.\n",
        "    \"\"\"\n",
        "    correct = sum(t == p for t, p in zip(true_labels, predicted_labels))\n",
        "    accuracy = correct / len(true_labels)\n",
        "    return round(accuracy, 4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "IGdr3yaLSAkz"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def split_data(documents, labels, test_size=0.2, random_state=42):\n",
        "    \"\"\"\n",
        "    Divide los documentos y etiquetas en conjuntos de entrenamiento y prueba\n",
        "\n",
        "    Args:\n",
        "        documents (list): Lista de documentos tokenizados.\n",
        "        labels (list): Lista de etiquetas correspondientes a cada documento.\n",
        "        test_size (float): Proporción del conjunto de prueba.\n",
        "        random_state (int): Semilla para el generador de números aleatorios.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (documents_train, documents_test, labels_train, labels_test)\n",
        "    \"\"\"\n",
        "    # Mezclar los datos para asegurar aleatoriedad\n",
        "    data = list(zip(documents, labels))\n",
        "    random.seed(random_state)\n",
        "    random.shuffle(data)\n",
        "    documents, labels = zip(*data)\n",
        "\n",
        "    # Calcular el tamaño del conjunto de prueba\n",
        "    split_index = int(len(documents) * (1 - test_size))\n",
        "    documents_train, documents_test = documents[:split_index], documents[split_index:]\n",
        "    labels_train, labels_test = labels[:split_index], labels[split_index:]\n",
        "\n",
        "    return list(documents_train), list(documents_test), list(labels_train), list(labels_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "g2yjSMRoV47a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tiempo de LSA inicial: 185.9575126171112 segundos\n",
            "Uso de memoria LSA inicial: Actual=14.98612 MB; Pico=1548.46632 MB\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import tracemalloc\n",
        "import numpy as np\n",
        "\n",
        "# Establecer parámetros\n",
        "k_dimensions = 80  # Dimensiones reducidas\n",
        "\n",
        "# Implementación Inicial\n",
        "start_time = time.time()\n",
        "tracemalloc.start()\n",
        "\n",
        "# LSA inicial\n",
        "term_vectors_initial, doc_vectors_initial_train, term_index = lsa_initial(documents, k_dimensions)\n",
        "\n",
        "current, peak = tracemalloc.get_traced_memory()\n",
        "end_time = time.time()\n",
        "tracemalloc.stop()\n",
        "\n",
        "print(f\"Tiempo de LSA inicial: {end_time - start_time} segundos\")\n",
        "print(f\"Uso de memoria LSA inicial: Actual={current / 10**6} MB; Pico={peak / 10**6} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "kIRE-utyWFG7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tiempo de LSA con mejoras: 83.52752566337585 segundos\n",
            "Uso de memoria LSA con mejoras: Actual=15.042239 MB; Pico=48.805111 MB\n"
          ]
        }
      ],
      "source": [
        "# Implementación Mejorada\n",
        "start_time = time.time()\n",
        "tracemalloc.start()\n",
        "\n",
        "# LSA mejorada\n",
        "term_vectors_improved, doc_vectors_improved_train, term_index = lsa_improved(documents, k_dimensions)\n",
        "\n",
        "current, peak = tracemalloc.get_traced_memory()\n",
        "end_time = time.time()\n",
        "tracemalloc.stop()\n",
        "\n",
        "print(f\"Tiempo de LSA con mejoras: {end_time - start_time} segundos\")\n",
        "print(f\"Uso de memoria LSA con mejoras: Actual={current / 10**6} MB; Pico={peak / 10**6} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BccuUR5yNSeq"
      },
      "source": [
        "# Word2Ve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rBueVtnWlqJ"
      },
      "source": [
        "# Preprocesamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ezzU5cZeWlHm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Todos los artículos han sido descargados y guardados en 'corpus.txt'.\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import json\n",
        "\n",
        "# Lista de títulos de artículos de Wikipedia en español\n",
        "articulos = [\n",
        "    \"Inteligencia artificial\", \"Aprendizaje automático\", \"Biotecnología\", \"Nanotecnología\", \"Genética\",\n",
        "    \"Tecnología de la información\", \"Robótica\", \"Revolución Industrial\", \"Segunda Guerra Mundial\", \"Edad Media\",\n",
        "    \"Guerra Fría\", \"Revolución francesa\", \"Historia de la ciencia\", \"Renacimiento\", \"Literatura en español\",\n",
        "    \"Pintura renacentista\", \"Música clásica\"\n",
        "]\n",
        "\n",
        "# Base URL de la API de Wikipedia para obtener el extracto en formato JSON\n",
        "base_url = 'https://es.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&explaintext&titles={}'\n",
        "\n",
        "# Archivo para guardar el corpus\n",
        "filename = 'corpus.txt'\n",
        "\n",
        "with open(filename, 'w', encoding='utf-8') as file:\n",
        "    for title in articulos:\n",
        "        # Codificar el título para URL\n",
        "        encoded_title = urllib.request.quote(title)\n",
        "        url = base_url.format(encoded_title)\n",
        "        #print(f\"Descargando el artículo: {title}...\")\n",
        "        response = urllib.request.urlopen(url)\n",
        "        data = response.read().decode('utf-8')\n",
        "        # Extraer el texto del JSON\n",
        "        json_data = json.loads(data)\n",
        "        pages = json_data['query']['pages']\n",
        "        page = next(iter(pages.values()))\n",
        "        extract = page.get('extract', '')\n",
        "        # Escribir el texto en el archivo si el extracto no está vacío\n",
        "        if extract:\n",
        "            file.write(extract + '\\n')\n",
        "            #print(f\"Artículo '{title}' descargado.\")\n",
        "        else:\n",
        "            print(f\"No se pudo descargar el artículo '{title}'.\")\n",
        "print(f\"Todos los artículos han sido descargados y guardados en '{filename}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0ggAc9nLWvj_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iniciando el preprocesamiento del corpus...\n",
            "Tamaño del vocabulario final: 3281 palabras\n",
            "Preprocesamiento completado.\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "def tokenize(text):\n",
        "    # Convertir todo a minúsculas\n",
        "    text = text.lower()\n",
        "    # Remover puntuación\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "    # Dividir en palabras\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "def stem(word):\n",
        "    suffixes = ['aciones', 'imientos', 'amiento', 'ición', 'adora', 'ación', 'adoras', 'adores', 'ante',\n",
        "                'ancia', 'mente', 'idad', 'ivas', 'ivos', 'anza', 'icos', 'icas', 'ico', 'ica',\n",
        "                'oso', 'osa', 'osos', 'osas', 'ismo', 'ismos', 'able', 'ables', 'ible', 'ibles',\n",
        "                'ente', 'entes', 'mente']\n",
        "    for suffix in suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            return word[:-len(suffix)]\n",
        "    return word\n",
        "\n",
        "stopwords = set([\n",
        "    'a', 'al', 'algo', 'algunas', 'algunos', 'ante', 'antes', 'como',\n",
        "    'con', 'contra', 'de', 'del', 'desde', 'donde', 'durante', 'e',\n",
        "    'el', 'ella', 'ellas', 'ellos', 'en', 'entre', 'era', 'erais',\n",
        "    'eran', 'eras', 'eres', 'es', 'esa', 'esas', 'ese', 'eso',\n",
        "    'esos', 'esta', 'estaba', 'estabais', 'estaban', 'estabas',\n",
        "    'estad', 'estada', 'estadas', 'estado', 'estados', 'estamos',\n",
        "    'estando', 'estar', 'estaremos', 'estará', 'estarán', 'estarás',\n",
        "    'estaré', 'estaréis', 'estaría', 'estaríais', 'estaríamos',\n",
        "    'estarían', 'estarías', 'estas', 'este', 'estemos', 'esto',\n",
        "    'estos', 'estoy', 'estuve', 'estuviera', 'estuvierais',\n",
        "    'estuvieran', 'estuvieras', 'estuvieron', 'estuviese',\n",
        "    'estuvieseis', 'estuviesen', 'estuvieses', 'estuvimos',\n",
        "    'estuviste', 'estuvisteis', 'estuviéramos', 'estuviésemos',\n",
        "    'estuvo', 'está', 'estábamos', 'estáis', 'están', 'estás',\n",
        "    'esté', 'estéis', 'estén', 'estés', 'fue', 'fuera', 'fuerais',\n",
        "    'fueran', 'fueras', 'fueron', 'fuese', 'fueseis', 'fuesen',\n",
        "    'fueses', 'fui', 'fuimos', 'fuiste', 'fuisteis', 'ha', 'habida',\n",
        "    'habidas', 'habido', 'habidos', 'habiendo', 'habremos', 'habrá',\n",
        "    'habrán', 'habrás', 'habré', 'habréis', 'habría', 'habríais',\n",
        "    'habríamos', 'habrían', 'habrías', 'habéis', 'había', 'habíais',\n",
        "    'habíamos', 'habían', 'habías', 'han', 'has', 'hasta', 'hay',\n",
        "    'haya', 'hayamos', 'hayan', 'hayas', 'hayáis', 'he', 'hemos',\n",
        "    'hube', 'hubiera', 'hubierais', 'hubieran', 'hubieras',\n",
        "    'hubieron', 'hubiese', 'hubieseis', 'hubiesen', 'hubieses',\n",
        "    'hubimos', 'hubiste', 'hubisteis', 'hubiéramos', 'hubiésemos',\n",
        "    'hubo', 'la', 'las', 'le', 'les', 'lo', 'los', 'me', 'mi',\n",
        "    'mis', 'mucho', 'muchos', 'muy', 'más', 'mí', 'mía', 'mías',\n",
        "    'mío', 'míos', 'nada', 'ni', 'no', 'nos', 'nosotras', 'nosotros',\n",
        "    'nuestra', 'nuestras', 'nuestro', 'nuestros', 'o', 'os', 'otra',\n",
        "    'otras', 'otro', 'otros', 'para', 'pero', 'poco', 'por', 'porque',\n",
        "    'que', 'quien', 'quienes', 'qué', 'se', 'sea', 'seamos', 'sean',\n",
        "    'seas', 'seremos', 'será', 'serán', 'serás', 'seré', 'seréis',\n",
        "    'sería', 'seríais', 'seríamos', 'serían', 'serías', 'seáis',\n",
        "    'si', 'sido', 'siendo', 'sin', 'sobre', 'sois', 'somos', 'son',\n",
        "    'soy', 'su', 'sus', 'suya', 'suyas', 'suyo', 'suyos', 'sí', 'también',\n",
        "    'tanto', 'te', 'tendremos', 'tendrá', 'tendrán', 'tendrás',\n",
        "    'tendré', 'tendréis', 'tendría', 'tendríais', 'tendríamos',\n",
        "    'tendrían', 'tendrías', 'tened', 'tenemos', 'tenga', 'tengamos',\n",
        "    'tengan', 'tengas', 'tengo', 'tengáis', 'tenida', 'tenidas',\n",
        "    'tenido', 'tenidos', 'teniendo', 'tenéis', 'tenía', 'teníais',\n",
        "    'teníamos', 'tenían', 'tenías', 'ti', 'tiene', 'tienen', 'tienes',\n",
        "    'todo', 'todos', 'tu', 'tus', 'tuve', 'tuviera', 'tuvierais',\n",
        "    'tuvieran', 'tuvieras', 'tuvieron', 'tuviese', 'tuvieseis',\n",
        "    'tuviesen', 'tuvieses', 'tuvimos', 'tuviste', 'tuvisteis',\n",
        "    'tuviéramos', 'tuviésemos', 'tuvo', 'tú', 'un', 'una', 'uno',\n",
        "    'unos', 'vosotras', 'vosotros', 'vuestra', 'vuestras', 'vuestro',\n",
        "    'vuestros', 'y', 'ya', 'yo', 'él', 'éramos'\n",
        "])\n",
        "\n",
        "print(\"Iniciando el preprocesamiento del corpus...\")\n",
        "with open('corpus.txt', 'r', encoding='utf-8') as file:\n",
        "    # Procesar el archivo línea por línea para manejar archivos grandes\n",
        "    word_counter = Counter()\n",
        "    final_tokens = []\n",
        "    for line in file:\n",
        "        tokens = tokenize(line)\n",
        "        stems = [stem(token) for token in tokens]\n",
        "        tokens_filtered = [word for word in stems if word not in stopwords]\n",
        "        word_counter.update(tokens_filtered)\n",
        "        final_tokens.extend(tokens_filtered)\n",
        "\n",
        "# Establecer el umbral de frecuencia\n",
        "threshold = 5\n",
        "\n",
        "# Obtener palabras que cumplen el umbral\n",
        "frequent_words = {word for word, count in word_counter.items() if count >= threshold}\n",
        "\n",
        "# Filtrar las palabras raras\n",
        "final_tokens = [word for word in final_tokens if word in frequent_words]\n",
        "\n",
        "print(f\"Tamaño del vocabulario final: {len(set(final_tokens))} palabras\")\n",
        "print(\"Preprocesamiento completado.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6juCeOIGNWsp"
      },
      "source": [
        "## Implementación de CBOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "9dBDbvk5NREw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de muestras de entrenamiento: 55805\n",
            "Epoch 1/50, Pérdida: 8.150338530613071\n",
            "Epoch 2/50, Pérdida: 7.4808715852180425\n",
            "Epoch 3/50, Pérdida: 6.980647591927219\n",
            "Epoch 4/50, Pérdida: 6.5984203726249175\n",
            "Epoch 5/50, Pérdida: 6.274850797763866\n",
            "Epoch 6/50, Pérdida: 5.988151156826689\n",
            "Epoch 7/50, Pérdida: 5.728892169815979\n",
            "Epoch 8/50, Pérdida: 5.492144297698888\n",
            "Epoch 9/50, Pérdida: 5.274788892160153\n",
            "Epoch 10/50, Pérdida: 5.074720989628966\n",
            "Epoch 11/50, Pérdida: 4.890112613342857\n",
            "Epoch 12/50, Pérdida: 4.719334432518657\n",
            "Epoch 13/50, Pérdida: 4.5610181967337295\n",
            "Epoch 14/50, Pérdida: 4.413986118020979\n",
            "Epoch 15/50, Pérdida: 4.277204574388703\n",
            "Epoch 16/50, Pérdida: 4.149760320882509\n",
            "Epoch 17/50, Pérdida: 4.0308404865246255\n",
            "Epoch 18/50, Pérdida: 3.919710051012402\n",
            "Epoch 19/50, Pérdida: 3.8156951331735347\n",
            "Epoch 20/50, Pérdida: 3.718175700941419\n",
            "Epoch 21/50, Pérdida: 3.626584087840843\n",
            "Epoch 22/50, Pérdida: 3.5404057369027826\n",
            "Epoch 23/50, Pérdida: 3.4591788343222056\n",
            "Epoch 24/50, Pérdida: 3.3824904597395853\n",
            "Epoch 25/50, Pérdida: 3.309970496216632\n",
            "Epoch 26/50, Pérdida: 3.2412856606095395\n",
            "Epoch 27/50, Pérdida: 3.1761348005778105\n",
            "Epoch 28/50, Pérdida: 3.1142453452679333\n",
            "Epoch 29/50, Pérdida: 3.0553703126484324\n",
            "Epoch 30/50, Pérdida: 2.999285532270458\n",
            "Epoch 31/50, Pérdida: 2.9457871667176234\n",
            "Epoch 32/50, Pérdida: 2.8946897146138317\n",
            "Epoch 33/50, Pérdida: 2.8458244516173976\n",
            "Epoch 34/50, Pérdida: 2.799038086946966\n",
            "Epoch 35/50, Pérdida: 2.7541914551509303\n",
            "Epoch 36/50, Pérdida: 2.7111581672250504\n",
            "Epoch 37/50, Pérdida: 2.6698232018359027\n",
            "Epoch 38/50, Pérdida: 2.630081479873699\n",
            "Epoch 39/50, Pérdida: 2.591836530889666\n",
            "Epoch 40/50, Pérdida: 2.554999349005946\n",
            "Epoch 41/50, Pérdida: 2.5194874814644193\n",
            "Epoch 42/50, Pérdida: 2.4852243415129567\n",
            "Epoch 43/50, Pérdida: 2.452138697169438\n",
            "Epoch 44/50, Pérdida: 2.4201642834249046\n",
            "Epoch 45/50, Pérdida: 2.3892394958047585\n",
            "Epoch 46/50, Pérdida: 2.3593071268914003\n",
            "Epoch 47/50, Pérdida: 2.330314112280626\n",
            "Epoch 48/50, Pérdida: 2.3022112621271233\n",
            "Epoch 49/50, Pérdida: 2.274952971322425\n",
            "Epoch 50/50, Pérdida: 2.248496921293021\n",
            "Tiempo de CBOW inicial: 4116.254072189331 segundos\n",
            "Uso de memoria CBOW inicial: Actual=48.942901 MB; Pico=51.809762 MB\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tracemalloc\n",
        "import time\n",
        "\n",
        "def build_vocabulary(tokens):\n",
        "    \"\"\"\n",
        "    Construye el vocabulario a partir de los tokens y genera mapas de palabras a índices y viceversa.\n",
        "\n",
        "    Args:\n",
        "        tokens (list): Lista de tokens del corpus.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (vocab_size, word_to_idx, idx_to_word)\n",
        "            - vocab_size (int): Tamaño del vocabulario.\n",
        "            - word_to_idx (dict): Diccionario que asigna a cada palabra un índice.\n",
        "            - idx_to_word (dict): Diccionario que asigna a cada índice una palabra.\n",
        "    \"\"\"\n",
        "    vocab = set(tokens)\n",
        "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "    vocab_size = len(vocab)\n",
        "    return vocab_size, word_to_idx, idx_to_word\n",
        "\n",
        "def initialize_weights(vocab_size, embedding_dim):\n",
        "    \"\"\"\n",
        "    Inicializa las matrices de pesos para los embeddings.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): Tamaño del vocabulario.\n",
        "        embedding_dim (int): Dimensión de los embeddings.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (W1, W2)\n",
        "            - W1 (np.ndarray): Matriz de pesos inicial entre input y capa oculta.\n",
        "            - W2 (np.ndarray): Matriz de pesos inicial entre capa oculta y output.\n",
        "    \"\"\"\n",
        "    W1 = np.random.uniform(-0.8, 0.8, (vocab_size, embedding_dim))\n",
        "    W2 = np.random.uniform(-0.8, 0.8, (embedding_dim, vocab_size))\n",
        "    return W1, W2\n",
        "\n",
        "def generate_cbow_data(tokens, window_size):\n",
        "    \"\"\"\n",
        "    Genera los datos de entrenamiento para el modelo CBOW.\n",
        "\n",
        "    Args:\n",
        "        tokens (list): Lista de tokens del corpus.\n",
        "        window_size (int): Tamaño de la ventana de contexto.\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de tuplas (context, target) para entrenamiento.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for i in range(window_size, len(tokens) - window_size):\n",
        "        context = tokens[i - window_size:i] + tokens[i + 1:i + window_size + 1]\n",
        "        target = tokens[i]\n",
        "        data.append((context, target))\n",
        "    return data\n",
        "\n",
        "def one_hot_encode(word_idx, vocab_size):\n",
        "    \"\"\"\n",
        "    Codifica una palabra en un vector one-hot.\n",
        "\n",
        "    Args:\n",
        "        word_idx (int): Índice de la palabra a codificar.\n",
        "        vocab_size (int): Tamaño del vocabulario.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Vector one-hot de longitud vocab_size con un 1 en la posición de word_idx.\n",
        "    \"\"\"\n",
        "    vec = np.zeros(vocab_size)\n",
        "    vec[word_idx] = 1\n",
        "    return vec\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    Calcula la función softmax de un vector.\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): Vector de entrada.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Vector con los valores softmax.\n",
        "    \"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "def train_cbow(training_data, W1, W2, word_to_idx, vocab_size, learning_rate, epochs, window_size):\n",
        "    \"\"\"\n",
        "    Entrena el modelo CBOW utilizando descenso de gradiente y softmax.\n",
        "\n",
        "    Args:\n",
        "        training_data (list): Datos de entrenamiento (context, target).\n",
        "        W1 (np.ndarray): Matriz de pesos entre input y capa oculta.\n",
        "        W2 (np.ndarray): Matriz de pesos entre capa oculta y output.\n",
        "        word_to_idx (dict): Diccionario que asigna índices a palabras.\n",
        "        vocab_size (int): Tamaño del vocabulario.\n",
        "        learning_rate (float): Tasa de aprendizaje para el descenso de gradiente.\n",
        "        epochs (int): Número de épocas de entrenamiento.\n",
        "        window_size (int): Tamaño de la ventana de contexto.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (W1, W2) Matrices de pesos actualizadas.\n",
        "    \"\"\"\n",
        "    for epoch in range(epochs):\n",
        "        loss = 0\n",
        "        for context_words, target_word in training_data:\n",
        "            # Vector de entrada (promedio de los embeddings de las palabras de contexto)\n",
        "            context_indices = [word_to_idx[word] for word in context_words]\n",
        "            x = np.mean(W1[context_indices], axis=0)  # Shape: (embedding_dim,)\n",
        "\n",
        "            # Cálculo hacia adelante\n",
        "            z = np.dot(x, W2)  # Shape: (vocab_size,)\n",
        "            y_pred = softmax(z)\n",
        "\n",
        "            # Vector de salida deseado (one-hot encoding)\n",
        "            y_true = one_hot_encode(word_to_idx[target_word], vocab_size)\n",
        "\n",
        "            # Cálculo de la pérdida (entropía cruzada)\n",
        "            loss -= np.log(y_pred[word_to_idx[target_word]] + 1e-9)\n",
        "\n",
        "            # Cálculo del error\n",
        "            e = y_pred - y_true  # Shape: (vocab_size,)\n",
        "\n",
        "            # Cálculo hacia atrás (gradientes)\n",
        "            dW2 = np.outer(x, e)  # Shape: (embedding_dim, vocab_size)\n",
        "            dW1 = np.dot(W2, e) / (2 * window_size)  # Promedio para las palabras de contexto\n",
        "\n",
        "            # Actualización de pesos\n",
        "            W1[context_indices] -= learning_rate * dW1\n",
        "            W2 -= learning_rate * dW2\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Pérdida: {loss / len(training_data)}\")\n",
        "    return W1, W2\n",
        "\n",
        "def measure_memory_time(func, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Mide el tiempo de ejecución y el uso de memoria de una función.\n",
        "\n",
        "    Args:\n",
        "        func (callable): Función a medir.\n",
        "        *args: Argumentos para la función.\n",
        "        **kwargs: Argumentos nombrados para la función.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (result, tiempo_ejecucion, current, peak)\n",
        "            - result: Resultado de la función.\n",
        "            - tiempo_ejecucion (float): Tiempo de ejecución en segundos.\n",
        "            - current (float): Uso de memoria actual en MB.\n",
        "            - peak (float): Uso máximo de memoria en MB.\n",
        "    \"\"\"\n",
        "    tracemalloc.start()\n",
        "    start_time = time.time()\n",
        "    result = func(*args, **kwargs)\n",
        "    end_time = time.time()\n",
        "    current, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "    tiempo_ejecucion = end_time - start_time\n",
        "    return result, tiempo_ejecucion, current / 10**6, peak / 10**6\n",
        "\n",
        "# Ejecución del modelo CBOW\n",
        "\n",
        "# Definir los parámetros\n",
        "embedding_dim = 50\n",
        "window_size = 2\n",
        "learning_rate = 0.05\n",
        "epochs = 50\n",
        "\n",
        "# Construir vocabulario\n",
        "vocab_size, word_to_idx, idx_to_word = build_vocabulary(final_tokens)\n",
        "\n",
        "# Inicializar pesos\n",
        "W1, W2 = initialize_weights(vocab_size, embedding_dim)\n",
        "\n",
        "# Generar datos de entrenamiento\n",
        "training_data = generate_cbow_data(final_tokens, window_size)\n",
        "print(f\"Número de muestras de entrenamiento: {len(training_data)}\")\n",
        "\n",
        "# Entrenar y medir tiempo y memoria\n",
        "(W1, W2), tiempo_ejecucion, current_mem, peak_mem = measure_memory_time(\n",
        "    train_cbow, training_data, W1, W2, word_to_idx, vocab_size, learning_rate, epochs, window_size\n",
        ")\n",
        "\n",
        "# Resultados de tiempo y memoria\n",
        "print(f\"Tiempo de CBOW inicial: {tiempo_ejecucion} segundos\")\n",
        "print(f\"Uso de memoria CBOW inicial: Actual={current_mem} MB; Pico={peak_mem} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56ctjVjbYZ3p"
      },
      "source": [
        "# Implementación de skip-gram con negative sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Oq6oTn1pYUw7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de muestras de entrenamiento: 223230\n",
            "Epoch 1/50, Pérdida: 4.15867491761536\n",
            "Epoch 2/50, Pérdida: 4.125658239919439\n",
            "Epoch 3/50, Pérdida: 3.4878207855883163\n",
            "Epoch 4/50, Pérdida: 2.6787751195252234\n",
            "Epoch 5/50, Pérdida: 2.4247308190645045\n",
            "Epoch 6/50, Pérdida: 2.350016609365114\n",
            "Epoch 7/50, Pérdida: 2.320956133911222\n",
            "Epoch 8/50, Pérdida: 2.3017132610258826\n",
            "Epoch 9/50, Pérdida: 2.28391191292416\n",
            "Epoch 10/50, Pérdida: 2.2607671848563826\n",
            "Epoch 11/50, Pérdida: 2.233405705380576\n",
            "Epoch 12/50, Pérdida: 2.205683352797052\n",
            "Epoch 13/50, Pérdida: 2.175820872680901\n",
            "Epoch 14/50, Pérdida: 2.1457642948034277\n",
            "Epoch 15/50, Pérdida: 2.117418745847363\n",
            "Epoch 16/50, Pérdida: 2.0905372548731282\n",
            "Epoch 17/50, Pérdida: 2.061761188686031\n",
            "Epoch 18/50, Pérdida: 2.031669964148663\n",
            "Epoch 19/50, Pérdida: 2.004781745223011\n",
            "Epoch 20/50, Pérdida: 1.9757679620528352\n",
            "Epoch 21/50, Pérdida: 1.9462197657310016\n",
            "Epoch 22/50, Pérdida: 1.9182686195333132\n",
            "Epoch 23/50, Pérdida: 1.89070036226386\n",
            "Epoch 24/50, Pérdida: 1.8624377682786502\n",
            "Epoch 25/50, Pérdida: 1.8329061054491762\n",
            "Epoch 26/50, Pérdida: 1.80454450442776\n",
            "Epoch 27/50, Pérdida: 1.779543514680727\n",
            "Epoch 28/50, Pérdida: 1.7487037625241317\n",
            "Epoch 29/50, Pérdida: 1.7227043426791842\n",
            "Epoch 30/50, Pérdida: 1.6956819259988898\n",
            "Epoch 31/50, Pérdida: 1.6722445377951698\n",
            "Epoch 32/50, Pérdida: 1.6455604981896543\n",
            "Epoch 33/50, Pérdida: 1.6206330394893673\n",
            "Epoch 34/50, Pérdida: 1.5963431718621126\n",
            "Epoch 35/50, Pérdida: 1.5737609146638578\n",
            "Epoch 36/50, Pérdida: 1.5546293979109984\n",
            "Epoch 37/50, Pérdida: 1.5313666843490963\n",
            "Epoch 38/50, Pérdida: 1.5119499750299787\n",
            "Epoch 39/50, Pérdida: 1.4947220204952465\n",
            "Epoch 40/50, Pérdida: 1.4770419023650079\n",
            "Epoch 41/50, Pérdida: 1.4578845913351586\n",
            "Epoch 42/50, Pérdida: 1.4425747491743655\n",
            "Epoch 43/50, Pérdida: 1.4266929332673155\n",
            "Epoch 44/50, Pérdida: 1.4129267394259832\n",
            "Epoch 45/50, Pérdida: 1.3977279221431336\n",
            "Epoch 46/50, Pérdida: 1.3847276774245516\n",
            "Epoch 47/50, Pérdida: 1.3724582708932356\n",
            "Epoch 48/50, Pérdida: 1.3607904948558283\n",
            "Epoch 49/50, Pérdida: 1.3494786615515455\n",
            "Epoch 50/50, Pérdida: 1.3380595917000142\n",
            "Tiempo de entrenamiento Skip-Gram inicial: 4098.213274717331 segundos\n",
            "Uso de memoria CBOW con mejoras: Actual=2.642986 MB; Pico=3.939404 MB\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def build_vocabulary(tokens):\n",
        "    \"\"\"\n",
        "    Construye el vocabulario y mapea palabras a índices y viceversa.\n",
        "\n",
        "    Args:\n",
        "        tokens (list): Lista de tokens del corpus.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (vocab_size, word_to_idx, idx_to_word)\n",
        "            - vocab_size (int): Tamaño del vocabulario.\n",
        "            - word_to_idx (dict): Diccionario que asigna índices a palabras.\n",
        "            - idx_to_word (dict): Diccionario que asigna palabras a índices.\n",
        "    \"\"\"\n",
        "    vocab = set(tokens)\n",
        "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "    vocab_size = len(vocab)\n",
        "    return vocab_size, word_to_idx, idx_to_word\n",
        "\n",
        "def initialize_weights(vocab_size, embedding_dim):\n",
        "    \"\"\"\n",
        "    Inicializa las matrices de pesos para las capas de embeddings.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): Tamaño del vocabulario.\n",
        "        embedding_dim (int): Dimensión de los embeddings.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (W1, W2) Matrices de pesos inicializadas.\n",
        "    \"\"\"\n",
        "    W1 = np.random.uniform(-0.8, 0.8, (vocab_size, embedding_dim))\n",
        "    W2 = np.random.uniform(-0.8, 0.8, (embedding_dim, vocab_size))\n",
        "    return W1, W2\n",
        "\n",
        "def generate_skipgram_data(tokens, window_size):\n",
        "    \"\"\"\n",
        "    Genera los datos de entrenamiento para el modelo Skip-Gram.\n",
        "\n",
        "    Args:\n",
        "        tokens (list): Lista de tokens del corpus.\n",
        "        window_size (int): Tamaño de la ventana de contexto.\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de tuplas (target, context) para entrenamiento.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for i in range(len(tokens)):\n",
        "        target = tokens[i]\n",
        "        context_indices = list(range(max(0, i - window_size), i)) + list(range(i + 1, min(len(tokens), i + window_size + 1)))\n",
        "        context = [tokens[idx] for idx in context_indices]\n",
        "        for context_word in context:\n",
        "            data.append((target, context_word))\n",
        "    return data\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Calcula la función sigmoide de un valor.\n",
        "\n",
        "    Args:\n",
        "        x (float or np.ndarray): Valor o array de entrada.\n",
        "\n",
        "    Returns:\n",
        "        float or np.ndarray: Valor o array con la función sigmoide aplicada.\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def get_negative_samples(target_idx, num_neg_samples, vocab_size):\n",
        "    \"\"\"\n",
        "    Genera muestras negativas aleatorias excluyendo la palabra objetivo.\n",
        "\n",
        "    Args:\n",
        "        target_idx (int): Índice de la palabra objetivo.\n",
        "        num_neg_samples (int): Número de muestras negativas a generar.\n",
        "        vocab_size (int): Tamaño del vocabulario.\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de índices de las muestras negativas.\n",
        "    \"\"\"\n",
        "    negative_samples = []\n",
        "    while len(negative_samples) < num_neg_samples:\n",
        "        neg_idx = np.random.randint(0, vocab_size)\n",
        "        if neg_idx != target_idx:\n",
        "            negative_samples.append(neg_idx)\n",
        "    return negative_samples\n",
        "\n",
        "def train_skipgram(training_data, word_to_idx, vocab_size, embedding_dim, learning_rate, epochs, num_neg_samples):\n",
        "    \"\"\"\n",
        "    Entrena el modelo Skip-Gram con muestreo negativo.\n",
        "\n",
        "    Args:\n",
        "        training_data (list): Datos de entrenamiento (target, context).\n",
        "        word_to_idx (dict): Diccionario que asigna índices a palabras.\n",
        "        vocab_size (int): Tamaño del vocabulario.\n",
        "        embedding_dim (int): Dimensión de los embeddings.\n",
        "        learning_rate (float): Tasa de aprendizaje.\n",
        "        epochs (int): Número de épocas de entrenamiento.\n",
        "        num_neg_samples (int): Número de muestras negativas.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (W1, W2) Matrices de pesos después del entrenamiento.\n",
        "    \"\"\"\n",
        "    # Inicializar pesos\n",
        "    W1 = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
        "    W2 = np.random.randn(embedding_dim, vocab_size) * 0.01\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        loss = 0\n",
        "        for target_word, context_word in training_data:\n",
        "            target_idx = word_to_idx[target_word]\n",
        "            context_idx = word_to_idx[context_word]\n",
        "\n",
        "            # Obtener muestras negativas\n",
        "            negative_indices = get_negative_samples(target_idx, num_neg_samples, vocab_size)\n",
        "\n",
        "            # Vectores para el target y el contexto\n",
        "            w_target = W1[target_idx]\n",
        "            w_context = W2[:, context_idx]\n",
        "\n",
        "            # Producto punto positivo\n",
        "            score = np.dot(w_target, w_context)\n",
        "            loss_pos = -np.log(sigmoid(score) + 1e-9)\n",
        "            grad_pos = sigmoid(score) - 1\n",
        "\n",
        "            # Actualización de vectores para el par positivo\n",
        "            W1[target_idx] -= learning_rate * grad_pos * w_context\n",
        "            W2[:, context_idx] -= learning_rate * grad_pos * w_target\n",
        "            loss += loss_pos\n",
        "\n",
        "            # Actualización de vectores para muestras negativas\n",
        "            for neg_idx in negative_indices:\n",
        "                w_negative = W2[:, neg_idx]\n",
        "                score_neg = np.dot(w_target, w_negative)\n",
        "                loss_neg = -np.log(sigmoid(-score_neg) + 1e-9)\n",
        "                grad_neg = (1 - sigmoid(-score_neg))\n",
        "\n",
        "                W1[target_idx] -= learning_rate * grad_neg * w_negative\n",
        "                W2[:, neg_idx] -= learning_rate * grad_neg * w_target\n",
        "                loss += loss_neg\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Pérdida: {loss / len(training_data)}\")\n",
        "    return W1, W2\n",
        "\n",
        "def measure_training_time_memory(func, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Mide el tiempo de ejecución y el uso de memoria de la función de entrenamiento.\n",
        "\n",
        "    Args:\n",
        "        func (callable): Función de entrenamiento a medir.\n",
        "        *args: Argumentos para la función.\n",
        "        **kwargs: Argumentos nombrados para la función.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (result, tiempo_ejecucion, current_mem, peak_mem)\n",
        "            - result: Resultado de la función de entrenamiento.\n",
        "            - tiempo_ejecucion (float): Tiempo de ejecución en segundos.\n",
        "            - current_mem (float): Uso de memoria actual en MB.\n",
        "            - peak_mem (float): Uso máximo de memoria en MB.\n",
        "    \"\"\"\n",
        "    tracemalloc.start()\n",
        "    start_time = time.time()\n",
        "    result = func(*args, **kwargs)\n",
        "    end_time = time.time()\n",
        "    current_mem, peak_mem = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "    tiempo_ejecucion = end_time - start_time\n",
        "    return result, tiempo_ejecucion, current_mem / 10**6, peak_mem / 10**6\n",
        "\n",
        "# Parámetros y Ejecución del modelo Skip-Gram\n",
        "\n",
        "# Definir los parámetros\n",
        "embedding_dim = 50\n",
        "window_size = 2\n",
        "learning_rate = 0.01\n",
        "epochs = 50\n",
        "num_neg_samples = 5\n",
        "\n",
        "# Construir vocabulario\n",
        "vocab_size, word_to_idx, idx_to_word = build_vocabulary(final_tokens)\n",
        "\n",
        "# Generar datos de entrenamiento\n",
        "training_data_sg = generate_skipgram_data(final_tokens, window_size)\n",
        "print(f\"Número de muestras de entrenamiento: {len(training_data_sg)}\")\n",
        "\n",
        "# Entrenar y medir tiempo de ejecución\n",
        "(W1, W2), tiempo_ejecucion, current_mem, peak_mem = measure_training_time_memory(\n",
        "    train_skipgram, training_data_sg, word_to_idx, vocab_size, embedding_dim, learning_rate, epochs, num_neg_samples\n",
        ")\n",
        "\n",
        "\n",
        "# Resultados del tiempo de ejecución\n",
        "print(f\"Tiempo de entrenamiento Skip-Gram inicial: {tiempo_ejecucion} segundos\")\n",
        "print(f\"Uso de memoria CBOW con mejoras: Actual={current_mem} MB; Pico={peak_mem} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6epIEmxLZKXO"
      },
      "source": [
        "# Implementación con mejoras de CBOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWGiV5bXZJ4m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from multiprocessing import Pool\n",
        "import multiprocessing\n",
        "import time\n",
        "import tracemalloc\n",
        "\n",
        "def build_vocabulary(tokens):\n",
        "    \"\"\"\n",
        "    Construye el vocabulario y mapea palabras a índices y viceversa.\n",
        "\n",
        "    Args:\n",
        "        tokens (list): Lista de tokens del corpus.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (vocab_size, word_to_idx, idx_to_word)\n",
        "            - vocab_size (int): Tamaño del vocabulario.\n",
        "            - word_to_idx (dict): Diccionario que asigna índices a palabras.\n",
        "            - idx_to_word (dict): Diccionario que asigna palabras a índices.\n",
        "    \"\"\"\n",
        "    vocab = set(tokens)\n",
        "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "    vocab_size = len(vocab)\n",
        "    return vocab_size, word_to_idx, idx_to_word\n",
        "\n",
        "def initialize_weights(vocab_size, embedding_dim):\n",
        "    \"\"\"\n",
        "    Inicializa las matrices de pesos utilizando inicialización Xavier/Glorot.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): Tamaño del vocabulario.\n",
        "        embedding_dim (int): Dimensión de los embeddings.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (W_input, W_output) Matrices de pesos inicializadas.\n",
        "            - W_input (np.ndarray): Matriz de pesos de entrada.\n",
        "            - W_output (np.ndarray): Matriz de pesos de salida.\n",
        "    \"\"\"\n",
        "    limit = np.sqrt(6 / (vocab_size + embedding_dim))\n",
        "    W_input = np.random.uniform(-limit, limit, (vocab_size, embedding_dim))\n",
        "    W_output = np.random.uniform(-limit, limit, (vocab_size, embedding_dim))\n",
        "    return W_input, W_output\n",
        "\n",
        "def split_data(data, num_chunks):\n",
        "    \"\"\"\n",
        "    Divide los datos en partes para distribuir entre múltiples procesos.\n",
        "\n",
        "    Args:\n",
        "        data (list): Lista de datos de entrenamiento.\n",
        "        num_chunks (int): Número de divisiones.\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de partes divididas.\n",
        "    \"\"\"\n",
        "    if num_chunks > len(data):\n",
        "        num_chunks = len(data)\n",
        "    return np.array_split(data, num_chunks)\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Calcula la función sigmoide.\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): Vector de entrada.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Vector con la función sigmoide aplicada.\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def get_negative_samples(target_idx, num_neg_samples, vocab_size):\n",
        "    \"\"\"\n",
        "    Genera muestras negativas aleatorias excluyendo la palabra objetivo.\n",
        "\n",
        "    Args:\n",
        "        target_idx (int): Índice de la palabra objetivo.\n",
        "        num_neg_samples (int): Número de muestras negativas a generar.\n",
        "        vocab_size (int): Tamaño del vocabulario.\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de índices de las muestras negativas.\n",
        "    \"\"\"\n",
        "    negative_samples = []\n",
        "    while len(negative_samples) < num_neg_samples:\n",
        "        neg_idx = np.random.randint(0, vocab_size)\n",
        "        if neg_idx != target_idx and neg_idx not in negative_samples:\n",
        "            negative_samples.append(neg_idx)\n",
        "    return negative_samples\n",
        "\n",
        "def train_cbow_process(args):\n",
        "    \"\"\"\n",
        "    Entrena una porción de datos en el modelo CBOW utilizando aprendizaje paralelo.\n",
        "\n",
        "    Args:\n",
        "        args (tuple): Contiene los siguientes elementos:\n",
        "            - data_chunk (list): Parte de los datos de entrenamiento.\n",
        "            - word_to_idx (dict): Diccionario que asigna palabras a índices.\n",
        "            - learning_rate (float): Tasa de aprendizaje.\n",
        "            - num_neg_samples (int): Número de muestras negativas.\n",
        "            - vocab_size (int): Tamaño del vocabulario.\n",
        "            - embedding_dim (int): Dimensión de los embeddings.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (total_loss, W_input, W_output)\n",
        "            - total_loss (float): Pérdida total para la porción de datos.\n",
        "            - W_input (np.ndarray): Gradientes acumulados de pesos de entrada.\n",
        "            - W_output (np.ndarray): Gradientes acumulados de pesos de salida.\n",
        "    \"\"\"\n",
        "    data_chunk, word_to_idx, learning_rate, num_neg_samples, vocab_size, embedding_dim = args\n",
        "    W_input = np.zeros((vocab_size, embedding_dim))\n",
        "    W_output = np.zeros((vocab_size, embedding_dim))\n",
        "    total_loss = 0\n",
        "\n",
        "    for context_words, target_word in data_chunk:\n",
        "        context_indices = [word_to_idx[word] for word in context_words]\n",
        "        target_idx = word_to_idx[target_word]\n",
        "\n",
        "        v_input = np.mean(W_input[context_indices], axis=0)\n",
        "\n",
        "        neg_indices = get_negative_samples(target_idx, num_neg_samples, vocab_size)\n",
        "\n",
        "        indices = [target_idx] + neg_indices\n",
        "        u_output = W_output[indices]\n",
        "\n",
        "        scores = np.dot(u_output, v_input)\n",
        "        labels = np.array([1] + [0] * len(neg_indices))\n",
        "\n",
        "        sigmoid_scores = sigmoid(scores)\n",
        "\n",
        "        loss = -np.sum(np.log(sigmoid_scores + 1e-9) * labels + np.log(1 - sigmoid_scores + 1e-9) * (1 - labels))\n",
        "        total_loss += loss\n",
        "\n",
        "        grad = sigmoid_scores - labels\n",
        "        grad_W_output = np.outer(grad, v_input)\n",
        "        grad_v_input = np.dot(grad, u_output)\n",
        "\n",
        "        for idx in indices:\n",
        "            W_output[idx] -= learning_rate * grad_W_output[indices.index(idx)]\n",
        "\n",
        "        for idx in context_indices:\n",
        "            W_input[idx] -= learning_rate * grad_v_input / len(context_indices)\n",
        "\n",
        "    return total_loss, W_input, W_output\n",
        "\n",
        "def train_cbow_parallel(training_data, word_to_idx, W_input, W_output, learning_rate, num_neg_samples, vocab_size, embedding_dim, num_processes, epochs):\n",
        "    \"\"\"\n",
        "    Entrena el modelo CBOW en paralelo utilizando múltiples procesos.\n",
        "\n",
        "    Args:\n",
        "        training_data (list): Datos de entrenamiento.\n",
        "        word_to_idx (dict): Diccionario que asigna palabras a índices.\n",
        "        W_input (np.ndarray): Matriz de pesos de entrada.\n",
        "        W_output (np.ndarray): Matriz de pesos de salida.\n",
        "        learning_rate (float): Tasa de aprendizaje.\n",
        "        num_neg_samples (int): Número de muestras negativas.\n",
        "        vocab_size (int): Tamaño del vocabulario.\n",
        "        embedding_dim (int): Dimensión de los embeddings.\n",
        "        num_processes (int): Número de procesos paralelos.\n",
        "        epochs (int): Número de épocas de entrenamiento.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (W_input, W_output) Matrices de pesos entrenadas.\n",
        "    \"\"\"\n",
        "    for epoch in range(epochs):\n",
        "        np.random.shuffle(training_data)\n",
        "        data_chunks = split_data(training_data, num_processes)\n",
        "        args = [\n",
        "            (\n",
        "                chunk,\n",
        "                word_to_idx,\n",
        "                learning_rate,\n",
        "                num_neg_samples,\n",
        "                vocab_size,\n",
        "                embedding_dim\n",
        "            )\n",
        "            for chunk in data_chunks\n",
        "        ]\n",
        "\n",
        "        with Pool(processes=num_processes) as pool:\n",
        "            results = pool.map(train_cbow_process, args)\n",
        "\n",
        "        total_loss = 0\n",
        "        grad_W_input = np.zeros_like(W_input)\n",
        "        grad_W_output = np.zeros_like(W_output)\n",
        "\n",
        "        for loss_chunk, W_input_chunk, W_output_chunk in results:\n",
        "            total_loss += loss_chunk\n",
        "            grad_W_input += W_input_chunk\n",
        "            grad_W_output += W_output_chunk\n",
        "\n",
        "        W_input -= grad_W_input\n",
        "        W_output -= grad_W_output\n",
        "\n",
        "        avg_loss = total_loss / len(training_data)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Pérdida Promedio: {avg_loss}\")\n",
        "\n",
        "    return W_input, W_output\n",
        "\n",
        "def measure_training_time_memory(func, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Mide el tiempo de ejecución y el uso de memoria de la función de entrenamiento.\n",
        "\n",
        "    Args:\n",
        "        func (callable): Función de entrenamiento a medir.\n",
        "        *args: Argumentos para la función.\n",
        "        **kwargs: Argumentos nombrados para la función.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (result, tiempo_ejecucion, current_mem, peak_mem)\n",
        "            - result: Resultado de la función de entrenamiento.\n",
        "            - tiempo_ejecucion (float): Tiempo de ejecución en segundos.\n",
        "            - current_mem (float): Uso de memoria actual en MB.\n",
        "            - peak_mem (float): Uso máximo de memoria en MB.\n",
        "    \"\"\"\n",
        "    tracemalloc.start()\n",
        "    start_time = time.time()\n",
        "    result = func(*args, **kwargs)\n",
        "    end_time = time.time()\n",
        "    current_mem, peak_mem = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "    tiempo_ejecucion = end_time - start_time\n",
        "    return result, tiempo_ejecucion, current_mem / 10**6, peak_mem / 10**6\n",
        "\n",
        "def generate_cbow_data(tokens, window_size):\n",
        "    \"\"\"\n",
        "    Genera datos para el modelo CBOW.\n",
        "\n",
        "    Args:\n",
        "        tokens (list): Lista de tokens del corpus.\n",
        "        window_size (int): Tamaño de la ventana de contexto.\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de tuplas (context, target) donde context es el contexto\n",
        "        de palabras y target es la palabra objetivo.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for idx in range(window_size, len(tokens) - window_size):\n",
        "        context = tokens[idx - window_size:idx] + tokens[idx + 1:idx + window_size + 1]\n",
        "        target = tokens[idx]\n",
        "        data.append((context, target))\n",
        "    return data\n",
        "\n",
        "# Parámetros y Ejecución del modelo CBOW paralelo\n",
        "\n",
        "embedding_dim = 50\n",
        "window_size = 2\n",
        "learning_rate = 0.01\n",
        "epochs = 5\n",
        "num_neg_samples = 5\n",
        "num_processes = multiprocessing.cpu_count()\n",
        "\n",
        "vocab_size, word_to_idx, idx_to_word = build_vocabulary(final_tokens)\n",
        "\n",
        "W_input, W_output = initialize_weights(vocab_size, embedding_dim)\n",
        "\n",
        "training_data = generate_cbow_data(final_tokens, window_size)\n",
        "\n",
        "(W_input, W_output), tiempo_ejecucion, current_mem, peak_mem = measure_training_time_memory(\n",
        "    train_cbow_parallel, training_data, word_to_idx, W_input, W_output, learning_rate, num_neg_samples, vocab_size, embedding_dim, num_processes, epochs\n",
        ")\n",
        "\n",
        "print(f\"Tiempo de ejecución de CBOW con mejoras: {tiempo_ejecucion} segundos\")\n",
        "print(f\"Uso de memoria CBOW con mejoras: Actual={current_mem} MB; Pico={peak_mem} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3G6Nc_uhaNBn"
      },
      "source": [
        "# Implementación con mejoras de Skip-gram con negative sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rT6oVq4jmgw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from multiprocessing import Pool, cpu_count\n",
        "import time\n",
        "import tracemalloc\n",
        "\n",
        "def build_vocabulary(tokens):\n",
        "    vocab = set(tokens)\n",
        "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "    vocab_size = len(vocab)\n",
        "    return vocab_size, word_to_idx, idx_to_word\n",
        "\n",
        "def initialize_weights(vocab_size, embedding_dim):\n",
        "    limit = np.sqrt(6 / (vocab_size + embedding_dim))\n",
        "    W_input = np.random.uniform(-limit, limit, (vocab_size, embedding_dim))\n",
        "    W_output = np.random.uniform(-limit, limit, (vocab_size, embedding_dim))\n",
        "    return W_input, W_output\n",
        "\n",
        "def generate_skipgram_data(tokens, window_size):\n",
        "    data = []\n",
        "    for i in range(len(tokens)):\n",
        "        target = tokens[i]\n",
        "        context_indices = list(range(max(0, i - window_size), i)) + list(range(i + 1, min(len(tokens), i + window_size + 1)))\n",
        "        context = [tokens[idx] for idx in context_indices]\n",
        "        for context_word in context:\n",
        "            data.append((target, context_word))\n",
        "    return data\n",
        "\n",
        "def create_alias_table(word_counts, vocab_size, idx_to_word):\n",
        "    total_count = sum(word_counts.values())\n",
        "    probs = np.array([word_counts[idx_to_word[i]] ** 0.75 for i in range(vocab_size)])\n",
        "    probs /= np.sum(probs)\n",
        "\n",
        "    scaled_probs = probs * vocab_size\n",
        "    alias = np.zeros(vocab_size, dtype=np.int32)\n",
        "    prob = np.zeros(vocab_size)\n",
        "\n",
        "    small = [i for i, sp in enumerate(scaled_probs) if sp < 1.0]\n",
        "    large = [i for i, sp in enumerate(scaled_probs) if sp >= 1.0]\n",
        "\n",
        "    while small and large:\n",
        "        s = small.pop()\n",
        "        l = large.pop()\n",
        "        alias[s] = l\n",
        "        prob[s] = scaled_probs[s]\n",
        "        scaled_probs[l] = scaled_probs[l] - (1.0 - scaled_probs[s])\n",
        "\n",
        "        if scaled_probs[l] < 1.0:\n",
        "            small.append(l)\n",
        "        else:\n",
        "            large.append(l)\n",
        "\n",
        "    prob = np.clip(scaled_probs, 0, 1)\n",
        "    return prob, alias\n",
        "\n",
        "def alias_sample(prob_table, alias_table, num_samples):\n",
        "    K = len(prob_table)\n",
        "    kk = np.random.randint(0, K, size=num_samples)\n",
        "    rr = np.random.uniform(0, 1, size=num_samples)\n",
        "    return np.where(rr < prob_table[kk], kk, alias_table[kk])\n",
        "\n",
        "def split_data(data, num_processes):\n",
        "    chunk_size = len(data) // num_processes\n",
        "    return [data[i * chunk_size:(i + 1) * chunk_size] for i in range(num_processes)]\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def train_skipgram_process(args):\n",
        "    \"\"\"\n",
        "    Entrena una porción de datos en el modelo Skip-Gram utilizando aprendizaje paralelo.\n",
        "\n",
        "    Args:\n",
        "        args (tuple): Tupla que contiene:\n",
        "            - data_chunk (list): Parte de los datos de entrenamiento.\n",
        "            - W_input (np.ndarray): Matriz de pesos de entrada.\n",
        "            - W_output (np.ndarray): Matriz de pesos de salida.\n",
        "            - word_to_idx (dict)\n",
        "            - prob_table (np.ndarray)\n",
        "            - alias_table (np.ndarray)\n",
        "            - num_neg_samples (int)\n",
        "            - learning_rate (float)\n",
        "    Returns:\n",
        "        tuple: (total_loss, W_input_update, W_output_update) Pérdida total y matrices de pesos actualizadas.\n",
        "    \"\"\"\n",
        "    (data_chunk, W_input, W_output, word_to_idx, prob_table, alias_table, num_neg_samples, learning_rate) = args\n",
        "    total_loss = 0\n",
        "\n",
        "    # Inicializar actualizaciones locales\n",
        "    W_input_update = np.zeros_like(W_input)\n",
        "    W_output_update = np.zeros_like(W_output)\n",
        "\n",
        "    for target_word, context_word in data_chunk:\n",
        "        target_idx = word_to_idx[target_word]\n",
        "        context_idx = word_to_idx[context_word]\n",
        "\n",
        "        neg_indices = alias_sample(prob_table, alias_table, num_neg_samples)\n",
        "        neg_indices = [idx for idx in neg_indices if idx != context_idx]\n",
        "\n",
        "        indices = [context_idx] + neg_indices\n",
        "        u_output = W_output[indices]\n",
        "        v_input = W_input[target_idx]\n",
        "        scores = np.dot(u_output, v_input)\n",
        "        labels = np.array([1] + [0] * len(neg_indices))\n",
        "        sigmoid_scores = sigmoid(scores)\n",
        "\n",
        "        loss = -np.sum(np.log(sigmoid_scores + 1e-9) * labels + np.log(1 - sigmoid_scores + 1e-9) * (1 - labels))\n",
        "        total_loss += loss\n",
        "\n",
        "        grad = sigmoid_scores - labels\n",
        "        grad_W_output = np.outer(grad, v_input)\n",
        "        grad_v_input = np.dot(grad, u_output)\n",
        "\n",
        "        # Acumular gradientes\n",
        "        W_output_update[indices] -= learning_rate * grad_W_output\n",
        "        W_input_update[target_idx] -= learning_rate * grad_v_input\n",
        "\n",
        "    return total_loss, W_input_update, W_output_update\n",
        "\n",
        "def train_skipgram_parallel(training_data_sg, W_input, W_output, word_to_idx, prob_table, alias_table,\n",
        "                            num_neg_samples, learning_rate, num_processes, epochs):\n",
        "    \"\"\"\n",
        "    Entrena el modelo Skip-Gram utilizando aprendizaje paralelo.\n",
        "\n",
        "    Returns:\n",
        "        W_input, W_output, tiempo_ejecucion, current_mem, peak_mem\n",
        "    \"\"\"\n",
        "    # Medir tiempo y memoria\n",
        "    tracemalloc.start()\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Mezclar datos\n",
        "        np.random.shuffle(training_data_sg)\n",
        "        # Dividir datos\n",
        "        data_chunks = split_data(training_data_sg, num_processes)\n",
        "        # Preparar argumentos para cada proceso\n",
        "        args = [ (data_chunk, W_input, W_output, word_to_idx, prob_table, alias_table, num_neg_samples, learning_rate)\n",
        "                 for data_chunk in data_chunks ]\n",
        "\n",
        "        with Pool(processes=num_processes) as pool:\n",
        "            results = pool.map(train_skipgram_process, args)\n",
        "\n",
        "        # Combinar actualizaciones\n",
        "        total_loss = 0\n",
        "        W_input_update = np.zeros_like(W_input)\n",
        "        W_output_update = np.zeros_like(W_output)\n",
        "\n",
        "        for res in results:\n",
        "            loss, W_input_grad, W_output_grad = res\n",
        "            total_loss += loss\n",
        "            W_input_update += W_input_grad\n",
        "            W_output_update += W_output_grad\n",
        "\n",
        "        # Actualizar pesos\n",
        "        W_input += W_input_update\n",
        "        W_output += W_output_update\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    current_mem, peak_mem = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "    tiempo_ejecucion = end_time - start_time\n",
        "    return W_input, W_output, tiempo_ejecucion, current_mem / 10**6, peak_mem / 10**6\n",
        "\n",
        "# Parámetros y Ejecución del modelo Skip-Gram con aprendizaje paralelo\n",
        "embedding_dim = 50\n",
        "window_size = 2\n",
        "learning_rate = 0.01\n",
        "epochs = 10  # Reducido para fines de ejemplo\n",
        "num_neg_samples = 5\n",
        "num_processes = cpu_count()\n",
        "\n",
        "# Construir vocabulario y datos\n",
        "vocab_size, word_to_idx, idx_to_word = build_vocabulary(final_tokens)\n",
        "W_input, W_output = initialize_weights(vocab_size, embedding_dim)\n",
        "training_data_sg = generate_skipgram_data(final_tokens, window_size)\n",
        "\n",
        "# Crear word_counts y tablas Alias\n",
        "word_counts = Counter(final_tokens)\n",
        "prob_table, alias_table = create_alias_table(word_counts, vocab_size, idx_to_word)\n",
        "\n",
        "# Entrenar y medir tiempo y memoria\n",
        "W_input, W_output, tiempo_ejecucion, current_mem, peak_mem = train_skipgram_parallel(\n",
        "    training_data_sg, W_input, W_output, word_to_idx, prob_table, alias_table,\n",
        "    num_neg_samples, learning_rate, num_processes, epochs\n",
        ")\n",
        "\n",
        "# Resultados de tiempo y memoria\n",
        "print(f\"Tiempo de ejecución de Skip-Gram con mejoras: {tiempo_ejecucion} segundos\")\n",
        "print(f\"Uso de memoria Skip-Gram con mejoras: Actual={current_mem} MB; Pico={peak_mem} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU_zGhSb6jgW"
      },
      "source": [
        "# GloVe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4us53dy8LRBm"
      },
      "source": [
        "## Implementación inicial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dY6VWt7eCYQT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Descargando el artículo: Inteligencia artificial\n",
            "Artículo 'Inteligencia artificial' descargado.\n",
            "Descargando el artículo: Aprendizaje automático\n",
            "Artículo 'Aprendizaje automático' descargado.\n",
            "Descargando el artículo: Biotecnología\n",
            "Artículo 'Biotecnología' descargado.\n",
            "Descargando el artículo: Nanotecnología\n",
            "Artículo 'Nanotecnología' descargado.\n",
            "Descargando el artículo: Genética\n",
            "Artículo 'Genética' descargado.\n",
            "Descargando el artículo: Tecnología de la información\n",
            "Artículo 'Tecnología de la información' descargado.\n",
            "Descargando el artículo: Robótica\n",
            "Artículo 'Robótica' descargado.\n",
            "Descargando el artículo: Revolución Industrial\n",
            "Artículo 'Revolución Industrial' descargado.\n",
            "Descargando el artículo: Segunda Guerra Mundial\n",
            "Artículo 'Segunda Guerra Mundial' descargado.\n",
            "Descargando el artículo: Edad Media\n",
            "Artículo 'Edad Media' descargado.\n",
            "Descargando el artículo: Guerra Fría\n",
            "Artículo 'Guerra Fría' descargado.\n",
            "Descargando el artículo: Revolución francesa\n",
            "Artículo 'Revolución francesa' descargado.\n",
            "Descargando el artículo: Historia de la ciencia\n",
            "Artículo 'Historia de la ciencia' descargado.\n",
            "Descargando el artículo: Renacimiento\n",
            "Artículo 'Renacimiento' descargado.\n",
            "Descargando el artículo: Literatura en español\n",
            "Artículo 'Literatura en español' descargado.\n",
            "Descargando el artículo: Pintura renacentista\n",
            "Artículo 'Pintura renacentista' descargado.\n",
            "Descargando el artículo: Música clásica\n",
            "Artículo 'Música clásica' descargado.\n",
            "Descargando el artículo: Filosofía griega\n",
            "Artículo 'Filosofía griega' descargado.\n",
            "Descargando el artículo: Arquitectura moderna\n",
            "Artículo 'Arquitectura moderna' descargado.\n",
            "Descargando el artículo: Sociología\n",
            "Artículo 'Sociología' descargado.\n",
            "Descargando el artículo: Antropología\n",
            "Artículo 'Antropología' descargado.\n",
            "Descargando el artículo: Psicología\n",
            "Artículo 'Psicología' descargado.\n",
            "Descargando el artículo: Parques nacionales de España\n",
            "Artículo 'Parques nacionales de España' descargado.\n",
            "Descargando el artículo: Economía de mercado\n",
            "Artículo 'Economía de mercado' descargado.\n",
            "Descargando el artículo: Globalización\n",
            "Artículo 'Globalización' descargado.\n",
            "Descargando el artículo: Política internacional\n",
            "Artículo 'Política internacional' descargado.\n",
            "Descargando el artículo: Corrupción política\n",
            "Artículo 'Corrupción política' descargado.\n",
            "Descargando el artículo: Economía de América Latina\n",
            "Artículo 'Economía de América Latina' descargado.\n",
            "Descargando el artículo: Nutrición humana\n",
            "Artículo 'Nutrición humana' descargado.\n",
            "Descargando el artículo: Genómica\n",
            "Artículo 'Genómica' descargado.\n",
            "Descargando el artículo: Epidemiología\n",
            "Artículo 'Epidemiología' descargado.\n",
            "Descargando el artículo: Neurociencia\n",
            "Artículo 'Neurociencia' descargado.\n",
            "Descargando el artículo: Psicología clínica\n",
            "Artículo 'Psicología clínica' descargado.\n",
            "Descargando el artículo: Medicina tradicional\n",
            "Artículo 'Medicina tradicional' descargado.\n",
            "Descargando el artículo: Salud mental\n",
            "Artículo 'Salud mental' descargado.\n",
            "Descargando el artículo: Juegos Olímpicos\n",
            "Artículo 'Juegos Olímpicos' descargado.\n",
            "Descargando el artículo: Arquitectura gótica\n",
            "Artículo 'Arquitectura gótica' descargado.\n",
            "Descargando el artículo: Ingeniería estructural\n",
            "Artículo 'Ingeniería estructural' descargado.\n",
            "Todos los artículos han sido descargados y guardados en 'corpus.txt'.\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import json\n",
        "\n",
        "articulos = [\n",
        "    \"Inteligencia artificial\", \"Aprendizaje automático\", \"Biotecnología\", \"Nanotecnología\", \"Genética\",\n",
        "    \"Tecnología de la información\", \"Robótica\", \"Revolución Industrial\", \"Segunda Guerra Mundial\", \"Edad Media\",\n",
        "    \"Guerra Fría\", \"Revolución francesa\", \"Historia de la ciencia\", \"Renacimiento\", \"Literatura en español\",\n",
        "    \"Pintura renacentista\", \"Música clásica\", \"Filosofía griega\", \"Arquitectura moderna\", \"Sociología\",\n",
        "    \"Antropología\", \"Psicología\", \"Parques nacionales de España\", \"Economía de mercado\", \"Globalización\",\n",
        "    \"Política internacional\", \"Corrupción política\", \"Economía de América Latina\", \"Nutrición humana\",\n",
        "    \"Genómica\", \"Epidemiología\", \"Neurociencia\", \"Psicología clínica\", \"Medicina tradicional\", \"Salud mental\",\n",
        "    \"Juegos Olímpicos\", \"Arquitectura gótica\", \"Ingeniería estructural\"\n",
        "]\n",
        "\n",
        "# URL de la API de Wikipedia para obtener el extracto en formato JSON\n",
        "base_url = 'https://es.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&explaintext&titles={}'\n",
        "\n",
        "# Archivo para guardar el corpus\n",
        "filename = 'corpus.txt'\n",
        "\n",
        "with open(filename, 'w', encoding='utf-8') as file:\n",
        "    for title in articulos:\n",
        "        \"\"\"\n",
        "        Descarga el contenido de artículos de Wikipedia y los guarda en un archivo de texto.\n",
        "\n",
        "        Args:\n",
        "            title (str): El título del artículo de Wikipedia a descargar.\n",
        "\n",
        "        Funcionalidad:\n",
        "            - Codifica el título del artículo para incluirlo en una URL de solicitud a la API de Wikipedia.\n",
        "            - Descarga el contenido del artículo en formato JSON y extrae el campo 'extract', que contiene el texto del artículo.\n",
        "            - Si el extracto no está vacío, lo escribe en el archivo especificado y confirma la descarga en consola.\n",
        "            - Si el extracto está vacío, indica que el artículo no se pudo descargar.\n",
        "\n",
        "        Output:\n",
        "            Archivo 'corpus.txt' con el contenido de todos los artículos descargados.\n",
        "        \"\"\"\n",
        "\n",
        "        # Codificar el título para URL\n",
        "        encoded_title = urllib.request.quote(title)\n",
        "        url = base_url.format(encoded_title)\n",
        "        print(f\"Descargando el artículo: {title}\")\n",
        "        response = urllib.request.urlopen(url)\n",
        "        data = response.read().decode('utf-8')\n",
        "\n",
        "        # Extraer el texto del JSON\n",
        "        json_data = json.loads(data)\n",
        "        pages = json_data['query']['pages']\n",
        "        page = next(iter(pages.values()))\n",
        "        extract = page.get('extract', '')\n",
        "\n",
        "        # Escribir el texto en el archivo si el extracto no está vacío\n",
        "        if extract:\n",
        "            file.write(extract + '\\n')\n",
        "            print(f\"Artículo '{title}' descargado.\")\n",
        "        else:\n",
        "            print(f\"No se pudo descargar el artículo '{title}'.\")\n",
        "\n",
        "print(f\"Todos los artículos han sido descargados y guardados en '{filename}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "M7vl-DxQDigc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iniciando el preprocesamiento del corpus\n",
            "Tamaño del vocabulario final: 4888 palabras\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Convierte el texto en minúsculas, remueve la puntuación y lo tokeniza en palabras.\n",
        "\n",
        "    Args:\n",
        "        text (str): Texto a procesar.\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de palabras (tokens) extraídas del texto.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "def stem(word):\n",
        "    \"\"\"\n",
        "    Aplica una reducción simple de sufijos comunes en español para el stemming.\n",
        "\n",
        "    Args:\n",
        "        word (str): Palabra a la que se le aplicará el stemming.\n",
        "\n",
        "    Returns:\n",
        "        str: La palabra sin sufijos comunes.\n",
        "    \"\"\"\n",
        "    suffixes = ['osos', 'osas', 'ismo', 'ismos', 'able', 'ables', 'ible', 'ibles',\n",
        "                'ente', 'entes', 'mente', 'aciones', 'imientos', 'amiento', 'ición', 'adora', 'ación', 'adoras', 'adores', 'ante',\n",
        "                'ancia', 'mente', 'idad', 'ivas', 'ivos', 'anza', 'icos', 'icas', 'ico', 'ica',\n",
        "                'oso', 'osa']\n",
        "    for suffix in suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            return word[:-len(suffix)]\n",
        "    return word\n",
        "\n",
        "stopwords = set([\n",
        "     'vuestro', 'vuestros', 'y', 'ya', 'yo', 'él', 'éramos','a', 'al', 'algo', 'algunas', 'algunos',\n",
        "     'ante', 'antes', 'como', 'con', 'contra', 'de', 'del', 'desde', 'donde', 'durante', 'e', 'el',\n",
        "     'ella', 'ellas', 'ellos', 'en', 'entre', 'era', 'erais', 'eran', 'eras', 'eres', 'es',\n",
        "     'esa', 'esas', 'ese', 'eso', 'esos', 'esta', 'estaba', 'estabais', 'estaban', 'estabas',\n",
        "    'vosotras', 'vosotros', 'vuestra', 'vuestras'\n",
        "])\n",
        "\n",
        "\n",
        "def preprocess_corpus(filename):\n",
        "    \"\"\"\n",
        "    Lee un archivo de texto, realiza tokenización, stemming, remueve stopwords y filtra palabras raras.\n",
        "\n",
        "    Args:\n",
        "        filename (str): Ruta del archivo de texto a procesar.\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de tokens procesados que cumplen con un umbral de frecuencia.\n",
        "    \"\"\"\n",
        "    print(\"Iniciando el preprocesamiento del corpus\")\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        word_counter = Counter()\n",
        "        final_tokens = []\n",
        "        for line in file:\n",
        "            tokens = tokenize(line)\n",
        "            stems = [stem(token) for token in tokens]\n",
        "            tokens_filtered = [word for word in stems if word not in stopwords]\n",
        "            word_counter.update(tokens_filtered)\n",
        "            final_tokens.extend(tokens_filtered)\n",
        "\n",
        "    # Filtrado de palabras raras\n",
        "    threshold = 5\n",
        "    frequent_words = {word for word, count in word_counter.items() if count >= threshold}\n",
        "    final_tokens = [word for word in final_tokens if word in frequent_words]\n",
        "\n",
        "    print(f\"Tamaño del vocabulario final: {len(set(final_tokens))} palabras\")\n",
        "    return final_tokens\n",
        "\n",
        "final_tokens = preprocess_corpus('corpus.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OAuSF_cNJz64"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "def build_vocab(tokens):\n",
        "    \"\"\"\n",
        "    Construye el vocabulario a partir de una lista de tokens.\n",
        "\n",
        "    Args:\n",
        "        tokens (list): Lista de palabras tokenizadas del corpus.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Un conjunto de palabras únicas (vocabulario), un diccionario que asigna un ID a cada palabra (word_to_id),\n",
        "               y un diccionario inverso de IDs a palabras (id_to_word).\n",
        "    \"\"\"\n",
        "    vocab = set(tokens)\n",
        "    word_to_id = {word: idx for idx, word in enumerate(vocab)}\n",
        "    id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
        "    return vocab, word_to_id, id_to_word\n",
        "\n",
        "def build_cooccurrence_matrix(tokens, vocab, word_to_id, window_size=5):\n",
        "    \"\"\"\n",
        "    Construye una matriz de co-ocurrencia de palabras utilizando una ventana de contexto.\n",
        "\n",
        "    Args:\n",
        "        tokens (list): Lista de palabras tokenizadas.\n",
        "        vocab (set): Conjunto de palabras únicas en el corpus.\n",
        "        word_to_id (dict): Diccionario que asigna un ID único a cada palabra del vocabulario.\n",
        "        window_size (int): Tamaño de la ventana de contexto para contar co-ocurrencias.\n",
        "\n",
        "    Returns:\n",
        "        defaultdict: Matriz de co-ocurrencia en formato de diccionario, donde las claves son pares (word_id, context_id)\n",
        "                     y los valores son las frecuencias ponderadas de co-ocurrencia.\n",
        "    \"\"\"\n",
        "    cooccurrences = defaultdict(float)\n",
        "    for i, word in enumerate(tokens):\n",
        "        word_id = word_to_id[word]\n",
        "        start = max(0, i - window_size)\n",
        "        end = min(len(tokens), i + window_size + 1)\n",
        "        for j in range(start, end):\n",
        "            if i != j:\n",
        "                context_word = tokens[j]\n",
        "                context_id = word_to_id[context_word]\n",
        "                distance = abs(i - j)\n",
        "                weight = 1.0 / distance  # Ponderación por distancia inversa\n",
        "                cooccurrences[(word_id, context_id)] += weight\n",
        "    return cooccurrences\n",
        "\n",
        "window_size = 5  # Tamaño de la ventana de contexto\n",
        "\n",
        "# Construcción del vocabulario\n",
        "vocab, word_to_id, id_to_word = build_vocab(final_tokens)\n",
        "\n",
        "# Construcción de la matriz de co-ocurrencia\n",
        "cooccurrence= build_cooccurrence_matrix(final_tokens, vocab, word_to_id, window_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JpgqNTnEKbv4"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def initialize_embeddings(vocab_size, embedding_dim=50):\n",
        "    \"\"\"\n",
        "    Inicializa matrices de embeddings y vectores de sesgo para el modelo.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): Número total de palabras en el vocabulario.\n",
        "        embedding_dim (int, opcional): Dimensionalidad de los vectores de embeddings.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Contiene las matrices de embeddings (W y W_tilde) y los vectores de sesgo (b y b_tilde).\n",
        "               - W (np.array): Matriz de embeddings para las palabras del vocabulario.\n",
        "               - W_tilde (np.array): Matriz de embeddings para las palabras en el contexto.\n",
        "               - b (np.array): Vector de sesgo para las palabras del vocabulario.\n",
        "               - b_tilde (np.array): Vector de sesgo para las palabras en el contexto.\n",
        "    \"\"\"\n",
        "    W = np.random.uniform(-0.5, 0.5, (vocab_size, embedding_dim))\n",
        "    W_tilde = np.random.uniform(-0.5, 0.5, (vocab_size, embedding_dim))\n",
        "    b = np.zeros(vocab_size)\n",
        "    b_tilde = np.zeros(vocab_size)\n",
        "    return W, W_tilde, b, b_tilde\n",
        "\n",
        "\n",
        "embedding_dim = 50  # Dimensión de los embeddings\n",
        "\n",
        "# Inicialización de embeddings y sesgos\n",
        "W, W_tilde, b, b_tilde = initialize_embeddings(len(vocab), embedding_dim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WA1jlJK3FM3i"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# Hiperparámetros\n",
        "X_max = 100\n",
        "alpha = 0.75\n",
        "learning_rate = 0.05\n",
        "epochs = 100 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ITOsdQyjFP1-"
      },
      "outputs": [],
      "source": [
        "def weighting_function(x_ij):\n",
        "    \"\"\"\n",
        "    Calcula un peso basado en la frecuencia de co-ocurrencia entre palabras,\n",
        "    aplicando una ponderación máxima para valores altos de frecuencia.\n",
        "\n",
        "    Args:\n",
        "        x_ij (float): Frecuencia de co-ocurrencia entre dos palabras.\n",
        "\n",
        "    Returns:\n",
        "        float: Peso ajustado en función de la frecuencia; es menor que 1 si\n",
        "               x_ij es menor que X_max y 1 en caso contrario.\n",
        "    \"\"\"\n",
        "    if x_ij < X_max:\n",
        "        return (x_ij / X_max) ** alpha\n",
        "    return 1.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pmLUl7eBLFkZ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import tracemalloc\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def train_embeddings(cooccurrences, W, W_tilde, b, b_tilde, epochs, learning_rate):\n",
        "    \"\"\"\n",
        "    Entrena los embeddings ajustando vectores de palabras y sesgos,\n",
        "    utilizando una función de ponderación en función de la frecuencia de co-ocurrencia.\n",
        "\n",
        "    Args:\n",
        "        cooccurrences (dict): Diccionario de co-ocurrencias con pares de palabras como claves y frecuencias como valores.\n",
        "        W (np.array): Matriz de embeddings de palabras.\n",
        "        W_tilde (np.array): Matriz de embeddings de contexto.\n",
        "        b (np.array): Vector de sesgos para las palabras.\n",
        "        b_tilde (np.array): Vector de sesgos para el contexto.\n",
        "        epochs (int): Número de iteraciones de entrenamiento.\n",
        "        learning_rate (float): Tasa de aprendizaje para la actualización de los parámetros.\n",
        "    \"\"\"\n",
        "    cooccurrence_items = list(cooccurrences.items())\n",
        "    inicio = time.time()\n",
        "    tracemalloc.start()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_cost = 0\n",
        "        random.shuffle(cooccurrence_items)\n",
        "\n",
        "        for (i, j), x_ij in cooccurrence_items:\n",
        "            # Calcula el peso f(x_ij)\n",
        "            f_ij = weighting_function(x_ij)\n",
        "\n",
        "            # Cálculo del producto punto y el costo para el par de palabras\n",
        "            w_i = W[i]\n",
        "            w_j = W_tilde[j]\n",
        "            bi = b[i]\n",
        "            bj = b_tilde[j]\n",
        "\n",
        "            dot_product = np.dot(w_i, w_j)\n",
        "            log_x_ij = math.log(x_ij)\n",
        "\n",
        "            # Costo de error para cada par\n",
        "            error = dot_product + bi + bj - log_x_ij\n",
        "            cost = f_ij * np.power(error, 2)\n",
        "            total_cost += 0.5 * cost\n",
        "\n",
        "            # Calcular gradiente\n",
        "            grad = f_ij * error\n",
        "            # Actualización de parámetros\n",
        "            W[i] -= learning_rate * grad * w_j\n",
        "            W_tilde[j] -= learning_rate * grad * w_i\n",
        "            b[i] -= learning_rate * grad\n",
        "            b_tilde[j] -= learning_rate * grad\n",
        "\n",
        "        average_cost = total_cost / len(cooccurrence_items)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Costo promedio: {average_cost}\")\n",
        "\n",
        "    fin = time.time()\n",
        "    current, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "\n",
        "    tiempo_ejecucion = fin - inicio\n",
        "    uso_memoria = peak / 10**6\n",
        "    print(f\"Tiempo total de ejecución: {tiempo_ejecucion} segundos\")\n",
        "    print(f\"Uso de memoria pico: {uso_memoria} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ke6GNoOTL_AP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100, Costo promedio: 0.01959478085628495\n",
            "Epoch 2/100, Costo promedio: 0.015277750936697995\n",
            "Epoch 3/100, Costo promedio: 0.013130331901944082\n",
            "Epoch 4/100, Costo promedio: 0.01173686852196658\n",
            "Epoch 5/100, Costo promedio: 0.010777448850583113\n",
            "Epoch 6/100, Costo promedio: 0.010068601194500822\n",
            "Epoch 7/100, Costo promedio: 0.009515412717139486\n",
            "Epoch 8/100, Costo promedio: 0.009074825120611512\n",
            "Epoch 9/100, Costo promedio: 0.008711276628052609\n",
            "Epoch 10/100, Costo promedio: 0.008401825756057324\n",
            "Epoch 11/100, Costo promedio: 0.008131627730565763\n",
            "Epoch 12/100, Costo promedio: 0.007894824078514167\n",
            "Epoch 13/100, Costo promedio: 0.00768236994657698\n",
            "Epoch 14/100, Costo promedio: 0.007493112190585492\n",
            "Epoch 15/100, Costo promedio: 0.007313902181939885\n",
            "Epoch 16/100, Costo promedio: 0.007151933590402577\n",
            "Epoch 17/100, Costo promedio: 0.0070050872181588635\n",
            "Epoch 18/100, Costo promedio: 0.006865083594179574\n",
            "Epoch 19/100, Costo promedio: 0.006736626340488856\n",
            "Epoch 20/100, Costo promedio: 0.006612259375701852\n",
            "Epoch 21/100, Costo promedio: 0.00649734832646753\n",
            "Epoch 22/100, Costo promedio: 0.006388195749342445\n",
            "Epoch 23/100, Costo promedio: 0.00628521883013055\n",
            "Epoch 24/100, Costo promedio: 0.006188116707640533\n",
            "Epoch 25/100, Costo promedio: 0.006091501801416796\n",
            "Epoch 26/100, Costo promedio: 0.0060049151384353175\n",
            "Epoch 27/100, Costo promedio: 0.0059201036288496515\n",
            "Epoch 28/100, Costo promedio: 0.005835599619374395\n",
            "Epoch 29/100, Costo promedio: 0.005758308265599935\n",
            "Epoch 30/100, Costo promedio: 0.0056836798574991364\n",
            "Epoch 31/100, Costo promedio: 0.005607951910016088\n",
            "Epoch 32/100, Costo promedio: 0.00553781694259304\n",
            "Epoch 33/100, Costo promedio: 0.005474135625980086\n",
            "Epoch 34/100, Costo promedio: 0.005405913262066156\n",
            "Epoch 35/100, Costo promedio: 0.005344483923565236\n",
            "Epoch 36/100, Costo promedio: 0.005283790894289462\n",
            "Epoch 37/100, Costo promedio: 0.005223597765065467\n",
            "Epoch 38/100, Costo promedio: 0.005166272754804746\n",
            "Epoch 39/100, Costo promedio: 0.005110113129620913\n",
            "Epoch 40/100, Costo promedio: 0.005058281927016528\n",
            "Epoch 41/100, Costo promedio: 0.005001992704619174\n",
            "Epoch 42/100, Costo promedio: 0.00495382386952118\n",
            "Epoch 43/100, Costo promedio: 0.0049056595174491\n",
            "Epoch 44/100, Costo promedio: 0.004857263544040072\n",
            "Epoch 45/100, Costo promedio: 0.004809375680045441\n",
            "Epoch 46/100, Costo promedio: 0.004765610512608936\n",
            "Epoch 47/100, Costo promedio: 0.004721015031779601\n",
            "Epoch 48/100, Costo promedio: 0.004678277439065487\n",
            "Epoch 49/100, Costo promedio: 0.004635279208738019\n",
            "Epoch 50/100, Costo promedio: 0.004596332617800761\n",
            "Epoch 51/100, Costo promedio: 0.004555670379562181\n",
            "Epoch 52/100, Costo promedio: 0.004516180751985817\n",
            "Epoch 53/100, Costo promedio: 0.004478700023423795\n",
            "Epoch 54/100, Costo promedio: 0.004442300931115147\n",
            "Epoch 55/100, Costo promedio: 0.004406112465281384\n",
            "Epoch 56/100, Costo promedio: 0.004369730939948138\n",
            "Epoch 57/100, Costo promedio: 0.0043343760612265456\n",
            "Epoch 58/100, Costo promedio: 0.004300889495633915\n",
            "Epoch 59/100, Costo promedio: 0.0042699447836071665\n",
            "Epoch 60/100, Costo promedio: 0.004236100123027184\n",
            "Epoch 61/100, Costo promedio: 0.0042030303021848535\n",
            "Epoch 62/100, Costo promedio: 0.004173574629933811\n",
            "Epoch 63/100, Costo promedio: 0.004142974423130118\n",
            "Epoch 64/100, Costo promedio: 0.004110021098903165\n",
            "Epoch 65/100, Costo promedio: 0.0040852113918486005\n",
            "Epoch 66/100, Costo promedio: 0.004056978919743715\n",
            "Epoch 67/100, Costo promedio: 0.004026993031269851\n",
            "Epoch 68/100, Costo promedio: 0.003998715738295528\n",
            "Epoch 69/100, Costo promedio: 0.00397434967930784\n",
            "Epoch 70/100, Costo promedio: 0.0039446912976117705\n",
            "Epoch 71/100, Costo promedio: 0.003920008845458232\n",
            "Epoch 72/100, Costo promedio: 0.0038964763134981565\n",
            "Epoch 73/100, Costo promedio: 0.003870589825351623\n",
            "Epoch 74/100, Costo promedio: 0.0038468927814222314\n",
            "Epoch 75/100, Costo promedio: 0.003821414829091157\n",
            "Epoch 76/100, Costo promedio: 0.0037971480388782446\n",
            "Epoch 77/100, Costo promedio: 0.003776436552584365\n",
            "Epoch 78/100, Costo promedio: 0.003753679143331174\n",
            "Epoch 79/100, Costo promedio: 0.0037312587856531453\n",
            "Epoch 80/100, Costo promedio: 0.0037092651007237425\n",
            "Epoch 81/100, Costo promedio: 0.00368734377643445\n",
            "Epoch 82/100, Costo promedio: 0.003667072247007625\n",
            "Epoch 83/100, Costo promedio: 0.003646406521619282\n",
            "Epoch 84/100, Costo promedio: 0.003625614299281573\n",
            "Epoch 85/100, Costo promedio: 0.0036060400686049265\n",
            "Epoch 86/100, Costo promedio: 0.003585517236316815\n",
            "Epoch 87/100, Costo promedio: 0.003565065658096188\n",
            "Epoch 88/100, Costo promedio: 0.003546686619440654\n",
            "Epoch 89/100, Costo promedio: 0.0035270319158931447\n",
            "Epoch 90/100, Costo promedio: 0.0035071691575885426\n",
            "Epoch 91/100, Costo promedio: 0.0034916654188086155\n",
            "Epoch 92/100, Costo promedio: 0.0034747323257895866\n",
            "Epoch 93/100, Costo promedio: 0.003454293741648521\n",
            "Epoch 94/100, Costo promedio: 0.0034358596637339687\n",
            "Epoch 95/100, Costo promedio: 0.0034232481977564885\n",
            "Epoch 96/100, Costo promedio: 0.003404269962849978\n",
            "Epoch 97/100, Costo promedio: 0.003386800993200849\n",
            "Epoch 98/100, Costo promedio: 0.003370242416208531\n",
            "Epoch 99/100, Costo promedio: 0.003355902630675994\n",
            "Epoch 100/100, Costo promedio: 0.003337323796176787\n",
            "Tiempo total de ejecución: 2011.3149185180664 segundos\n",
            "Uso de memoria pico: 0.025134 MB\n"
          ]
        }
      ],
      "source": [
        "train_embeddings(cooccurrence, W, W_tilde, b, b_tilde, epochs, learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he_ArqgMFx01"
      },
      "source": [
        "## Tareas de evaluación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0_SbPxojFxhC"
      },
      "outputs": [],
      "source": [
        "embeddings_glove_initial = W + W_tilde"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZzB5fRdUF4d3"
      },
      "outputs": [],
      "source": [
        "analogias = [\n",
        "    ('hombre', 'mujer', 'rey', 'reina')\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ROsQCln7F5Ub"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Todas las palabras de las analogías están en el vocabulario.\n"
          ]
        }
      ],
      "source": [
        "missing_words = set()\n",
        "for analogia in analogias:\n",
        "    for word in analogia:\n",
        "        if word not in word_to_id:\n",
        "            missing_words.add(word)\n",
        "\n",
        "if missing_words:\n",
        "    print(\"Las siguientes palabras no están en el vocabulario:\", missing_words)\n",
        "else:\n",
        "    print(\"Todas las palabras de las analogías están en el vocabulario.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oIHVDrTvGKOu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_analogies(analogies, embeddings, word_to_idx, idx_to_word, top_n=1):\n",
        "    correct = 0\n",
        "    total = len(analogies)\n",
        "\n",
        "    for word_a, word_b, word_c, word_d in analogies:\n",
        "        if word_a not in word_to_idx or word_b not in word_to_idx or word_c not in word_to_idx:\n",
        "            continue  # Saltar si alguna palabra no está en el vocabulario\n",
        "\n",
        "        vec_a = embeddings[word_to_idx[word_a]]\n",
        "        vec_b = embeddings[word_to_idx[word_b]]\n",
        "        vec_c = embeddings[word_to_idx[word_c]]\n",
        "\n",
        "        # Calcular el vector de analogía\n",
        "        analogy_vector = vec_b - vec_a + vec_c\n",
        "\n",
        "        # Calcular similitudes\n",
        "        similarities = embeddings @ analogy_vector\n",
        "        best_indices = np.argsort(-similarities)\n",
        "\n",
        "        # Excluir las palabras originales\n",
        "        best_indices = [idx for idx in best_indices if idx not in [word_to_idx[word_a], word_to_idx[word_b], word_to_idx[word_c]]]\n",
        "\n",
        "        predicted_word = idx_to_word[best_indices[0]]\n",
        "\n",
        "        if predicted_word == word_d:\n",
        "            correct += 1\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Exactitud en analogías: {accuracy * 100:.2f}% ({correct}/{total})\")\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "lqGFKgfzGR_u"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exactitud en analogías: 0.00% (0/1)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_analogies(analogias, embeddings_glove_initial, word_to_id, id_to_word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El valor de la exactitud de 0 en las analogías puede deberse a la falta de robustez de los corpus así como al ajuste de hiperparámetros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBzMO4iyHWv4"
      },
      "source": [
        "# Implementación mejorada de GloVe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xfbunpeSjCbJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "def build_vocab(tokens):\n",
        "    \"\"\"\n",
        "    Construye el vocabulario a partir de una lista de tokens.\n",
        "\n",
        "    Args:\n",
        "        tokens (list): Lista de palabras tokenizadas del corpus.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Un conjunto de palabras únicas (vocabulario), un diccionario que asigna un ID a cada palabra (word_to_id),\n",
        "               y un diccionario inverso de IDs a palabras (id_to_word).\n",
        "    \"\"\"\n",
        "    vocab = set(tokens)\n",
        "    word_to_id = {word: idx for idx, word in enumerate(vocab)}\n",
        "    id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
        "    return vocab, word_to_id, id_to_word\n",
        "\n",
        "def build_cooccurrence_matrix(tokens, vocab, word_to_id, window_size=5):\n",
        "    \"\"\"\n",
        "    Construye una matriz de co-ocurrencia de palabras utilizando una ventana de contexto.\n",
        "\n",
        "    Args:\n",
        "        tokens (list): Lista de palabras tokenizadas.\n",
        "        vocab (set): Conjunto de palabras únicas en el corpus.\n",
        "        word_to_id (dict): Diccionario que asigna un ID único a cada palabra del vocabulario.\n",
        "        window_size (int): Tamaño de la ventana de contexto para contar co-ocurrencias.\n",
        "\n",
        "    Returns:\n",
        "        defaultdict: Matriz de co-ocurrencia en formato de diccionario, donde las claves son pares (word_id, context_id)\n",
        "                     y los valores son las frecuencias ponderadas de co-ocurrencia.\n",
        "    \"\"\"\n",
        "    cooccurrences = defaultdict(float)\n",
        "    for i, word in enumerate(tokens):\n",
        "        word_id = word_to_id[word]\n",
        "        start = max(0, i - window_size)\n",
        "        end = min(len(tokens), i + window_size + 1)\n",
        "        for j in range(start, end):\n",
        "            if i != j:\n",
        "                context_word = tokens[j]\n",
        "                context_id = word_to_id[context_word]\n",
        "                distance = abs(i - j)\n",
        "                weight = 1.0 / distance  # Ponderación por distancia inversa\n",
        "                cooccurrences[(word_id, context_id)] += weight\n",
        "    return cooccurrences\n",
        "\n",
        "window_size = 5  # Tamaño de la ventana de contexto\n",
        "\n",
        "# Construcción del vocabulario\n",
        "vocab, word_to_id, id_to_word = build_vocab(final_tokens)\n",
        "\n",
        "# Construcción de la matriz de co-ocurrencia\n",
        "cooccurrence= build_cooccurrence_matrix(final_tokens, vocab, word_to_id, window_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "FGTwFaCsR9hQ"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import coo_matrix\n",
        "vocab_size = len(vocab)\n",
        "# Obtener listas de índices y valores para construir la matriz dispersa\n",
        "rows = []\n",
        "cols = []\n",
        "data = []\n",
        "\n",
        "for (i, j), x_ij in cooccurrence.items():\n",
        "    rows.append(i)\n",
        "    cols.append(j)\n",
        "    data.append(x_ij)\n",
        "\n",
        "# Crear la matriz de co-ocurrencia dispersa\n",
        "X = coo_matrix((data, (rows, cols)), shape=(vocab_size, vocab_size), dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "bITxV2_DSH8M"
      },
      "outputs": [],
      "source": [
        "def precompute_weights_and_logs(X, weighting_function):\n",
        "    \"\"\"\n",
        "    Precomputa los valores ponderados y los logaritmos de las co-ocurrencias,\n",
        "    optimizando el proceso de entrenamiento.\n",
        "\n",
        "    Args:\n",
        "        X (scipy.sparse.coo_matrix): Matriz de co-ocurrencias en formato disperso.\n",
        "        weighting_function (function): Función para calcular el peso de cada co-ocurrencia.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Dos arreglos de numpy, `f_X_data` y `log_X_data`, que contienen los valores\n",
        "               ponderados y logarítmicos de las co-ocurrencias en `X`.\n",
        "               - f_X_data (np.array): Peso f(X_{ij}) de cada co-ocurrencia.\n",
        "               - log_X_data (np.array): Logaritmo log(X_{ij}) de cada co-ocurrencia.\n",
        "    \"\"\"\n",
        "    f_X_data = np.zeros_like(X.data)\n",
        "    log_X_data = np.zeros_like(X.data)\n",
        "\n",
        "    for idx in range(len(X.data)):\n",
        "        x_ij = X.data[idx]\n",
        "        f_X_data[idx] = weighting_function(x_ij)\n",
        "        log_X_data[idx] = np.log(x_ij)\n",
        "\n",
        "    return f_X_data, log_X_data\n",
        "\n",
        "# Ejecución de precomputación\n",
        "f_X_data, log_X_data = precompute_weights_and_logs(X, weighting_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "9b5CVO_6Sh9H"
      },
      "outputs": [],
      "source": [
        "def initialize_vectors_and_biases(vocab_size, embedding_dim):\n",
        "    \"\"\"\n",
        "    Inicializa las matrices de embeddings y los vectores de sesgos para el modelo,\n",
        "    asignando valores aleatorios a los embeddings y ceros a los sesgos.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): Número de palabras en el vocabulario.\n",
        "        embedding_dim (int): Dimensionalidad de los vectores de embeddings.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Matrices de embeddings y vectores de sesgos inicializados.\n",
        "               - W (np.array): Matriz de embeddings para palabras, con valores aleatorios.\n",
        "               - W_tilde (np.array): Matriz de embeddings para contexto, con valores aleatorios.\n",
        "               - b (np.array): Vector de sesgo para palabras, inicializado en ceros.\n",
        "               - b_tilde (np.array): Vector de sesgo para contexto, inicializado en ceros.\n",
        "    \"\"\"\n",
        "    W = np.random.uniform(-0.5, 0.5, (vocab_size, embedding_dim)).astype(np.float32)\n",
        "    W_tilde = np.random.uniform(-0.5, 0.5, (vocab_size, embedding_dim)).astype(np.float32)\n",
        "    b = np.zeros(vocab_size, dtype=np.float32)\n",
        "    b_tilde = np.zeros(vocab_size, dtype=np.float32)\n",
        "\n",
        "    return W, W_tilde, b, b_tilde\n",
        "\n",
        "# Ejecución de la inicialización\n",
        "W, W_tilde, b, b_tilde = initialize_vectors_and_biases(vocab_size, embedding_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "mZoJ_4KeSrKq"
      },
      "outputs": [],
      "source": [
        "def initialize_gradient_squares(vocab_size, embedding_dim):\n",
        "    \"\"\"\n",
        "    Inicializa los acumuladores de cuadrados de gradiente para las matrices de embeddings\n",
        "    y los vectores de sesgos, necesarios para optimización adaptativa.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): Número de palabras en el vocabulario.\n",
        "        embedding_dim (int): Dimensionalidad de los vectores de embeddings.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Acumuladores de cuadrados de gradiente inicializados con unos.\n",
        "               - gradsq_W (np.array): Acumulador para los gradientes de embeddings de palabras.\n",
        "               - gradsq_W_tilde (np.array): Acumulador para los gradientes de embeddings de contexto.\n",
        "               - gradsq_b (np.array): Acumulador para los gradientes de sesgo de palabras.\n",
        "               - gradsq_b_tilde (np.array): Acumulador para los gradientes de sesgo de contexto.\n",
        "    \"\"\"\n",
        "    gradsq_W = np.ones((vocab_size, embedding_dim), dtype=np.float32)\n",
        "    gradsq_W_tilde = np.ones((vocab_size, embedding_dim), dtype=np.float32)\n",
        "    gradsq_b = np.ones(vocab_size, dtype=np.float32)\n",
        "    gradsq_b_tilde = np.ones(vocab_size, dtype=np.float32)\n",
        "\n",
        "    return gradsq_W, gradsq_W_tilde, gradsq_b, gradsq_b_tilde\n",
        "\n",
        "# Ejecución de la inicialización de acumuladores de gradiente\n",
        "gradsq_W, gradsq_W_tilde, gradsq_b, gradsq_b_tilde = initialize_gradient_squares(vocab_size, embedding_dim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ASpOFEiYS3Y3"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import tracemalloc\n",
        "\n",
        "def train_model_with_batches(X, f_X_data, log_X_data, W, W_tilde, b, b_tilde, gradsq_W, gradsq_W_tilde, gradsq_b, gradsq_b_tilde, epochs, learning_rate, batch_size=2048):\n",
        "    \"\"\"\n",
        "    Entrena el modelo de embeddings en múltiples épocas, dividiendo los datos en lotes para optimizar la actualización\n",
        "    de parámetros. Utiliza AdaGrad para una optimización adaptativa si los acumuladores de gradiente están definidos.\n",
        "\n",
        "    Args:\n",
        "        X (scipy.sparse.coo_matrix): Matriz dispersa de co-ocurrencias.\n",
        "        f_X_data (np.array): Pesos precomputados para cada co-ocurrencia.\n",
        "        log_X_data (np.array): Logaritmos precomputados de cada co-ocurrencia.\n",
        "        W (np.array): Matriz de embeddings de palabras.\n",
        "        W_tilde (np.array): Matriz de embeddings de contexto.\n",
        "        b (np.array): Vector de sesgos para palabras.\n",
        "        b_tilde (np.array): Vector de sesgos para contexto.\n",
        "        gradsq_W (np.array): Acumulador de gradientes de `W` para AdaGrad.\n",
        "        gradsq_W_tilde (np.array): Acumulador de gradientes de `W_tilde` para AdaGrad.\n",
        "        gradsq_b (np.array): Acumulador de gradientes de `b` para AdaGrad.\n",
        "        gradsq_b_tilde (np.array): Acumulador de gradientes de `b_tilde` para AdaGrad.\n",
        "        epochs (int): Número total de épocas para el entrenamiento.\n",
        "        learning_rate (float): Tasa de aprendizaje.\n",
        "        batch_size (int, opcional): Tamaño de cada lote para la actualización.\n",
        "    \"\"\"\n",
        "    inicio = time.time()\n",
        "    tracemalloc.start()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_cost = 0\n",
        "        permutation = np.random.permutation(len(X.data))\n",
        "        X_data_shuffled = X.data[permutation]\n",
        "        f_X_data_shuffled = f_X_data[permutation]\n",
        "        log_X_data_shuffled = log_X_data[permutation]\n",
        "        row_indices = X.row[permutation]\n",
        "        col_indices = X.col[permutation]\n",
        "\n",
        "        num_batches = int(np.ceil(len(X_data_shuffled) / batch_size))\n",
        "\n",
        "        for batch_idx in range(num_batches):\n",
        "            start = batch_idx * batch_size\n",
        "            end = min((batch_idx + 1) * batch_size, len(X_data_shuffled))\n",
        "\n",
        "            i_s = row_indices[start:end]\n",
        "            j_s = col_indices[start:end]\n",
        "            x_ijs = X_data_shuffled[start:end]\n",
        "            f_ijs = f_X_data_shuffled[start:end]\n",
        "            log_x_ijs = log_X_data_shuffled[start:end]\n",
        "\n",
        "            w_i = W[i_s]\n",
        "            w_j = W_tilde[j_s]\n",
        "            b_i = b[i_s]\n",
        "            b_j = b_tilde[j_s]\n",
        "\n",
        "            dot_products = np.sum(w_i * w_j, axis=1)\n",
        "            errors = (dot_products + b_i + b_j - log_x_ijs)\n",
        "            costs = f_ijs * errors ** 2\n",
        "            total_cost += 0.5 * np.sum(costs)\n",
        "\n",
        "            grad = f_ijs * errors\n",
        "\n",
        "            grad_w_i = grad[:, np.newaxis] * w_j\n",
        "            grad_w_j = grad[:, np.newaxis] * w_i\n",
        "\n",
        "            if gradsq_W is not None:\n",
        "                gradsq_W[i_s] += grad_w_i ** 2\n",
        "                gradsq_W_tilde[j_s] += grad_w_j ** 2\n",
        "                gradsq_b[i_s] += grad ** 2\n",
        "                gradsq_b_tilde[j_s] += grad ** 2\n",
        "\n",
        "                W[i_s] -= (learning_rate / np.sqrt(gradsq_W[i_s])) * grad_w_i\n",
        "                W_tilde[j_s] -= (learning_rate / np.sqrt(gradsq_W_tilde[j_s])) * grad_w_j\n",
        "                b[i_s] -= (learning_rate / np.sqrt(gradsq_b[i_s])) * grad\n",
        "                b_tilde[j_s] -= (learning_rate / np.sqrt(gradsq_b_tilde[j_s])) * grad\n",
        "            else:\n",
        "                W[i_s] -= learning_rate * grad_w_i\n",
        "                W_tilde[j_s] -= learning_rate * grad_w_j\n",
        "                b[i_s] -= learning_rate * grad\n",
        "                b_tilde[j_s] -= learning_rate * grad\n",
        "\n",
        "        average_cost = total_cost / len(X.data)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Costo promedio: {average_cost}\")\n",
        "\n",
        "    current, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "    fin = time.time()\n",
        "    tiempo_ejecucion = fin - inicio\n",
        "    uso_memoria = peak / 10**6\n",
        "    print(f\"Tiempo total de ejecución: {tiempo_ejecucion} segundos\")\n",
        "    print(f\"Uso de memoria pico: {uso_memoria} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "uta0yrL5jyHK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100, Costo promedio: 0.02945303899735189\n",
            "Epoch 2/100, Costo promedio: 0.02456833467961881\n",
            "Epoch 3/100, Costo promedio: 0.02178615036928054\n",
            "Epoch 4/100, Costo promedio: 0.019753966248337813\n",
            "Epoch 5/100, Costo promedio: 0.018119247065061544\n",
            "Epoch 6/100, Costo promedio: 0.016720278399914468\n",
            "Epoch 7/100, Costo promedio: 0.015510732433338783\n",
            "Epoch 8/100, Costo promedio: 0.014544588550946272\n",
            "Epoch 9/100, Costo promedio: 0.013661633414290605\n",
            "Epoch 10/100, Costo promedio: 0.012914699356060837\n",
            "Epoch 11/100, Costo promedio: 0.012259993639432278\n",
            "Epoch 12/100, Costo promedio: 0.011670634573155854\n",
            "Epoch 13/100, Costo promedio: 0.01114636960560641\n",
            "Epoch 14/100, Costo promedio: 0.01067463420399928\n",
            "Epoch 15/100, Costo promedio: 0.01026386065948475\n",
            "Epoch 16/100, Costo promedio: 0.009885646283670637\n",
            "Epoch 17/100, Costo promedio: 0.009554518201733786\n",
            "Epoch 18/100, Costo promedio: 0.009258546024394377\n",
            "Epoch 19/100, Costo promedio: 0.008972534924792594\n",
            "Epoch 20/100, Costo promedio: 0.008706704087350855\n",
            "Epoch 21/100, Costo promedio: 0.00848071172659279\n",
            "Epoch 22/100, Costo promedio: 0.008276553681904142\n",
            "Epoch 23/100, Costo promedio: 0.00808475535007957\n",
            "Epoch 24/100, Costo promedio: 0.007915252779822926\n",
            "Epoch 25/100, Costo promedio: 0.007751624228881405\n",
            "Epoch 26/100, Costo promedio: 0.007596926127788814\n",
            "Epoch 27/100, Costo promedio: 0.00745942792192184\n",
            "Epoch 28/100, Costo promedio: 0.007328789271298114\n",
            "Epoch 29/100, Costo promedio: 0.007207227408899134\n",
            "Epoch 30/100, Costo promedio: 0.007091942056425582\n",
            "Epoch 31/100, Costo promedio: 0.006986930419573796\n",
            "Epoch 32/100, Costo promedio: 0.006890537483205501\n",
            "Epoch 33/100, Costo promedio: 0.006794736584226047\n",
            "Epoch 34/100, Costo promedio: 0.0067050607645370115\n",
            "Epoch 35/100, Costo promedio: 0.006617987490653912\n",
            "Epoch 36/100, Costo promedio: 0.006538327101761853\n",
            "Epoch 37/100, Costo promedio: 0.006460614685057547\n",
            "Epoch 38/100, Costo promedio: 0.006386963611357634\n",
            "Epoch 39/100, Costo promedio: 0.006314494387866686\n",
            "Epoch 40/100, Costo promedio: 0.006244355554067572\n",
            "Epoch 41/100, Costo promedio: 0.0061785820984491206\n",
            "Epoch 42/100, Costo promedio: 0.006114433895864391\n",
            "Epoch 43/100, Costo promedio: 0.006054071640203229\n",
            "Epoch 44/100, Costo promedio: 0.005994288223574544\n",
            "Epoch 45/100, Costo promedio: 0.005936618593301319\n",
            "Epoch 46/100, Costo promedio: 0.005880747750235513\n",
            "Epoch 47/100, Costo promedio: 0.00582640022994871\n",
            "Epoch 48/100, Costo promedio: 0.00577461823942971\n",
            "Epoch 49/100, Costo promedio: 0.005724351946934093\n",
            "Epoch 50/100, Costo promedio: 0.005675444668878109\n",
            "Epoch 51/100, Costo promedio: 0.005628236222220435\n",
            "Epoch 52/100, Costo promedio: 0.005581256215029811\n",
            "Epoch 53/100, Costo promedio: 0.00553544329678594\n",
            "Epoch 54/100, Costo promedio: 0.0054922191101362035\n",
            "Epoch 55/100, Costo promedio: 0.005449022734412679\n",
            "Epoch 56/100, Costo promedio: 0.0054074548087905796\n",
            "Epoch 57/100, Costo promedio: 0.005366012458418232\n",
            "Epoch 58/100, Costo promedio: 0.005325021947435698\n",
            "Epoch 59/100, Costo promedio: 0.00528547766896469\n",
            "Epoch 60/100, Costo promedio: 0.005246261971544419\n",
            "Epoch 61/100, Costo promedio: 0.0052078567933175005\n",
            "Epoch 62/100, Costo promedio: 0.005173004859184243\n",
            "Epoch 63/100, Costo promedio: 0.005136846092907539\n",
            "Epoch 64/100, Costo promedio: 0.005101437210603147\n",
            "Epoch 65/100, Costo promedio: 0.005066841964215035\n",
            "Epoch 66/100, Costo promedio: 0.005033149273233885\n",
            "Epoch 67/100, Costo promedio: 0.005000441076970728\n",
            "Epoch 68/100, Costo promedio: 0.004967845488455857\n",
            "Epoch 69/100, Costo promedio: 0.004935124860928995\n",
            "Epoch 70/100, Costo promedio: 0.004904877977806532\n",
            "Epoch 71/100, Costo promedio: 0.0048743077749437485\n",
            "Epoch 72/100, Costo promedio: 0.004843457920271857\n",
            "Epoch 73/100, Costo promedio: 0.004814242555010996\n",
            "Epoch 74/100, Costo promedio: 0.004785265015377517\n",
            "Epoch 75/100, Costo promedio: 0.004757326527109954\n",
            "Epoch 76/100, Costo promedio: 0.00472909104692188\n",
            "Epoch 77/100, Costo promedio: 0.004701653085160776\n",
            "Epoch 78/100, Costo promedio: 0.004674941695571467\n",
            "Epoch 79/100, Costo promedio: 0.004648147122054198\n",
            "Epoch 80/100, Costo promedio: 0.004622423703275994\n",
            "Epoch 81/100, Costo promedio: 0.004596650471827175\n",
            "Epoch 82/100, Costo promedio: 0.004571996047386076\n",
            "Epoch 83/100, Costo promedio: 0.004547221246111314\n",
            "Epoch 84/100, Costo promedio: 0.00452273065666615\n",
            "Epoch 85/100, Costo promedio: 0.0044991362023168715\n",
            "Epoch 86/100, Costo promedio: 0.004475961267489622\n",
            "Epoch 87/100, Costo promedio: 0.004452382483098342\n",
            "Epoch 88/100, Costo promedio: 0.0044295445986064766\n",
            "Epoch 89/100, Costo promedio: 0.004407009440143644\n",
            "Epoch 90/100, Costo promedio: 0.004385488048981904\n",
            "Epoch 91/100, Costo promedio: 0.00436380869308261\n",
            "Epoch 92/100, Costo promedio: 0.004341962315662658\n",
            "Epoch 93/100, Costo promedio: 0.004321016642084305\n",
            "Epoch 94/100, Costo promedio: 0.004300367457090069\n",
            "Epoch 95/100, Costo promedio: 0.004278880381509457\n",
            "Epoch 96/100, Costo promedio: 0.0042586775404677615\n",
            "Epoch 97/100, Costo promedio: 0.0042382350613150545\n",
            "Epoch 98/100, Costo promedio: 0.004218892483244406\n",
            "Epoch 99/100, Costo promedio: 0.004199313440863186\n",
            "Epoch 100/100, Costo promedio: 0.004180667001926363\n",
            "Tiempo total de ejecución: 46.91589140892029 segundos\n",
            "Uso de memoria pico: 29.312214 MB\n"
          ]
        }
      ],
      "source": [
        "train_model_with_batches(X, f_X_data, log_X_data, W, W_tilde, b, b_tilde, gradsq_W, gradsq_W_tilde, gradsq_b, gradsq_b_tilde, epochs, learning_rate, batch_size=2048)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
