{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTnWIB4JMNkh"
      },
      "source": [
        "Nombres y apellidos: Fiorella Meza Rodriguez <br>\n",
        "CÃ³digo: 20192730G"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHnMc8PYxtA5"
      },
      "source": [
        "# Parte 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6qTZKVNM-k3"
      },
      "source": [
        "## Ejercicio 0.1.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1g7hBPURCO6"
      },
      "source": [
        "Dadas tres oraciones \"all models are wrong\", a model is wrong, y some models are useful y un vocabulario [\"<s>\", \"</s>\", 'a', 'all', 'are', 'model', 'models',\n",
        "               'some', 'useful', 'wrong']. En codigo responda las siguientes preguntas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yLShdilRWa0"
      },
      "source": [
        "### a.Calcule las probabilidades de todos los bigramas sin suavizado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aflN8zojW4gx"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "\n",
        "corpus = [ \"all models are wrong\", \"a model is wrong\",\n",
        "            \"some models are useful\"]\n",
        "\n",
        "vocabulario = ['<s>', '</s>', 'a', 'all', 'are', 'model', 'models',\n",
        "               'some', 'useful', 'wrong']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6s2Tth3W_B-",
        "outputId": "0f37bebd-089f-43ff-edca-47cf74fc2437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probabilidades de bigramas sin suavizado:\n",
            "P('models' | 'all') = 1.000\n",
            "P('are' | 'models') = 1.000\n",
            "P('wrong' | 'are') = 0.500\n",
            "P('model' | 'a') = 1.000\n",
            "P('is' | 'model') = 1.000\n",
            "P('wrong' | 'is') = 1.000\n",
            "P('models' | 'some') = 1.000\n",
            "P('useful' | 'are') = 0.500\n"
          ]
        }
      ],
      "source": [
        "# Inicializamos los conteos\n",
        "bigram_counts = defaultdict(int)\n",
        "unigram_counts = defaultdict(int)\n",
        "\n",
        "# Poblamos los conteos\n",
        "for sentence in corpus:\n",
        "    words = sentence.split()\n",
        "    for i in range(len(words) - 1):\n",
        "        bigram_counts[(words[i], words[i+1])] += 1\n",
        "        unigram_counts[words[i]] += 1\n",
        "    unigram_counts[words[-1]] += 1\n",
        "\n",
        "\n",
        "## Probabilidad de los bigramas\n",
        "def bigram_probability(w1, w2):\n",
        "    return bigram_counts[(w1, w2)] / unigram_counts[w1] if unigram_counts[w1] > 0 else 0\n",
        "\n",
        "\n",
        "print(\"Probabilidades de bigramas sin suavizado:\")\n",
        "for i in bigram_counts:\n",
        "  prob = bigram_probability(i[0],i[1])\n",
        "  print(f\"P('{i[1]}' | '{i[0]}') = {prob:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deJJ9B5JSEVF"
      },
      "source": [
        "### b. Calcule todas las probabilidades de todos los bigramas y el brigrama no visto 'a models' con suavizado add-one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQGzul5hm794"
      },
      "source": [
        "Referencia: https://github.com/kapumota/Actividades-CC0C2/blob/3d0cde28776954bfc6367f2b94725dffcfaf8f38/Cuadernos-CC0C2/Clase2/Counts-backoff-suavizado.ipynb#L113"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dOqHI6jPO2z2"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "bigram_counts = defaultdict(int)\n",
        "unigram_counts = defaultdict(int)\n",
        "\n",
        "corpus = [\"all models are wrong\", \"a model is wrong\",\n",
        "            \"some models are useful\"]\n",
        "\n",
        "# Poblamos los conteos\n",
        "for sentence in corpus:\n",
        "    words = sentence.split()\n",
        "    for i in range(len(words) - 1):\n",
        "        bigram_counts[(words[i], words[i+1])] += 1\n",
        "        unigram_counts[words[i]] += 1\n",
        "    unigram_counts[words[-1]] += 1\n",
        "\n",
        "\n",
        "beta = 1\n",
        "vocab_size = len(vocabulario)\n",
        "\n",
        "def laplace_smoothing_bigram_probability(w1, w2, beta, vocab_size):\n",
        "    bigram_prob = (bigram_counts[(w1, w2)] + beta) / (unigram_counts[w1] + beta * vocab_size)\n",
        "    return bigram_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63rBn8FBYeiX"
      },
      "source": [
        "Ahora calcularemos de todos los bigramas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChdckeNQYg_N",
        "outputId": "d1cb0601-d72c-4bda-a23d-57eba618d447"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La probabilidad de los bigramas con suavizado de laplace:\n",
            "P('models' | 'all') = 0.1818\n",
            "P('are' | 'models') = 0.2500\n",
            "P('wrong' | 'are') = 0.1667\n",
            "P('model' | 'a') = 0.1818\n",
            "P('is' | 'model') = 0.1818\n",
            "P('wrong' | 'is') = 0.1818\n",
            "P('models' | 'some') = 0.1818\n",
            "P('useful' | 'are') = 0.1667\n"
          ]
        }
      ],
      "source": [
        "print(f\"La probabilidad de los bigramas con suavizado de laplace:\")\n",
        "for bigram in bigram_counts:\n",
        "  prob = laplace_smoothing_bigram_probability(bigram[0],bigram[1], beta, vocab_size)\n",
        "  print(f\"P('{bigram[1]}' | '{bigram[0]}') = {prob:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bH-hirVfy9Rr",
        "outputId": "24ee2aba-6022-46d6-dfe3-38779f2b123a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La probabilidad del bigrama 'a models' con suavizado de Laplace es:\n",
            "P('models' | 'a') = 0.0909\n"
          ]
        }
      ],
      "source": [
        "prob = laplace_smoothing_bigram_probability(\"a\", \"models\", beta, vocab_size)\n",
        "print(f\"La probabilidad del bigrama 'a models' con suavizado de Laplace es:\")\n",
        "print(f\"P('models' | 'a') = {prob:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WotfA-QRTSmk"
      },
      "source": [
        "### C. Calcule la probabilidad de todos los bigramas y el bigrama no visto \"a models\" con suavizado add-k. Pruebe con k=00.5 y k=0.15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGnBsvLjnD2t"
      },
      "source": [
        "Referencia: https://github.com/kapumota/Actividades-CC0C2/blob/3d0cde28776954bfc6367f2b94725dffcfaf8f38/Cuadernos-CC0C2/Clase2/Modelos-lenguaje2.ipynb#L12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8h_c06APThMr",
        "outputId": "7fede1c9-66f9-4d3a-c519-6f2eb5e0d4f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Probabilidades de bigramas suavizadas con Add-k (k=0.05):\n",
            "P(models | all) = 0.7241\n",
            "P(are | models) = 0.8367\n",
            "P(wrong | are) = 0.4286\n",
            "P(model | a) = 0.7241\n",
            "P(is | model) = 0.7241\n",
            "P(wrong | is) = 0.7241\n",
            "P(models | some) = 0.7241\n",
            "P(useful | are) = 0.4286\n",
            "P(<UNK> | <UNK>) = 0.0006\n",
            "\n",
            "Probabilidades de bigramas suavizadas con Add-k (k=0.15):\n",
            "P(models | all) = 0.4894\n",
            "P(are | models) = 0.6418\n",
            "P(wrong | are) = 0.3433\n",
            "P(model | a) = 0.4894\n",
            "P(is | model) = 0.4894\n",
            "P(wrong | is) = 0.4894\n",
            "P(models | some) = 0.4894\n",
            "P(useful | are) = 0.3433\n",
            "P(<UNK> | <UNK>) = 0.0018\n"
          ]
        }
      ],
      "source": [
        "bigram_counts = defaultdict(int)\n",
        "unigram_counts = defaultdict(int)\n",
        "\n",
        "#Suavizado add-k para bigrama\n",
        "def add_k_smoothing_bigram(corpus, k):\n",
        "    # Conteo de bigramas y unigrams\n",
        "    bigram_counts = {}\n",
        "    unigram_counts = {}\n",
        "\n",
        "    # Construir bigramas y contar unigramas\n",
        "    for sentence in corpus:\n",
        "        words = sentence.split()\n",
        "        for i in range(len(words)-1):\n",
        "            bigram = (words[i], words[i + 1])\n",
        "            unigram = words[i]\n",
        "\n",
        "            # Conteo de bigramas\n",
        "            if bigram in bigram_counts:\n",
        "                bigram_counts[bigram] += 1\n",
        "            else:\n",
        "                bigram_counts[bigram] = 1\n",
        "\n",
        "            # Conteo de unigrams\n",
        "            if unigram in unigram_counts:\n",
        "                unigram_counts[unigram] += 1\n",
        "            else:\n",
        "                unigram_counts[unigram] = 1\n",
        "\n",
        "        # Contar el Ãºltimo unigrama de la oraciÃ³n\n",
        "        last_word = words[-1]\n",
        "        if last_word in unigram_counts:\n",
        "            unigram_counts[last_word] += 1\n",
        "        else:\n",
        "            unigram_counts[last_word] = 1\n",
        "\n",
        "    # TamaÃ±o del vocabulario\n",
        "    V = len(unigram_counts)\n",
        "\n",
        "    # CÃ¡lculo de las probabilidades suavizadas para bigramas\n",
        "    add_k_probabilities = {}\n",
        "    for bigram, bigram_count in bigram_counts.items():\n",
        "        w_n_1 = bigram[0]  # w_{n-1}\n",
        "        # Aplicando la ecuaciÃ³n P_Add-k(w_n | w_{n-1}) = (C(w_{n-1}w_n) + k) / (C(w_{n-1}) + kV)\n",
        "        add_k_probabilities[bigram] = (bigram_count + k) / (unigram_counts[w_n_1] + k * V)\n",
        "\n",
        "    # Probabilidad para un bigrama no visto\n",
        "    add_k_probabilities[('<UNK>', '<UNK>')] = k / (V * (V + k))\n",
        "\n",
        "    return add_k_probabilities\n",
        "\n",
        "\n",
        "k = 0.05\n",
        "add_k_prob_bigrams = add_k_smoothing_bigram(corpus, k)\n",
        "print(\"\\nProbabilidades de bigramas suavizadas con Add-k (k=0.05):\")\n",
        "for bigram, prob in add_k_prob_bigrams.items():\n",
        "    print(f\"P({bigram[1]} | {bigram[0]}) = {prob:.4f}\")\n",
        "\n",
        "k = 0.15\n",
        "add_k_prob_bigrams = add_k_smoothing_bigram(corpus, k)\n",
        "print(\"\\nProbabilidades de bigramas suavizadas con Add-k (k=0.15):\")\n",
        "for bigram, prob in add_k_prob_bigrams.items():\n",
        "    print(f\"P({bigram[1]} | {bigram[0]}) = {prob:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZEGdjAkuV3w"
      },
      "source": [
        "En la salida anterior se observa que para la probabilidad para k= 0.05, el valor de P( models | a) es 0.0006 y para k=0.15 el valor de\n",
        "P(models | a) = 0.0018."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUW6VE3lZ5RS"
      },
      "source": [
        "### d. Calcule las probabilidades de todos los bigramas y el bigrama no visto a models con back-off y stupid-off:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyrtijcTofkG"
      },
      "source": [
        "Referencia: https://github.com/kapumota/Actividades-CC0C2/blob/3d0cde28776954bfc6367f2b94725dffcfaf8f38/Cuadernos-CC0C2/Clase2/Counts-backoff-suavizado.ipynb#L171"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLlAzbPgzmZ4",
        "outputId": "a15ad7b3-8ded-4b65-e6a0-8f18655126c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probabilidades de bigramas con back-off:\n",
            "P('wrong' | 'are') = 0.5000\n",
            "P('model' | 'a') = 1.0000\n",
            "P('</s>' | 'useful') = 1.0000\n",
            "P('are' | 'models') = 1.0000\n",
            "P('wrong' | 'is') = 1.0000\n",
            "P('some' | '<s>') = 0.3333\n",
            "P('a' | '<s>') = 0.3333\n",
            "P('useful' | 'are') = 0.5000\n",
            "P('is' | 'model') = 1.0000\n",
            "P('all' | '<s>') = 0.3333\n",
            "P('models' | 'some') = 1.0000\n",
            "P('models' | 'all') = 1.0000\n",
            "P('</s>' | 'wrong') = 1.0000\n",
            "\n",
            "P('models' | 'a') con back-off = 0.1333\n",
            "\n",
            "Probabilidades de bigramas con stupid-backoff:\n",
            "P('wrong' | 'are') = 0.5000\n",
            "P('model' | 'a') = 1.0000\n",
            "P('</s>' | 'useful') = 1.0000\n",
            "P('are' | 'models') = 1.0000\n",
            "P('wrong' | 'is') = 1.0000\n",
            "P('some' | '<s>') = 0.3333\n",
            "P('a' | '<s>') = 0.3333\n",
            "P('useful' | 'are') = 0.5000\n",
            "P('is' | 'model') = 1.0000\n",
            "P('all' | '<s>') = 0.3333\n",
            "P('models' | 'some') = 1.0000\n",
            "P('models' | 'all') = 1.0000\n",
            "P('</s>' | 'wrong') = 1.0000\n",
            "\n",
            "P('models' | 'a') con stupid-backoff = 0.0533\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import collections\n",
        "from typing import List, Tuple\n",
        "\n",
        "# El vocabulario y el corpus proporcionados\n",
        "vocabulario = ['<s>', '</s>', 'a', 'all', 'are', 'model', 'models', 'some', 'useful', 'wrong']\n",
        "corpus = [\"all models are wrong\", \"a model is wrong\", \"some models are useful\"]\n",
        "\n",
        "# Convertimos el corpus en una lista de listas de tokens\n",
        "train_corpus = [sentence.split() for sentence in corpus]\n",
        "\n",
        "# ImplementaciÃ³n de modelos N-grama\n",
        "class NGramModel:\n",
        "    def __init__(self, n: int):\n",
        "        self.n = n\n",
        "        self.ngram_counts = collections.Counter()\n",
        "        self.context_counts = collections.Counter()\n",
        "        self.vocab = set()\n",
        "        self.total_ngrams = 0\n",
        "\n",
        "    def train(self, corpus: List[List[str]]):\n",
        "        for document in corpus:\n",
        "            tokens = ['<s>'] * (self.n - 1) + document + ['</s>']\n",
        "            self.vocab.update(tokens)\n",
        "            for i in range(len(tokens) - self.n + 1):\n",
        "                ngram = tuple(tokens[i:i + self.n])\n",
        "                context = tuple(tokens[i:i + self.n - 1])\n",
        "                self.ngram_counts[ngram] += 1\n",
        "                self.context_counts[context] += 1\n",
        "                self.total_ngrams += 1\n",
        "\n",
        "    def get_ngram_prob(self, ngram: Tuple[str, ...]) -> float:\n",
        "        count = self.ngram_counts.get(ngram, 0)\n",
        "        context = ngram[:-1]\n",
        "        context_count = self.context_counts.get(context, 0)\n",
        "        if context_count == 0:\n",
        "            return 0.0\n",
        "        else:\n",
        "            return count / context_count\n",
        "\n",
        "# ImplementaciÃ³n de Backoff EstÃ¡ndar\n",
        "class BackoffNGramModel(NGramModel):\n",
        "    def __init__(self, n: int, models: List[NGramModel]):\n",
        "        super().__init__(n)\n",
        "        self.models = models  # Lista de modelos de diferentes Ã³rdenes, ordenados de mayor a menor\n",
        "        # Actualizamos self.vocab con la uniÃ³n de los vocabularios de los modelos\n",
        "        self.vocab = set()\n",
        "        for model in self.models:\n",
        "            self.vocab.update(model.vocab)\n",
        "\n",
        "    def get_ngram_prob(self, ngram: Tuple[str, ...]) -> float:\n",
        "        for model in self.models:\n",
        "            ngram_adjusted = ngram[-model.n:]\n",
        "            prob = model.get_ngram_prob(ngram_adjusted)\n",
        "            if prob > 0:\n",
        "                return prob\n",
        "        # Si ningÃºn modelo tiene el n-grama, retrocedemos al unigrama\n",
        "        return 0.0\n",
        "\n",
        "# ImplementaciÃ³n del Stupid Backoff\n",
        "class StupidBackoffNGramModel(NGramModel):\n",
        "    def __init__(self, n: int, models: List[NGramModel], alpha: float = 0.4):\n",
        "        super().__init__(n)\n",
        "        self.models = models  # Lista de modelos de diferentes Ã³rdenes, ordenados de mayor a menor\n",
        "        self.alpha = alpha    # Factor de escala fijo\n",
        "        # Actualizamos self.vocab con la uniÃ³n de los vocabularios de los modelos\n",
        "        self.vocab = set()\n",
        "        for model in self.models:\n",
        "            self.vocab.update(model.vocab)\n",
        "\n",
        "    def get_ngram_prob(self, ngram: Tuple[str, ...]) -> float:\n",
        "        for i, model in enumerate(self.models):\n",
        "            ngram_adjusted = ngram[-model.n:]\n",
        "            prob = model.get_ngram_prob(ngram_adjusted)\n",
        "            if prob > 0:\n",
        "                return (self.alpha ** i) * prob\n",
        "        # Si ningÃºn modelo tiene el n-grama, asignamos probabilidad cero\n",
        "        return 0.0\n",
        "\n",
        "# Entrenamiento de los modelos\n",
        "# Entrenamos modelos base\n",
        "unigram_model = NGramModel(n=1)\n",
        "bigram_model = NGramModel(n=2)\n",
        "\n",
        "unigram_model.train(train_corpus)\n",
        "bigram_model.train(train_corpus)\n",
        "\n",
        "# Modelos para backoff estÃ¡ndar\n",
        "backoff_model = BackoffNGramModel(n=2, models=[bigram_model, unigram_model])\n",
        "\n",
        "# Modelo Stupid Backoff\n",
        "stupid_backoff_model = StupidBackoffNGramModel(n=2, models=[bigram_model, unigram_model], alpha=0.4)\n",
        "\n",
        "# Lista de bigramas del corpus\n",
        "bigrams_in_corpus = set()\n",
        "for sentence in train_corpus:\n",
        "    tokens = ['<s>'] + sentence + ['</s>']\n",
        "    for i in range(len(tokens) - 1):\n",
        "        bigr = (tokens[i], tokens[i + 1])\n",
        "        bigrams_in_corpus.add(bigr)\n",
        "\n",
        "# CÃ¡lculo de probabilidades con back-off para los bigramas del corpus\n",
        "print(\"Probabilidades de bigramas con back-off:\")\n",
        "for bigram in bigrams_in_corpus:\n",
        "    prob = backoff_model.get_ngram_prob(bigram)\n",
        "    print(f\"P('{bigram[1]}' | '{bigram[0]}') = {prob:.4f}\")\n",
        "\n",
        "# Probabilidad del bigrama no visto 'a models' con back-off\n",
        "prob_a_models_backoff = backoff_model.get_ngram_prob(('a', 'models'))\n",
        "print(f\"\\nP('models' | 'a') con back-off = {prob_a_models_backoff:.4f}\")\n",
        "\n",
        "# CÃ¡lculo de probabilidades con stupid-backoff para los bigramas del corpus\n",
        "print(\"\\nProbabilidades de bigramas con stupid-backoff:\")\n",
        "for bigram in bigrams_in_corpus:\n",
        "    prob = stupid_backoff_model.get_ngram_prob(bigram)\n",
        "    print(f\"P('{bigram[1]}' | '{bigram[0]}') = {prob:.4f}\")\n",
        "\n",
        "# Probabilidad del bigrama no visto 'a models' con stupid-backoff\n",
        "prob_a_models_stupid = stupid_backoff_model.get_ngram_prob(('a', 'models'))\n",
        "print(f\"\\nP('models' | 'a') con stupid-backoff = {prob_a_models_stupid:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrEJ30ZGcy97"
      },
      "source": [
        "# Parte 2:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XycIxfpyAmTx"
      },
      "source": [
        "Referencia: https://github.com/kapumota/Actividades-CC0C2/blob/3d0cde28776954bfc6367f2b94725dffcfaf8f38/Cuadernos-CC0C2/Clase2/Modelos-lenguaje2.ipynb#L913"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cpnxPBMVBid"
      },
      "source": [
        "Referencia: https://github.com/kapumota/Actividades-CC0C2/blob/3d0cde28776954bfc6367f2b94725dffcfaf8f38/Cuadernos-CC0C2/Clase2/Topicos-avanzados.ipynb#L501"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z4ClZgQc4f4"
      },
      "source": [
        "### e. El suavizado de Good-Turing reasigna la masa de probabilidad de los n-gramas ricos a los n-gramas pobres. Dado un corpus D, supongamos que tratamos todas las unigramas desconocidas como â¨UNKâ© por lo tanto, el vocabulario es {w:wâD}âª{â¨UNKâ©} y N0=1. Calcula r, Nr, para todas las unigramas de la parte 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "o9OnsViqhZ_i"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import math\n",
        "from typing import List, Tuple, Dict\n",
        "import numpy as np\n",
        "\n",
        "corpus = [ \"all models are wrong\", \"a model is wrong\",\n",
        "            \"some models are useful\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zhVp8eWLAFHe"
      },
      "outputs": [],
      "source": [
        "def tokenize_corpus(corpus: List[str]) -> List[List[str]]:\n",
        "    tokenized_corpus = []\n",
        "    for sentence in corpus:\n",
        "        tokens = sentence.lower().split()\n",
        "        tokens = ['<s>'] + tokens + ['</s>']  # AÃ±adimos tokens de inicio y fin de oraciÃ³n\n",
        "        tokenized_corpus.append(tokens)\n",
        "    return tokenized_corpus\n",
        "\n",
        "\n",
        "tokenized_corpus = tokenize_corpus(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2u46GW6mltA7",
        "outputId": "1fe6dd2c-6737-4688-aba1-8e9a65a77011"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Conteos de Unigramas:\n",
            "('<s>',): 3\n",
            "('all',): 1\n",
            "('models',): 2\n",
            "('are',): 2\n",
            "('wrong',): 2\n",
            "('</s>',): 3\n",
            "('a',): 1\n",
            "('model',): 1\n",
            "('is',): 1\n",
            "('some',): 1\n",
            "('useful',): 1\n"
          ]
        }
      ],
      "source": [
        "# Conteo de N-gramas\n",
        "def count_ngrams(tokenized_corpus: List[List[str]], n: int) -> Dict[Tuple[str, ...], int]:\n",
        "    ngram_counts = collections.Counter()\n",
        "    for tokens in tokenized_corpus:\n",
        "        for i in range(len(tokens) - n + 1):\n",
        "            ngram = tuple(tokens[i:i + n])\n",
        "            ngram_counts[ngram] += 1\n",
        "    return ngram_counts\n",
        "\n",
        "# Contamos unigramas e incluimos <UNK>\n",
        "unigram_counts = count_ngrams(tokenized_corpus, 1)\n",
        "unigram_counts[('<UNK>',)] = 0  # Para representar unigramas desconocidos\n",
        "\n",
        "unigram_counts = count_ngrams(tokenized_corpus, 1)\n",
        "print(\"\\nConteos de Unigramas:\")\n",
        "for unigram, count in unigram_counts.items():\n",
        "    print(f\"{unigram}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Mq0I6s4lAReZ"
      },
      "outputs": [],
      "source": [
        "bigram_counts = count_ngrams(tokenized_corpus, 2)\n",
        "trigram_counts = count_ngrams(tokenized_corpus, 3)\n",
        "\n",
        "#  CÃ¡lculo de N(C)\n",
        "def calculate_NC(ngram_counts: Dict[Tuple[str, ...], int]) -> Dict[int, int]:\n",
        "    count_of_counts = collections.Counter()\n",
        "    for count in ngram_counts.values():\n",
        "        count_of_counts[count] += 1\n",
        "    return count_of_counts\n",
        "\n",
        "def sort_NC(NC: Dict[int, int]) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    counts = np.array(list(NC.keys()))\n",
        "    frequencies = np.array([NC[count] for count in counts])\n",
        "    sorted_indices = np.argsort(counts)\n",
        "    return counts[sorted_indices], frequencies[sorted_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tLTd1zHSnX4g"
      },
      "outputs": [],
      "source": [
        "# Calculamos N(C) para bigramas (puede aplicarse a unigramas y trigramas de manera similar)\n",
        "NC_bigram = calculate_NC(bigram_counts)\n",
        "counts_bigram, frequencies_bigram = sort_NC(NC_bigram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "J0LOitM9na9R"
      },
      "outputs": [],
      "source": [
        "# Suavizado Good-Turing\n",
        "def good_turing_discounting(ngram_counts: Dict[Tuple[str, ...], int]) -> Dict[Tuple[str, ...], float]:\n",
        "    # Calculamos N(C)\n",
        "    NC = calculate_NC(ngram_counts)\n",
        "    counts, frequencies = sort_NC(NC)\n",
        "\n",
        "    # Ajuste de conteos\n",
        "    total_ngrams = sum(ngram_counts.values())\n",
        "    max_count = max(counts)\n",
        "    adjusted_counts = {}\n",
        "\n",
        "    for ngram, count in ngram_counts.items():\n",
        "        if count < max_count:\n",
        "            Nc = NC[count]\n",
        "            Nc1 = NC.get(count + 1, 0)\n",
        "            if Nc > 0:\n",
        "                C_star = (count + 1) * (Nc1 / Nc)\n",
        "                adjusted_counts[ngram] = C_star\n",
        "            else:\n",
        "                adjusted_counts[ngram] = count\n",
        "        else:\n",
        "            adjusted_counts[ngram] = count  # Para conteos mÃ¡ximos, no ajustamos\n",
        "    return adjusted_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ifznnUoAo68K"
      },
      "outputs": [],
      "source": [
        "#CÃ¡lculo de probabilidades ajustadas\n",
        "def calculate_probabilities(adjusted_counts: Dict[Tuple[str, ...], float],\n",
        "                            n_minus1_counts: Dict[Tuple[str, ...], int]) -> Dict[Tuple[str, ...], float]:\n",
        "    probabilities = {}\n",
        "    for ngram, adjusted_count in adjusted_counts.items():\n",
        "        context = ngram[:-1]\n",
        "        context_count = n_minus1_counts.get(context, sum(n_minus1_counts.values()))\n",
        "        probability = adjusted_count / context_count if context_count > 0 else 0.0\n",
        "        probabilities[ngram] = probability\n",
        "    return probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kzqanCIh1XPQ"
      },
      "outputs": [],
      "source": [
        "adjusted_bigram_counts = good_turing_discounting(bigram_counts)\n",
        "\n",
        "adjusted_unigram_counts = good_turing_discounting(unigram_counts)\n",
        "\n",
        "# Calculamos las probabilidades ajustadas para bigramas\n",
        "unigram_total_count = sum(unigram_counts.values())\n",
        "bigram_probabilities = calculate_probabilities(adjusted_bigram_counts, unigram_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FAJSzk3C2v9Z"
      },
      "outputs": [],
      "source": [
        "# AsignaciÃ³n de probabilidad a N-gramas no observados\n",
        "def probability_of_unseen(NC: Dict[int, int], total_ngrams: int) -> float:\n",
        "    N1 = NC.get(1, 0)\n",
        "    return N1 / total_ngrams if total_ngrams > 0 else 0.0\n",
        "\n",
        "# Probabilidad de n-gramas no observados para bigramas\n",
        "total_bigrams = sum(bigram_counts.values())\n",
        "P_unseen_bigram = probability_of_unseen(NC_bigram, total_bigrams)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2paoc320pGFU"
      },
      "outputs": [],
      "source": [
        "#Uso del modelo para calcular la probabilidad de una oraciÃ³n\n",
        "def sentence_probability(sentence: str, bigram_probabilities: Dict[Tuple[str, str], float], P_unseen: float) -> float:\n",
        "    tokens = ['<s>'] + sentence.lower().split() + ['</s>']\n",
        "    probability_log_sum = 0.0\n",
        "    for i in range(len(tokens) - 1):\n",
        "        bigram = (tokens[i], tokens[i + 1])\n",
        "        prob = bigram_probabilities.get(bigram, P_unseen)\n",
        "        probability_log_sum += math.log(prob) if prob > 0 else float('-inf')\n",
        "    return math.exp(probability_log_sum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdthLr8vDP3X",
        "outputId": "7467c929-1e94-4841-814d-0b101980d64b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " r (veces que ocurre un unigrama):\n",
            "\n",
            "('<s>',): 3\n",
            "('all',): 1\n",
            "('models',): 2\n",
            "('are',): 2\n",
            "('wrong',): 2\n",
            "('</s>',): 3\n",
            "('a',): 1\n",
            "('model',): 1\n",
            "('is',): 1\n",
            "('some',): 1\n",
            "('useful',): 1\n",
            "\n",
            " Nr (nÃºmero de unigramas que ocurren exactamente r veces):\n",
            "\n",
            "Counter({1: 6, 2: 3, 3: 2, 0: 1})\n"
          ]
        }
      ],
      "source": [
        "NC = calculate_NC(unigram_counts)\n",
        "NC[0] = 1  # Agregamos N0 = 1 para el caso <UNK>\n",
        "counts_unigram, frequencies_unigram  = sort_NC(NC)\n",
        "\n",
        "print(\"\\n r (veces que ocurre un unigrama):\\n\")\n",
        "for unigram, count in unigram_counts.items():\n",
        "    print(f\"{unigram}: {count}\")\n",
        "\n",
        "print(\"\\n Nr (nÃºmero de unigramas que ocurren exactamente r veces):\\n\")\n",
        "print(NC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgzcRy37DxIt"
      },
      "source": [
        "### f. Para r<3, calcula cr y las probabilidades de todas las unigramas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeMkFnAS_q_1",
        "outputId": "87b66c43-38e1-4dde-fda3-fbe8b6ecde5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Probabilidades ajustadas para r < 3:\n",
            "C('all',): 1.0\n",
            "C('models',): 2.0\n",
            "C('are',): 2.0\n",
            "C('wrong',): 2.0\n",
            "C('a',): 1.0\n",
            "C('model',): 1.0\n",
            "C('is',): 1.0\n",
            "C('some',): 1.0\n",
            "C('useful',): 1.0\n"
          ]
        }
      ],
      "source": [
        "# Mostramos las probabilidades para r < 3\n",
        "print(\"\\nProbabilidades ajustadas para r < 3:\")\n",
        "for unigram, count in adjusted_unigram_counts.items():\n",
        "    if unigram_counts[unigram] < 3:\n",
        "        print(f\"C{unigram}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PQE5JqVffRf",
        "outputId": "4c07d110-28af-4e4a-acc8-6ba690ba8ee2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Probabilidades ajustadas para r < 3:\n",
            "P('all',): 0.05555555555555555\n",
            "P('models',): 0.1111111111111111\n",
            "P('are',): 0.1111111111111111\n",
            "P('wrong',): 0.1111111111111111\n",
            "P('a',): 0.05555555555555555\n",
            "P('model',): 0.05555555555555555\n",
            "P('is',): 0.05555555555555555\n",
            "P('some',): 0.05555555555555555\n",
            "P('useful',): 0.05555555555555555\n"
          ]
        }
      ],
      "source": [
        "unigram_probabilities = calculate_probabilities(adjusted_unigram_counts, {'<s>': unigram_total_count})\n",
        "\n",
        "# Mostramos las probabilidades para r < 3\n",
        "print(\"\\nProbabilidades ajustadas para r < 3:\")\n",
        "for unigram, prob in unigram_probabilities.items():\n",
        "    if unigram_counts[unigram] < 3:\n",
        "        print(f\"P{unigram}: {prob}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2THL2vknuukW"
      },
      "source": [
        "### g. Para el valor mÃ¡ximo de r, Nr+1=0. En este caso, la probabilidad P(w: #w=r) aÃºn puede estimarse mediante MLE (MÃ¡xima Verosimilitud). Calcula la   probabilidad de las unigramas de la parte 1 que aparecen con mayor frecuencia, es decir, r=3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "junsGpFaP9gy",
        "outputId": "7c81dc3b-5530-4cf5-bd94-704254c87383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Probabilidades para unigramas con r = 3 usando MLE:\n",
            "C('<s>',): 3, P(w) = 0.1667\n",
            "C('</s>',): 3, P(w) = 0.1667\n"
          ]
        }
      ],
      "source": [
        "total_tokens = sum(unigram_counts.values())\n",
        "probabilities_g = {}\n",
        "print(\"\\nProbabilidades para unigramas con r = 3 usando MLE:\")\n",
        "for unigram, count in unigram_counts.items():\n",
        "    if count == 3:\n",
        "        probability = count / total_tokens\n",
        "        unigram_probabilities[unigram] = probability\n",
        "        print(f\"C{unigram}: {count}, P(w) = {probability:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Funq9Pg2Hh98"
      },
      "source": [
        "### h. Muestra que la suma de las probabilidades de todas las unigramas dadas en (f) y (g) no es 1. Intenta normalizar las probabilidades."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7UUA1EX5LDio"
      },
      "outputs": [],
      "source": [
        "NC_unigram = calculate_NC(unigram_counts)\n",
        "total_unigrams = sum(unigram_counts.values())\n",
        "P_unseen_unigram = probability_of_unseen(NC_unigram, total_unigrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfvShEl7F-ro",
        "outputId": "14dc0fed-786a-42c6-d3e4-1b64252c0a3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Probabilidades normalizadas:\n",
            "('<s>',): P(w) = 0.1250\n",
            "('all',): P(w) = 0.0417\n",
            "('models',): P(w) = 0.0833\n",
            "('are',): P(w) = 0.0833\n",
            "('wrong',): P(w) = 0.0833\n",
            "('</s>',): P(w) = 0.1250\n",
            "('a',): P(w) = 0.0417\n",
            "('model',): P(w) = 0.0417\n",
            "('is',): P(w) = 0.0417\n",
            "('some',): P(w) = 0.0417\n",
            "('useful',): P(w) = 0.0417\n"
          ]
        }
      ],
      "source": [
        "probabilities_normalized = {w: p / (sum(unigram_probabilities.values())+P_unseen_unigram) for w, p in unigram_probabilities.items()}\n",
        "\n",
        "suma_prob = 0\n",
        "print(\"\\nProbabilidades normalizadas:\")\n",
        "for unigram, prob in probabilities_normalized.items():\n",
        "    print(f\"{unigram}: P(w) = {prob:.4f}\")\n",
        "    suma_prob += prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Fpa3rG1Ot3D"
      },
      "source": [
        "A continuaciÃ³n se observa que la suma de las probabilidades vistas en f y g no suman 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2Acn1SLLwLm",
        "outputId": "5156e203-2d8b-44fc-c22e-6eba3ca78c0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7499999999999998\n"
          ]
        }
      ],
      "source": [
        "print(suma_prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Hobd5SzNAPc"
      },
      "source": [
        "### i. En un corpus grande, Nr puede ser cero para valores grandes de r. Esto puede ser problemÃ¡tico, ya que conduce a que los valores estimados de P(w:#w=r) sean cero. Una forma de resolver este problema es usar una lÃ­nea suavizada para ajustar aproximadamente la distribuciÃ³n de los valores conocidos. Supongamos que cambiamos la segunda oraciÃ³n de ejemplo de la parte 1 a âa model is wrong wrong wrong wrongâ de modo que N4=1, pero S_5=0. Adivina un buen valor suavizado de N_5. Usa el valor aproximado de N_5 y el valor original de los otros recuentos de frecuencia para calcular las probabilidades de todas las unigramas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-S8bE1tOs8H",
        "outputId": "9dcce103-d990-4d1d-de0a-224e3a149a26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "r y N_r:\n",
            "r = 1, N_r = 6\n",
            "r = 2, N_r = 2\n",
            "r = 3, N_r = 2\n",
            "r = 5, N_r = 1\n"
          ]
        }
      ],
      "source": [
        "# Actualizamos el corpus\n",
        "corpus_i = [\n",
        "    \"all models are wrong\",\n",
        "    \"a model is wrong wrong wrong wrong\",\n",
        "    \"some models are useful\"\n",
        "]\n",
        "\n",
        "# Tokenizamos y contamos nuevamente\n",
        "tokenized_corpus_i = tokenize_corpus(corpus_i)\n",
        "unigram_counts_i = count_ngrams(tokenized_corpus_i, 1)\n",
        "NC_unigram_i = calculate_NC(unigram_counts_i)\n",
        "\n",
        "# Mostramos los nuevos valores de N_r\n",
        "print(\"\\nr y N_r:\")\n",
        "for r, Nr in sorted(NC_unigram_i.items()):\n",
        "    print(f\"r = {r}, N_r = {Nr}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhuBpdIFPAQ4",
        "outputId": "34b213c0-a213-4233-c490-6fb66aec523c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Probabilidades ajustadas tras el suavizado:\n",
            "('<s>',): 0.05882352941176472\n",
            "('all',): 0.03921568627450981\n",
            "('models',): 0.17647058823529416\n",
            "('are',): 0.17647058823529416\n",
            "('wrong',): 0.2941176470588236\n",
            "('</s>',): 0.05882352941176472\n",
            "('a',): 0.03921568627450981\n",
            "('model',): 0.03921568627450981\n",
            "('is',): 0.03921568627450981\n",
            "('some',): 0.03921568627450981\n",
            "('useful',): 0.03921568627450981\n"
          ]
        }
      ],
      "source": [
        "# FunciÃ³n de suavizado Good-Turing con valor estimado de N_r\n",
        "def good_turing_discounting_unigrams_smoothed(unigram_counts: Dict[Tuple[str, ...], int], N4_estimate: float) -> Dict[Tuple[str, ...], float]:\n",
        "    NC = calculate_NC(unigram_counts)\n",
        "    # Estimamos N_4\n",
        "    NC[4] = N4_estimate\n",
        "    adjusted_counts = {}\n",
        "    counts = sorted(NC.keys())\n",
        "    max_r = max(counts)\n",
        "\n",
        "    for unigram, count in unigram_counts.items():\n",
        "        r = count\n",
        "        Nr = NC.get(r, 0)\n",
        "        Nr1 = NC.get(r + 1, 0)\n",
        "        if Nr > 0 and Nr1 > 0:\n",
        "            c_star = (r + 1) * (Nr1 / Nr)\n",
        "        else:\n",
        "            c_star = r  # Si Nr o Nr1 es 0, no ajustamos\n",
        "        adjusted_counts[unigram] = c_star\n",
        "    return adjusted_counts\n",
        "\n",
        "# Aplicamos el suavizado con N4_estimate = 0.5\n",
        "adjusted_unigram_counts_i = good_turing_discounting_unigrams_smoothed(unigram_counts_i, N4_estimate=0.5)\n",
        "\n",
        "# Calculamos las probabilidades ajustadas\n",
        "unigram_probabilities_i = calculate_probabilities(adjusted_unigram_counts_i, {'<s>': unigram_total_count})\n",
        "\n",
        "# Normalizamos las probabilidades\n",
        "total_probability_i = sum(unigram_probabilities_i.values())\n",
        "if total_probability_i != 1.0:\n",
        "    for unigram in unigram_probabilities_i:\n",
        "        unigram_probabilities_i[unigram] /= total_probability_i\n",
        "\n",
        "# Mostramos las probabilidades finales\n",
        "print(\"\\nProbabilidades ajustadas tras el suavizado:\")\n",
        "for unigram, prob in unigram_probabilities_i.items():\n",
        "    print(f\"{unigram}: {prob}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ9h4pMfrOXj"
      },
      "source": [
        "## Parte 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOx2Io1TrP43"
      },
      "source": [
        "## ImplementaciÃ³n inicial del Brown Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAJ4B4K9gqwn",
        "outputId": "2a311b8e-0b0d-4cf7-a4cc-e3781a2cac6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Descargando el artÃ­culo: Inteligencia artificial\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import json\n",
        "\n",
        "articulos = [\n",
        "    \"Inteligencia artificial\", \"Aprendizaje automÃ¡tico\", \"BiotecnologÃ­a\"\n",
        "]\n",
        "\n",
        "# URL de la API de Wikipedia para obtener el extracto en formato JSON\n",
        "base_url = 'https://es.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&explaintext&titles={}'\n",
        "\n",
        "# Archivo para guardar el corpus\n",
        "filename = 'corpus.txt'\n",
        "\n",
        "with open(filename, 'w', encoding='utf-8') as file:\n",
        "    for title in articulos:\n",
        "        \"\"\"\n",
        "        Descarga el contenido de artÃ­culos de Wikipedia y los guarda en un archivo de texto.\n",
        "\n",
        "        Args:\n",
        "            title (str): El tÃ­tulo del artÃ­culo de Wikipedia a descargar.\n",
        "\n",
        "        Funcionalidad:\n",
        "            - Codifica el tÃ­tulo del artÃ­culo para incluirlo en una URL de solicitud a la API de Wikipedia.\n",
        "            - Descarga el contenido del artÃ­culo en formato JSON y extrae el campo 'extract', que contiene el texto del artÃ­culo.\n",
        "            - Si el extracto no estÃ¡ vacÃ­o, lo escribe en el archivo especificado y confirma la descarga en consola.\n",
        "            - Si el extracto estÃ¡ vacÃ­o, indica que el artÃ­culo no se pudo descargar.\n",
        "\n",
        "        Output:\n",
        "            Archivo 'corpus.txt' con el contenido de todos los artÃ­culos descargados.\n",
        "        \"\"\"\n",
        "\n",
        "        # Codificar el tÃ­tulo para URL\n",
        "        encoded_title = urllib.request.quote(title)\n",
        "        url = base_url.format(encoded_title)\n",
        "        print(f\"Descargando el artÃ­culo: {title}\")\n",
        "        response = urllib.request.urlopen(url)\n",
        "        data = response.read().decode('utf-8')\n",
        "\n",
        "        # Extraer el texto del JSON\n",
        "        json_data = json.loads(data)\n",
        "        pages = json_data['query']['pages']\n",
        "        page = next(iter(pages.values()))\n",
        "        extract = page.get('extract', '')\n",
        "\n",
        "        # Escribir el texto en el archivo si el extracto no estÃ¡ vacÃ­o\n",
        "        if extract:\n",
        "            file.write(extract + '\\n')\n",
        "            print(f\"ArtÃ­culo '{title}' descargado.\")\n",
        "        else:\n",
        "            print(f\"No se pudo descargar el artÃ­culo '{title}'.\")\n",
        "\n",
        "print(f\"Todos los artÃ­culos han sido descargados y guardados en '{filename}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNUg4NpIrKVm",
        "outputId": "2b3a99c9-14e8-4e3c-a8aa-194ccf1502e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iniciando el preprocesamiento del corpus\n",
            "TamaÃ±o del vocabulario final: 477 palabras\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Convierte el texto en minÃºsculas, remueve la puntuaciÃ³n y lo tokeniza en palabras.\n",
        "\n",
        "    Args:\n",
        "        text (str): Texto a procesar.\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de palabras (tokens) extraÃ­das del texto.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "def stem(word):\n",
        "    \"\"\"\n",
        "    Aplica una reducciÃ³n simple de sufijos comunes en espaÃ±ol para simular el stemming.\n",
        "\n",
        "    Args:\n",
        "        word (str): Palabra a la que se le aplicarÃ¡ el stemming.\n",
        "\n",
        "    Returns:\n",
        "        str: La palabra sin sufijos comunes.\n",
        "    \"\"\"\n",
        "    suffixes = ['aciones', 'imientos', 'amiento', 'iciÃ³n', 'adora', 'aciÃ³n', 'adoras', 'adores', 'ante',\n",
        "                'ancia', 'mente', 'idad', 'ivas', 'ivos', 'anza', 'icos', 'icas', 'ico', 'ica',\n",
        "                'oso', 'osa', 'osos', 'osas', 'ismo', 'ismos', 'able', 'ables', 'ible', 'ibles',\n",
        "                'ente', 'entes', 'mente']\n",
        "    for suffix in suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            return word[:-len(suffix)]\n",
        "    return word\n",
        "\n",
        "stopwords = set([\n",
        "     'vuestro', 'vuestros', 'y', 'ya', 'yo', 'Ã©l', 'Ã©ramos','a', 'al', 'algo', 'algunas', 'algunos',\n",
        "     'ante', 'antes', 'como', 'con', 'contra', 'de', 'del', 'desde', 'donde', 'durante', 'e', 'el',\n",
        "     'ella', 'ellas', 'ellos', 'en', 'entre', 'era', 'erais', 'eran', 'eras', 'eres', 'es',\n",
        "     'esa', 'esas', 'ese', 'eso', 'esos', 'esta', 'estaba', 'estabais', 'estaban', 'estabas',\n",
        "    'vosotras', 'vosotros', 'vuestra', 'vuestras'\n",
        "])\n",
        "\n",
        "\n",
        "def preprocess_corpus(filename):\n",
        "    \"\"\"\n",
        "    Lee un archivo de texto, realiza tokenizaciÃ³n, stemming, remueve stopwords y filtra palabras raras.\n",
        "\n",
        "    Args:\n",
        "        filename (str): Ruta del archivo de texto a procesar.\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de tokens procesados que cumplen con un umbral de frecuencia.\n",
        "    \"\"\"\n",
        "    print(\"Iniciando el preprocesamiento del corpus\")\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        word_counter = Counter()\n",
        "        final_tokens = []\n",
        "        for line in file:\n",
        "            tokens = tokenize(line)\n",
        "            stems = [stem(token) for token in tokens]\n",
        "            tokens_filtered = [word for word in stems if word not in stopwords]\n",
        "            word_counter.update(tokens_filtered)\n",
        "            final_tokens.extend(tokens_filtered)\n",
        "            \n",
        "    threshold = 5\n",
        "    frequent_words = {word for word, count in word_counter.items() if count >= threshold}\n",
        "    final_tokens = [word for word in final_tokens if word in frequent_words]\n",
        "\n",
        "    print(f\"TamaÃ±o del vocabulario final: {len(set(final_tokens))} palabras\")\n",
        "    return final_tokens\n",
        "\n",
        "final_tokens = preprocess_corpus('corpus.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "TsqVWvb02mHI"
      },
      "outputs": [],
      "source": [
        "def brown_clustering_initial(final_tokens):\n",
        "    \"\"\"\n",
        "    Brown Clustering.\n",
        "\n",
        "    Pasos:\n",
        "    1. InicializaciÃ³n: asigne cada palabra a su propio grupo.\n",
        "\n",
        "    2. CÃ¡lculos de probabilidad: estime P(c) y P(c_i, c_j).\n",
        "\n",
        "    3. BÃºsqueda de combinaciÃ³n Ã³ptima: evalÃºe ÎI(c_i, c_j) para cada par.\n",
        "\n",
        "    4. CombinaciÃ³n de grupos: combine los grupos que minimicen ÎI.\n",
        "\n",
        "    5. Repita hasta alcanzar la cantidad deseada de grupos.\n",
        "\n",
        "    \"\"\"\n",
        "    # InicializaciÃ³n\n",
        "    vocabulary = set(final_tokens)\n",
        "    clusters = {}  # cluster_id: conjunto de palabras\n",
        "    word_to_cluster = {}  # word: cluster_id\n",
        "\n",
        "    for i, word in enumerate(vocabulary):\n",
        "        clusters[i] = set([word])\n",
        "        word_to_cluster[word] = i\n",
        "\n",
        "    # CÃ¡culo de probabilidades\n",
        "    from collections import defaultdict\n",
        "    import math\n",
        "\n",
        "    # Cuenta los unigramas y bigramas\n",
        "    bigram_counts = defaultdict(int)\n",
        "    unigram_counts = defaultdict(int)\n",
        "\n",
        "    for i in range(len(final_tokens) - 1):\n",
        "        w1 = final_tokens[i]\n",
        "        w2 = final_tokens[i + 1]\n",
        "        bigram_counts[(w1, w2)] += 1\n",
        "        unigram_counts[w1] += 1\n",
        "    unigram_counts[final_tokens[-1]] += 1  # Ãºltima palabra\n",
        "\n",
        "    total_bigrams = sum(bigram_counts.values())\n",
        "    total_words = sum(unigram_counts.values())\n",
        "\n",
        "    cluster_counts = defaultdict(int) \n",
        "    cluster_bigram_counts = defaultdict(int) \n",
        "\n",
        "    for word, count in unigram_counts.items():\n",
        "        c = word_to_cluster[word]\n",
        "        cluster_counts[c] += count\n",
        "\n",
        "    for (w1, w2), count in bigram_counts.items():\n",
        "        c1 = word_to_cluster[w1]\n",
        "        c2 = word_to_cluster[w2]\n",
        "        cluster_bigram_counts[(c1, c2)] += count\n",
        "\n",
        "    # Caclula la informaciÃ³n mutua\n",
        "    def compute_mutual_information(cluster_counts, cluster_bigram_counts, total_bigrams, total_words):\n",
        "        I = 0.0\n",
        "        for (c_i, c_j), count in cluster_bigram_counts.items():\n",
        "            p_ci_cj = count / total_bigrams\n",
        "            p_ci = cluster_counts[c_i] / total_words\n",
        "            p_cj = cluster_counts[c_j] / total_words\n",
        "            if p_ci_cj > 0 and p_ci > 0 and p_cj > 0:\n",
        "                I += p_ci_cj * math.log(p_ci_cj / (p_ci * p_cj))\n",
        "        return I\n",
        "\n",
        "    I_current = compute_mutual_information(cluster_counts, cluster_bigram_counts, total_bigrams, total_words)\n",
        "\n",
        "    # Loop \n",
        "    desired_num_clusters = 100 \n",
        "    num_clusters = len(clusters)\n",
        "\n",
        "    while num_clusters > desired_num_clusters:\n",
        "        min_delta_I = None\n",
        "        best_pair = None\n",
        "        best_cluster_counts = None\n",
        "        best_cluster_bigram_counts = None\n",
        "        best_clusters = None\n",
        "        best_word_to_cluster = None\n",
        "\n",
        "        cluster_ids = list(clusters.keys())\n",
        "\n",
        "        for idx_i in range(len(cluster_ids)):\n",
        "            c_i = cluster_ids[idx_i]\n",
        "            for idx_j in range(idx_i + 1, len(cluster_ids)):\n",
        "                c_j = cluster_ids[idx_j]\n",
        "\n",
        "                # Crea clusters temporales\n",
        "                clusters_temp = clusters.copy()\n",
        "                clusters_temp[c_i] = clusters[c_i].union(clusters[c_j])\n",
        "                del clusters_temp[c_j]\n",
        "\n",
        "                # Actualiza word_to_cluster\n",
        "                word_to_cluster_temp = word_to_cluster.copy()\n",
        "                for word in clusters[c_j]:\n",
        "                    word_to_cluster_temp[word] = c_i\n",
        "\n",
        "                # Actualiza cluster_counts_temp\n",
        "                cluster_counts_temp = cluster_counts.copy()\n",
        "                cluster_counts_temp[c_i] += cluster_counts_temp[c_j]\n",
        "                del cluster_counts_temp[c_j]\n",
        "\n",
        "                # Actualiza cluster_bigram_counts_temp\n",
        "                cluster_bigram_counts_temp = defaultdict(int)\n",
        "                for (ci, cj), count in cluster_bigram_counts.items():\n",
        "                    ci_new = ci\n",
        "                    cj_new = cj\n",
        "                    if ci == c_j:\n",
        "                        ci_new = c_i\n",
        "                    if cj == c_j:\n",
        "                        cj_new = c_i\n",
        "                    cluster_bigram_counts_temp[(ci_new, cj_new)] += count\n",
        "\n",
        "                # Calula la informaciÃ³n mutua\n",
        "                I_merge = compute_mutual_information(cluster_counts_temp, cluster_bigram_counts_temp, total_bigrams, total_words)\n",
        "\n",
        "                delta_I = I_merge - I_current\n",
        "\n",
        "                if min_delta_I is None or delta_I < min_delta_I:\n",
        "                    min_delta_I = delta_I\n",
        "                    best_pair = (c_i, c_j)\n",
        "                    best_cluster_counts = cluster_counts_temp\n",
        "                    best_cluster_bigram_counts = cluster_bigram_counts_temp\n",
        "                    best_clusters = clusters_temp\n",
        "                    best_word_to_cluster = word_to_cluster_temp\n",
        "\n",
        "        # Une los mejores pares\n",
        "        c_i, c_j = best_pair\n",
        "        clusters = best_clusters\n",
        "        cluster_counts = best_cluster_counts\n",
        "        cluster_bigram_counts = best_cluster_bigram_counts\n",
        "        word_to_cluster = best_word_to_cluster\n",
        "        I_current += min_delta_I\n",
        "        num_clusters -= 1\n",
        "        print(f\"Se unieron los clÃºsteres {c_i} y {c_j}; nÃºmero de clÃºsteres: {num_clusters}\")\n",
        "\n",
        "    return clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "DF2H8FuM2nXZ"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[37], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      6\u001b[0m tracemalloc\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m----> 8\u001b[0m clusters_initial \u001b[38;5;241m=\u001b[39m \u001b[43mbrown_clustering_initial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m current, peak \u001b[38;5;241m=\u001b[39m tracemalloc\u001b[38;5;241m.\u001b[39mget_traced_memory()\n\u001b[0;32m     11\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
            "Cell \u001b[1;32mIn[36], line 110\u001b[0m, in \u001b[0;36mbrown_clustering_initial\u001b[1;34m(final_tokens)\u001b[0m\n\u001b[0;32m    108\u001b[0m ci_new \u001b[38;5;241m=\u001b[39m ci\n\u001b[0;32m    109\u001b[0m cj_new \u001b[38;5;241m=\u001b[39m cj\n\u001b[1;32m--> 110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ci \u001b[38;5;241m==\u001b[39m c_j:\n\u001b[0;32m    111\u001b[0m     ci_new \u001b[38;5;241m=\u001b[39m c_i\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cj \u001b[38;5;241m==\u001b[39m c_j:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import time\n",
        "import tracemalloc\n",
        "\n",
        "start_time = time.time()\n",
        "tracemalloc.start()\n",
        "\n",
        "clusters_initial = brown_clustering_initial(final_tokens)\n",
        "\n",
        "current, peak = tracemalloc.get_traced_memory()\n",
        "end_time = time.time()\n",
        "\n",
        "tracemalloc.stop()\n",
        "\n",
        "print(f\"Tiempo de ejecuciÃ³n de la implementaciÃ³n inicial: {end_time - start_time} segundos\")\n",
        "print(f\"Uso de memoria de la implementaciÃ³n inicial: Actual={current / 10**6} MB; Pico={peak / 10**6} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyD04ftm4Y8v"
      },
      "outputs": [],
      "source": [
        "def calculate_silhouette_coefficient(clusters, word_to_cluster, bigram_counts):\n",
        "    \"\"\"\n",
        "    Calcula el coeficiente de silhouette para evaluar la calidad del clustering\n",
        "    \"\"\"\n",
        "    from collections import defaultdict\n",
        "\n",
        "    # Calcular la distancia media entre grupos a(i)\n",
        "    a_values = {}\n",
        "    for cluster_id, words in clusters.items():\n",
        "        for word in words:\n",
        "            # Calcular la distancia promedio a otras palabras en el mismo grupo\n",
        "            intra_distances = []\n",
        "            for other_word in words:\n",
        "                if word != other_word:\n",
        "                    distance = 1 - (bigram_counts.get((word, other_word), 0) + bigram_counts.get((other_word, word), 0))\n",
        "                    intra_distances.append(distance)\n",
        "            a_values[word] = sum(intra_distances) / (len(words) - 1) if len(words) > 1 else 0\n",
        "\n",
        "    # Calcular la distancia promedio del grupo mÃ¡s cercano b(i)\n",
        "    b_values = {}\n",
        "    for word in word_to_cluster:\n",
        "        cluster_id = word_to_cluster[word]\n",
        "        min_distance = None\n",
        "        for other_cluster_id, words in clusters.items():\n",
        "            if other_cluster_id != cluster_id:\n",
        "                inter_distances = []\n",
        "                for other_word in words:\n",
        "                    distance = 1 - (bigram_counts.get((word, other_word), 0) + bigram_counts.get((other_word, word), 0))\n",
        "                    inter_distances.append(distance)\n",
        "                average_distance = sum(inter_distances) / len(inter_distances)\n",
        "                if min_distance is None or average_distance < min_distance:\n",
        "                    min_distance = average_distance\n",
        "        b_values[word] = min_distance\n",
        "\n",
        "    # Calcular puntuaciones de silueta\n",
        "    s_values = {}\n",
        "    for word in word_to_cluster:\n",
        "        a = a_values[word]\n",
        "        b = b_values[word]\n",
        "        s = (b - a) / max(a, b) if max(a, b) > 0 else 0\n",
        "        s_values[word] = s\n",
        "\n",
        "    # Devuelve la puntuaciÃ³n media de silueta\n",
        "    overall_silhouette = sum(s_values.values()) / len(s_values)\n",
        "    return overall_silhouette"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YBzbe483CKf"
      },
      "outputs": [],
      "source": [
        "def brown_clustering_improved(final_tokens):\n",
        "    \"\"\"\n",
        "    ImplementaciÃ³n mejorada de Brown Clustering.\n",
        "\n",
        "    Mejoras:\n",
        "    - Utiliza estructuras de datos eficientes: utiliza colas de prioridad para gestionar pares de clÃºsteres y su ÎI.\n",
        "    - Implementa tÃ©cnicas de poda: limita las fusiones de candidatos mediante clÃºsteres vecinos.\n",
        "\n",
        "    \"\"\"\n",
        "    # InicializaciÃ³n\n",
        "    vocabulary = set(final_tokens)\n",
        "    clusters = {}  \n",
        "    word_to_cluster = {}  \n",
        "\n",
        "    for i, word in enumerate(vocabulary):\n",
        "        clusters[i] = set([word])\n",
        "        word_to_cluster[word] = i\n",
        "\n",
        "    # CÃ¡lculo de probabilidades\n",
        "    from collections import defaultdict\n",
        "    import math\n",
        "    import heapq\n",
        "\n",
        "    # Contar bigramas y unigramas\n",
        "    bigram_counts = defaultdict(int)\n",
        "    unigram_counts = defaultdict(int)\n",
        "\n",
        "    for i in range(len(final_tokens) - 1):\n",
        "        w1 = final_tokens[i]\n",
        "        w2 = final_tokens[i + 1]\n",
        "        bigram_counts[(w1, w2)] += 1\n",
        "        unigram_counts[w1] += 1\n",
        "    unigram_counts[final_tokens[-1]] += 1  # Ãºltima palabra\n",
        "\n",
        "    total_bigrams = sum(bigram_counts.values())\n",
        "    total_words = sum(unigram_counts.values())\n",
        "\n",
        "    # Inicializar cluster_counts y cluster_bigram_counts\n",
        "    cluster_counts = defaultdict(int)  \n",
        "    cluster_bigram_counts = defaultdict(int) \n",
        "\n",
        "    for word, count in unigram_counts.items():\n",
        "        c = word_to_cluster[word]\n",
        "        cluster_counts[c] += count\n",
        "\n",
        "    for (w1, w2), count in bigram_counts.items():\n",
        "        c1 = word_to_cluster[w1]\n",
        "        c2 = word_to_cluster[w2]\n",
        "        cluster_bigram_counts[(c1, c2)] += count\n",
        "\n",
        "    # Inicializar clÃºsteres vecinos\n",
        "    neighbors = defaultdict(set)  \n",
        "    for (c1, c2) in cluster_bigram_counts.keys():\n",
        "        if c1 != c2:\n",
        "            neighbors[c1].add(c2)\n",
        "            neighbors[c2].add(c1)\n",
        "\n",
        "    # Inicializar el montÃ³n con ÎI para cada par de vecinos\n",
        "    heap = []\n",
        "\n",
        "    def compute_delta_I(c_i, c_j):\n",
        "        # CÃ¡lculo eficiente de ÎI basado en recuentos de clÃºsteres y recuentos de bigramas\n",
        "        delta_I = 0.0\n",
        "        # Calcular P(c_i), P(c_j), P(c_i, c_j)\n",
        "        p_ci = cluster_counts[c_i] / total_words\n",
        "        p_cj = cluster_counts[c_j] / total_words\n",
        "        p_ci_cj = cluster_bigram_counts.get((c_i, c_j), 0) / total_bigrams\n",
        "\n",
        "        if p_ci_cj > 0 and p_ci > 0 and p_cj > 0:\n",
        "            delta_I = p_ci_cj * math.log(p_ci_cj / (p_ci * p_cj))\n",
        "        else:\n",
        "            delta_I = 0.0\n",
        "        return delta_I\n",
        "\n",
        "    for c_i in clusters.keys():\n",
        "        for c_j in neighbors[c_i]:\n",
        "            if c_i < c_j:  # Evitar duplicados\n",
        "                delta_I = compute_delta_I(c_i, c_j)\n",
        "                heapq.heappush(heap, (delta_I, c_i, c_j))\n",
        "\n",
        "    desired_num_clusters = 100  \n",
        "    num_clusters = len(clusters)\n",
        "\n",
        "    while num_clusters > desired_num_clusters and heap:\n",
        "        delta_I, c_i, c_j = heapq.heappop(heap)\n",
        "\n",
        "        # Verificar si los clÃºsteres se han fusionado\n",
        "        if c_i not in clusters or c_j not in clusters:\n",
        "            continue  # Omitir si los clÃºsteres ya no existen\n",
        "\n",
        "        # Fusionar los clÃºsteres c_i y c_j en c_new\n",
        "        c_new = c_i  # Utilice c_i como el nuevo ID del clÃºster\n",
        "\n",
        "        clusters[c_new].update(clusters[c_j])\n",
        "        del clusters[c_j]\n",
        "\n",
        "        # Actualiza word_to_cluster\n",
        "        for word in clusters[c_new]:\n",
        "            word_to_cluster[word] = c_new\n",
        "\n",
        "        # Actualiza cluster_counts\n",
        "        cluster_counts[c_new] += cluster_counts[c_j]\n",
        "        del cluster_counts[c_j]\n",
        "\n",
        "        # Actualiza cluster_bigram_counts\n",
        "        cluster_bigram_counts_updated = defaultdict(int)\n",
        "        for (ci, cj), count in cluster_bigram_counts.items():\n",
        "            ci_new = ci\n",
        "            cj_new = cj\n",
        "            if ci == c_j:\n",
        "                ci_new = c_new\n",
        "            if cj == c_j:\n",
        "                cj_new = c_new\n",
        "            cluster_bigram_counts_updated[(ci_new, cj_new)] += count\n",
        "        cluster_bigram_counts = cluster_bigram_counts_updated\n",
        "\n",
        "        # Actualiza neighbors\n",
        "        neighbors[c_new].update(neighbors[c_j])\n",
        "        neighbors[c_new].discard(c_new)\n",
        "        del neighbors[c_j]\n",
        "        for neighbor in neighbors:\n",
        "            if c_j in neighbors[neighbor]:\n",
        "                neighbors[neighbor].discard(c_j)\n",
        "                neighbors[neighbor].add(c_new)\n",
        "\n",
        "        # Actualiza heap\n",
        "        for c_k in neighbors[c_new]:\n",
        "            if c_k != c_new:\n",
        "                delta_I = compute_delta_I(c_new, c_k)\n",
        "                heapq.heappush(heap, (delta_I, c_new, c_k))\n",
        "\n",
        "        num_clusters -= 1\n",
        "        print(f\"Se unieron los clÃºsteres {c_i} y {c_j}; nÃºmero de clÃºsteres: {num_clusters}\")\n",
        "\n",
        "    # Llamar a la funciÃ³n calculate_silhouette_coefficient\n",
        "    silhouette_score = calculate_silhouette_coefficient(clusters, word_to_cluster, bigram_counts)\n",
        "    print(f\"Coeficiente de Silhouette para la implementaciÃ³n con mejoras: {silhouette_score}\")\n",
        "\n",
        "    return clusters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIVEGHfl3tEk"
      },
      "outputs": [],
      "source": [
        "inicio = time.time()\n",
        "tracemalloc.start()\n",
        "\n",
        "clusters_improved = brown_clustering_improved(final_tokens)\n",
        "\n",
        "current, peak = tracemalloc.get_traced_memory()\n",
        "fin = time.time()\n",
        "\n",
        "tracemalloc.stop()\n",
        "\n",
        "print(f\"Tiempo de implementaciÃ³n con mejoras: {fin - inicio} segundos\")\n",
        "print(f\"Uso mejorado de la memoria de implementaciÃ³n: Actual={current / 10**6} MB; Pico={peak / 10**6} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vlQlyeA6tos"
      },
      "source": [
        "# LSA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYMa0DePQh7_"
      },
      "source": [
        "## Pre procesamiento del corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def get_random_wikipedia_articles(num_articles):\n",
        "    S = requests.Session()\n",
        "    URL = \"https://es.wikipedia.org/w/api.php\"\n",
        "    articles = []\n",
        "\n",
        "    while len(articles) < num_articles:\n",
        "        PARAMS = {\n",
        "            \"format\": \"json\",\n",
        "            \"action\": \"query\",\n",
        "            \"list\": \"random\",\n",
        "            \"rnnamespace\": 0,  \n",
        "            \"rnlimit\": min(500, num_articles - len(articles)),  \n",
        "        }\n",
        "\n",
        "        response = S.get(url=URL, params=PARAMS)\n",
        "        data = response.json()\n",
        "        random_articles = data['query']['random']\n",
        "        articles.extend([item['title'] for item in random_articles])\n",
        "\n",
        "    return articles\n",
        "\n",
        "# Obtener 1000 artÃ­culos aleatorios\n",
        "articulos_lsa= get_random_wikipedia_articles(100000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "Hi4GDPZFUST1"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Convierte el texto en minÃºsculas, remueve la puntuaciÃ³n y lo tokeniza en palabras.\n",
        "\n",
        "    Args:\n",
        "        text (str): Texto a procesar.\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de palabras (tokens) extraÃ­das del texto.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "def stem(word):\n",
        "    \"\"\"\n",
        "    Aplica una reducciÃ³n simple de sufijos comunes en espaÃ±ol para simular el stemming.\n",
        "\n",
        "    Args:\n",
        "        word (str): Palabra a la que se le aplicarÃ¡ el stemming.\n",
        "\n",
        "    Returns:\n",
        "        str: La palabra sin sufijos comunes.\n",
        "    \"\"\"\n",
        "    suffixes = ['aciones', 'imientos', 'amiento', 'iciÃ³n', 'adora', 'aciÃ³n', 'adoras', 'adores', 'ante',\n",
        "                'ancia', 'mente', 'idad', 'ivas', 'ivos', 'anza', 'icos', 'icas', 'ico', 'ica',\n",
        "                'oso', 'osa', 'osos', 'osas', 'ismo', 'ismos', 'able', 'ables', 'ible', 'ibles',\n",
        "                'ente', 'entes', 'mente']\n",
        "    for suffix in suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            return word[:-len(suffix)]\n",
        "    return word\n",
        "\n",
        "stopwords = set([\n",
        "    'vuestro', 'vuestros', 'y', 'ya', 'yo', 'Ã©l', 'Ã©ramos', 'a', 'al', 'algo', 'algunas', 'algunos',\n",
        "    'ante', 'antes', 'como', 'con', 'contra', 'de', 'del', 'desde', 'donde', 'durante', 'e', 'el',\n",
        "    'ella', 'ellas', 'ellos', 'en', 'entre', 'era', 'erais', 'eran', 'eras', 'eres', 'es',\n",
        "    'esa', 'esas', 'ese', 'eso', 'esos', 'esta', 'estaba', 'estabais', 'estaban', 'estabas',\n",
        "    'vosotras', 'vosotros', 'vuestra', 'vuestras'\n",
        "])\n",
        "\n",
        "def preprocess_corpus(filename, articulos):\n",
        "    \"\"\"\n",
        "    Lee un archivo de texto, realiza tokenizaciÃ³n, stemming, remueve stopwords y filtra palabras raras.\n",
        "    TambiÃ©n separa el corpus en documentos individuales basados en los tÃ­tulos de los artÃ­culos.\n",
        "\n",
        "    Args:\n",
        "        filename (str): Ruta del archivo de texto a procesar.\n",
        "        articulos (list): Lista de tÃ­tulos de los artÃ­culos.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (documents, labels)\n",
        "            - documents: Lista de documentos, cada uno es una lista de tokens procesados.\n",
        "            - labels: Lista de etiquetas correspondientes a cada documento.\n",
        "    \"\"\"\n",
        "    print(\"Iniciando el preprocesamiento del corpus\")\n",
        "    documents = []\n",
        "    labels = []\n",
        "    word_counter = Counter()\n",
        "\n",
        "    # Leer todo el contenido del archivo\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    # Dividir el contenido en documentos utilizando doble salto de lÃ­nea como separador\n",
        "    raw_documents = content.strip().split('\\n\\n')  # Ajustar segÃºn el formato real del archivo\n",
        "\n",
        "    # Asegurar que el nÃºmero de documentos coincide con el nÃºmero de tÃ­tulos\n",
        "    min_length = min(len(raw_documents), len(articulos))\n",
        "    raw_documents = raw_documents[:min_length]\n",
        "    \n",
        "    articulos = articulos[:min_length]\n",
        "\n",
        "    for idx, text in enumerate(raw_documents):\n",
        "        # TokenizaciÃ³n y preprocesamiento\n",
        "        tokens = tokenize(text)\n",
        "        stems = [stem(token) for token in tokens]\n",
        "        tokens_filtered = [word for word in stems if word not in stopwords]\n",
        "        word_counter.update(tokens_filtered)\n",
        "        documents.append(tokens_filtered)\n",
        "        labels.append(articulos[idx])\n",
        "\n",
        "    # Filtrado de palabras raras\n",
        "    threshold = 3\n",
        "    frequent_words = {word for word, count in word_counter.items() if count >= threshold}\n",
        "\n",
        "    # Actualizar los documentos con las palabras frecuentes\n",
        "    final_documents = []\n",
        "    for doc in documents:\n",
        "        doc_filtered = [word for word in doc if word in frequent_words]\n",
        "        final_documents.append(doc_filtered)\n",
        "\n",
        "    print(f\"TamaÃ±o del vocabulario final: {len(frequent_words)} palabras\")\n",
        "    return final_documents, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "3CEVk834-Lng"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iniciando el preprocesamiento del corpus\n",
            "TamaÃ±o del vocabulario final: 16821 palabras\n"
          ]
        }
      ],
      "source": [
        "# Preprocesar el corpus y obtener documentos y etiquetas\n",
        "documents, labels = preprocess_corpus(filename, articulos_lsa)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhkqiftyQvFm"
      },
      "source": [
        "## ImplementaciÃ³n inicial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "BBGRpcSi-SpD"
      },
      "outputs": [],
      "source": [
        "def lsa_initial(documents, k):\n",
        "    \"\"\"\n",
        "    AnÃ¡lisis SemÃ¡ntico Latente (LSA) implementaciÃ³n inicial.\n",
        "\n",
        "    Pasos:\n",
        "    1. Construir la matriz tÃ©rmino-documento X con pesos TF-IDF.\n",
        "    2. Implementar SVD para descomponer X en U, Î£, V^T.\n",
        "    3. ReducciÃ³n de Dimensionalidad: Seleccionar los k valores singulares mÃ¡s grandes y sus vectores correspondientes.\n",
        "    4. ProyecciÃ³n: Representar tÃ©rminos y documentos en el espacio reducido.\n",
        "\n",
        "    ParÃ¡metros:\n",
        "    - documents: Lista de documentos, cada documento es una lista de tokens.\n",
        "    - k: NÃºmero de dimensiones para reducir.\n",
        "\n",
        "    Retorna:\n",
        "    - term_vectors: Vectores de tÃ©rminos en dimensionalidad reducida.\n",
        "    - document_vectors: Vectores de documentos en dimensionalidad reducida.\n",
        "    - term_index: Diccionario que mapea tÃ©rminos a Ã­ndices.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    import math\n",
        "    from collections import defaultdict\n",
        "\n",
        "    # Construir la matriz tÃ©rmino-documento X con pesos TF-IDF.\n",
        "    # Construir vocabulario\n",
        "    vocabulary = set()\n",
        "    for doc in documents:\n",
        "        vocabulary.update(doc)\n",
        "    vocabulary = list(vocabulary)\n",
        "    term_index = {term: idx for idx, term in enumerate(vocabulary)}\n",
        "    num_terms = len(vocabulary)\n",
        "    num_docs = len(documents)\n",
        "\n",
        "    # Inicializar matriz tÃ©rmino-documento\n",
        "    X = np.zeros((num_terms, num_docs))\n",
        "\n",
        "    # Calcular frecuencias de tÃ©rminos y frecuencias de documentos\n",
        "    df = defaultdict(int)  # Frecuencia de documentos\n",
        "    for idx, doc in enumerate(documents):\n",
        "        term_counts = defaultdict(int)\n",
        "        for term in doc:\n",
        "            term_counts[term] += 1\n",
        "        for term, count in term_counts.items():\n",
        "            term_idx = term_index[term]\n",
        "            X[term_idx, idx] = count  # Frecuencia de tÃ©rmino\n",
        "        for term in set(doc):\n",
        "            df[term] += 1\n",
        "\n",
        "    # Calcular IDF y aplicar ponderaciÃ³n TF-IDF\n",
        "    idf = {}\n",
        "    for term in vocabulary:\n",
        "        idf[term] = math.log(num_docs / (1 + df[term]))\n",
        "    for term, idx in term_index.items():\n",
        "        X[idx, :] = X[idx, :] * idf[term]\n",
        "\n",
        "    # Implementar SVD\n",
        "    U, Sigma, VT = np.linalg.svd(X, full_matrices=False)\n",
        "\n",
        "    # ReducciÃ³n de Dimensionalidad\n",
        "    U_k = U[:, :k]\n",
        "    Sigma_k = np.diag(Sigma[:k])\n",
        "    VT_k = VT[:k, :]\n",
        "\n",
        "    # ProyecciÃ³n\n",
        "    term_vectors = np.dot(U_k, Sigma_k)\n",
        "    document_vectors = np.dot(Sigma_k, VT_k)\n",
        "\n",
        "    return term_vectors, document_vectors.T, term_index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "455Zig2R-X5E"
      },
      "outputs": [],
      "source": [
        "def lsa_improved(documents, k, iterations=5):\n",
        "    \"\"\"\n",
        "    AnÃ¡lisis SemÃ¡ntico Latente (LSA) implementaciÃ³n mejorada usando mÃ©todo iterativo.\n",
        "\n",
        "    Mejoras:\n",
        "    - Implementar SVD truncado usando el mÃ©todo de potencia para calcular los k valores y vectores singulares mÃ¡s significativos.\n",
        "    - Optimizar operaciones usando matrices dispersas.\n",
        "\n",
        "    ParÃ¡metros:\n",
        "    - documents: Lista de documentos, cada documento es una lista de tokens.\n",
        "    - k: NÃºmero de dimensiones para reducir.\n",
        "    - iterations: NÃºmero de iteraciones para el mÃ©todo de potencia.\n",
        "\n",
        "    Retorna:\n",
        "    - term_vectors: Vectores de tÃ©rminos en dimensionalidad reducida.\n",
        "    - document_vectors: Vectores de documentos en dimensionalidad reducida.\n",
        "    - term_index: Diccionario que mapea tÃ©rminos a Ã­ndices.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    import math\n",
        "    from collections import defaultdict\n",
        "    from scipy.sparse import csc_matrix\n",
        "\n",
        "    # Construir la matriz tÃ©rmino-documento X con pesos TF-IDF.\n",
        "    # Construir vocabulario\n",
        "    vocabulary = set()\n",
        "    for doc in documents:\n",
        "        vocabulary.update(doc)\n",
        "    vocabulary = list(vocabulary)\n",
        "    term_index = {term: idx for idx, term in enumerate(vocabulary)}\n",
        "    num_terms = len(vocabulary)\n",
        "    num_docs = len(documents)\n",
        "\n",
        "    # Inicializar datos para matriz dispersa\n",
        "    data = []\n",
        "    rows = []\n",
        "    cols = []\n",
        "\n",
        "    # Calcular frecuencias de tÃ©rminos y frecuencias de documentos\n",
        "    df = defaultdict(int)  # Frecuencia de documentos\n",
        "    for idx, doc in enumerate(documents):\n",
        "        term_counts = defaultdict(int)\n",
        "        for term in doc:\n",
        "            term_counts[term] += 1\n",
        "        for term, count in term_counts.items():\n",
        "            term_idx = term_index[term]\n",
        "            data.append(count)\n",
        "            rows.append(term_idx)\n",
        "            cols.append(idx)\n",
        "        for term in set(doc):\n",
        "            df[term] += 1\n",
        "\n",
        "    # Crear matriz tÃ©rmino-documento dispersa\n",
        "    X = csc_matrix((data, (rows, cols)), shape=(num_terms, num_docs))\n",
        "\n",
        "    # Calcular IDF y aplicar ponderaciÃ³n TF-IDF\n",
        "    idf = {}\n",
        "    for term in vocabulary:\n",
        "        idf[term] = math.log(num_docs / (1 + df[term]))\n",
        "    for term, idx in term_index.items():\n",
        "        X[idx, :] = X[idx, :].multiply(idf[term])\n",
        "\n",
        "    # Implementar SVD truncado usando el mÃ©todo de potencia\n",
        "    def truncated_svd(X, k, iterations):\n",
        "        n, m = X.shape\n",
        "        V = np.random.rand(m, k)\n",
        "        for _ in range(iterations):\n",
        "            U = X.dot(V)\n",
        "            U, _ = np.linalg.qr(U)\n",
        "            V = X.T.dot(U)\n",
        "            V, _ = np.linalg.qr(V)\n",
        "        Sigma = np.diag(np.linalg.norm(X.dot(V), axis=0))\n",
        "        return U, Sigma, V.T\n",
        "\n",
        "    U_k, Sigma_k, VT_k = truncated_svd(X, k, iterations)\n",
        "\n",
        "    # ProyecciÃ³n\n",
        "    term_vectors = U_k.dot(Sigma_k)\n",
        "    document_vectors = VT_k.T.dot(Sigma_k)\n",
        "\n",
        "    return term_vectors, document_vectors, term_index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "iOFW4b0X-sAK"
      },
      "outputs": [],
      "source": [
        "def classify_documents(document_vectors_train, labels_train, document_vectors_test, k=3):\n",
        "    \"\"\"\n",
        "    Clasificar documentos usando el algoritmo k-NN.\n",
        "\n",
        "    ParÃ¡metros:\n",
        "    - document_vectors_train: Array de vectores de documentos de entrenamiento.\n",
        "    - labels_train: Lista de etiquetas para los documentos de entrenamiento.\n",
        "    - document_vectors_test: Array de vectores de documentos a clasificar.\n",
        "    - k: NÃºmero de vecinos a considerar.\n",
        "\n",
        "    Retorna:\n",
        "    - predictions: Etiquetas predichas para los documentos de prueba.\n",
        "    \"\"\"\n",
        "    from collections import Counter\n",
        "    import numpy as np\n",
        "\n",
        "    predictions = []\n",
        "    for test_vec in document_vectors_test:\n",
        "        # Calcular distancias a todos los documentos de entrenamiento\n",
        "        distances = np.linalg.norm(document_vectors_train - test_vec, axis=1)\n",
        "        # Obtener Ã­ndices de los k vecinos mÃ¡s cercanos\n",
        "        neighbor_indices = distances.argsort()[:k]\n",
        "        # Obtener etiquetas de los vecinos mÃ¡s cercanos\n",
        "        neighbor_labels = [labels_train[idx] for idx in neighbor_indices]\n",
        "        # Predecir la etiqueta mÃ¡s comÃºn\n",
        "        most_common = Counter(neighbor_labels).most_common(1)[0][0]\n",
        "        predictions.append(most_common)\n",
        "\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "bQ9ZH17u-rGl"
      },
      "outputs": [],
      "source": [
        "def evaluate_classification(true_labels, predicted_labels):\n",
        "    \"\"\"\n",
        "    Evaluar la exactitud de la clasificaciÃ³n.\n",
        "\n",
        "    ParÃ¡metros:\n",
        "    - true_labels: Lista de etiquetas verdaderas.\n",
        "    - predicted_labels: Lista de etiquetas predichas.\n",
        "\n",
        "    Retorna:\n",
        "    - accuracy: Exactitud de la clasificaciÃ³n.\n",
        "    \"\"\"\n",
        "    correct = sum(t == p for t, p in zip(true_labels, predicted_labels))\n",
        "    accuracy = correct / len(true_labels)\n",
        "    return round(accuracy, 4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "IGdr3yaLSAkz"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def split_data(documents, labels, test_size=0.2, random_state=42):\n",
        "    \"\"\"\n",
        "    Divide los documentos y etiquetas en conjuntos de entrenamiento y prueba\n",
        "\n",
        "    Args:\n",
        "        documents (list): Lista de documentos tokenizados.\n",
        "        labels (list): Lista de etiquetas correspondientes a cada documento.\n",
        "        test_size (float): ProporciÃ³n del conjunto de prueba.\n",
        "        random_state (int): Semilla para el generador de nÃºmeros aleatorios.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (documents_train, documents_test, labels_train, labels_test)\n",
        "    \"\"\"\n",
        "    # Mezclar los datos para asegurar aleatoriedad\n",
        "    data = list(zip(documents, labels))\n",
        "    random.seed(random_state)\n",
        "    random.shuffle(data)\n",
        "    documents, labels = zip(*data)\n",
        "\n",
        "    # Calcular el tamaÃ±o del conjunto de prueba\n",
        "    split_index = int(len(documents) * (1 - test_size))\n",
        "    documents_train, documents_test = documents[:split_index], documents[split_index:]\n",
        "    labels_train, labels_test = labels[:split_index], labels[split_index:]\n",
        "\n",
        "    return list(documents_train), list(documents_test), list(labels_train), list(labels_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "g2yjSMRoV47a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tiempo de LSA inicial: 185.9575126171112 segundos\n",
            "Uso de memoria LSA inicial: Actual=14.98612 MB; Pico=1548.46632 MB\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import tracemalloc\n",
        "import numpy as np\n",
        "\n",
        "# Establecer parÃ¡metros\n",
        "k_dimensions = 80  # Dimensiones reducidas\n",
        "\n",
        "# ImplementaciÃ³n Inicial\n",
        "start_time = time.time()\n",
        "tracemalloc.start()\n",
        "\n",
        "# LSA inicial\n",
        "term_vectors_initial, doc_vectors_initial_train, term_index = lsa_initial(documents, k_dimensions)\n",
        "\n",
        "current, peak = tracemalloc.get_traced_memory()\n",
        "end_time = time.time()\n",
        "tracemalloc.stop()\n",
        "\n",
        "print(f\"Tiempo de LSA inicial: {end_time - start_time} segundos\")\n",
        "print(f\"Uso de memoria LSA inicial: Actual={current / 10**6} MB; Pico={peak / 10**6} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "kIRE-utyWFG7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tiempo de LSA con mejoras: 83.52752566337585 segundos\n",
            "Uso de memoria LSA con mejoras: Actual=15.042239 MB; Pico=48.805111 MB\n"
          ]
        }
      ],
      "source": [
        "# ImplementaciÃ³n Mejorada\n",
        "start_time = time.time()\n",
        "tracemalloc.start()\n",
        "\n",
        "# LSA mejorada\n",
        "term_vectors_improved, doc_vectors_improved_train, term_index = lsa_improved(documents, k_dimensions)\n",
        "\n",
        "current, peak = tracemalloc.get_traced_memory()\n",
        "end_time = time.time()\n",
        "tracemalloc.stop()\n",
        "\n",
        "print(f\"Tiempo de LSA con mejoras: {end_time - start_time} segundos\")\n",
        "print(f\"Uso de memoria LSA con mejoras: Actual={current / 10**6} MB; Pico={peak / 10**6} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BccuUR5yNSeq"
      },
      "source": [
        "# Word2Ve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rBueVtnWlqJ"
      },
      "source": [
        "# Preprocesamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ezzU5cZeWlHm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Todos los artÃ­culos han sido descargados y guardados en 'corpus.txt'.\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import json\n",
        "\n",
        "# Lista de tÃ­tulos de artÃ­culos de Wikipedia en espaÃ±ol\n",
        "articulos = [\n",
        "    \"Inteligencia artificial\", \"Aprendizaje automÃ¡tico\", \"BiotecnologÃ­a\", \"NanotecnologÃ­a\", \"GenÃ©tica\",\n",
        "    \"TecnologÃ­a de la informaciÃ³n\", \"RobÃ³tica\", \"RevoluciÃ³n Industrial\", \"Segunda Guerra Mundial\", \"Edad Media\",\n",
        "    \"Guerra FrÃ­a\", \"RevoluciÃ³n francesa\", \"Historia de la ciencia\", \"Renacimiento\", \"Literatura en espaÃ±ol\",\n",
        "    \"Pintura renacentista\", \"MÃºsica clÃ¡sica\"\n",
        "]\n",
        "\n",
        "# Base URL de la API de Wikipedia para obtener el extracto en formato JSON\n",
        "base_url = 'https://es.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&explaintext&titles={}'\n",
        "\n",
        "# Archivo para guardar el corpus\n",
        "filename = 'corpus.txt'\n",
        "\n",
        "with open(filename, 'w', encoding='utf-8') as file:\n",
        "    for title in articulos:\n",
        "        # Codificar el tÃ­tulo para URL\n",
        "        encoded_title = urllib.request.quote(title)\n",
        "        url = base_url.format(encoded_title)\n",
        "        #print(f\"Descargando el artÃ­culo: {title}...\")\n",
        "        response = urllib.request.urlopen(url)\n",
        "        data = response.read().decode('utf-8')\n",
        "        # Extraer el texto del JSON\n",
        "        json_data = json.loads(data)\n",
        "        pages = json_data['query']['pages']\n",
        "        page = next(iter(pages.values()))\n",
        "        extract = page.get('extract', '')\n",
        "        # Escribir el texto en el archivo si el extracto no estÃ¡ vacÃ­o\n",
        "        if extract:\n",
        "            file.write(extract + '\\n')\n",
        "            #print(f\"ArtÃ­culo '{title}' descargado.\")\n",
        "        else:\n",
        "            print(f\"No se pudo descargar el artÃ­culo '{title}'.\")\n",
        "print(f\"Todos los artÃ­culos han sido descargados y guardados en '{filename}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0ggAc9nLWvj_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iniciando el preprocesamiento del corpus...\n",
            "TamaÃ±o del vocabulario final: 3281 palabras\n",
            "Preprocesamiento completado.\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "def tokenize(text):\n",
        "    # Convertir todo a minÃºsculas\n",
        "    text = text.lower()\n",
        "    # Remover puntuaciÃ³n\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "    # Dividir en palabras\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "def stem(word):\n",
        "    suffixes = ['aciones', 'imientos', 'amiento', 'iciÃ³n', 'adora', 'aciÃ³n', 'adoras', 'adores', 'ante',\n",
        "                'ancia', 'mente', 'idad', 'ivas', 'ivos', 'anza', 'icos', 'icas', 'ico', 'ica',\n",
        "                'oso', 'osa', 'osos', 'osas', 'ismo', 'ismos', 'able', 'ables', 'ible', 'ibles',\n",
        "                'ente', 'entes', 'mente']\n",
        "    for suffix in suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            return word[:-len(suffix)]\n",
        "    return word\n",
        "\n",
        "stopwords = set([\n",
        "    'a', 'al', 'algo', 'algunas', 'algunos', 'ante', 'antes', 'como',\n",
        "    'con', 'contra', 'de', 'del', 'desde', 'donde', 'durante', 'e',\n",
        "    'el', 'ella', 'ellas', 'ellos', 'en', 'entre', 'era', 'erais',\n",
        "    'eran', 'eras', 'eres', 'es', 'esa', 'esas', 'ese', 'eso',\n",
        "    'esos', 'esta', 'estaba', 'estabais', 'estaban', 'estabas',\n",
        "    'estad', 'estada', 'estadas', 'estado', 'estados', 'estamos',\n",
        "    'estando', 'estar', 'estaremos', 'estarÃ¡', 'estarÃ¡n', 'estarÃ¡s',\n",
        "    'estarÃ©', 'estarÃ©is', 'estarÃ­a', 'estarÃ­ais', 'estarÃ­amos',\n",
        "    'estarÃ­an', 'estarÃ­as', 'estas', 'este', 'estemos', 'esto',\n",
        "    'estos', 'estoy', 'estuve', 'estuviera', 'estuvierais',\n",
        "    'estuvieran', 'estuvieras', 'estuvieron', 'estuviese',\n",
        "    'estuvieseis', 'estuviesen', 'estuvieses', 'estuvimos',\n",
        "    'estuviste', 'estuvisteis', 'estuviÃ©ramos', 'estuviÃ©semos',\n",
        "    'estuvo', 'estÃ¡', 'estÃ¡bamos', 'estÃ¡is', 'estÃ¡n', 'estÃ¡s',\n",
        "    'estÃ©', 'estÃ©is', 'estÃ©n', 'estÃ©s', 'fue', 'fuera', 'fuerais',\n",
        "    'fueran', 'fueras', 'fueron', 'fuese', 'fueseis', 'fuesen',\n",
        "    'fueses', 'fui', 'fuimos', 'fuiste', 'fuisteis', 'ha', 'habida',\n",
        "    'habidas', 'habido', 'habidos', 'habiendo', 'habremos', 'habrÃ¡',\n",
        "    'habrÃ¡n', 'habrÃ¡s', 'habrÃ©', 'habrÃ©is', 'habrÃ­a', 'habrÃ­ais',\n",
        "    'habrÃ­amos', 'habrÃ­an', 'habrÃ­as', 'habÃ©is', 'habÃ­a', 'habÃ­ais',\n",
        "    'habÃ­amos', 'habÃ­an', 'habÃ­as', 'han', 'has', 'hasta', 'hay',\n",
        "    'haya', 'hayamos', 'hayan', 'hayas', 'hayÃ¡is', 'he', 'hemos',\n",
        "    'hube', 'hubiera', 'hubierais', 'hubieran', 'hubieras',\n",
        "    'hubieron', 'hubiese', 'hubieseis', 'hubiesen', 'hubieses',\n",
        "    'hubimos', 'hubiste', 'hubisteis', 'hubiÃ©ramos', 'hubiÃ©semos',\n",
        "    'hubo', 'la', 'las', 'le', 'les', 'lo', 'los', 'me', 'mi',\n",
        "    'mis', 'mucho', 'muchos', 'muy', 'mÃ¡s', 'mÃ­', 'mÃ­a', 'mÃ­as',\n",
        "    'mÃ­o', 'mÃ­os', 'nada', 'ni', 'no', 'nos', 'nosotras', 'nosotros',\n",
        "    'nuestra', 'nuestras', 'nuestro', 'nuestros', 'o', 'os', 'otra',\n",
        "    'otras', 'otro', 'otros', 'para', 'pero', 'poco', 'por', 'porque',\n",
        "    'que', 'quien', 'quienes', 'quÃ©', 'se', 'sea', 'seamos', 'sean',\n",
        "    'seas', 'seremos', 'serÃ¡', 'serÃ¡n', 'serÃ¡s', 'serÃ©', 'serÃ©is',\n",
        "    'serÃ­a', 'serÃ­ais', 'serÃ­amos', 'serÃ­an', 'serÃ­as', 'seÃ¡is',\n",
        "    'si', 'sido', 'siendo', 'sin', 'sobre', 'sois', 'somos', 'son',\n",
        "    'soy', 'su', 'sus', 'suya', 'suyas', 'suyo', 'suyos', 'sÃ­', 'tambiÃ©n',\n",
        "    'tanto', 'te', 'tendremos', 'tendrÃ¡', 'tendrÃ¡n', 'tendrÃ¡s',\n",
        "    'tendrÃ©', 'tendrÃ©is', 'tendrÃ­a', 'tendrÃ­ais', 'tendrÃ­amos',\n",
        "    'tendrÃ­an', 'tendrÃ­as', 'tened', 'tenemos', 'tenga', 'tengamos',\n",
        "    'tengan', 'tengas', 'tengo', 'tengÃ¡is', 'tenida', 'tenidas',\n",
        "    'tenido', 'tenidos', 'teniendo', 'tenÃ©is', 'tenÃ­a', 'tenÃ­ais',\n",
        "    'tenÃ­amos', 'tenÃ­an', 'tenÃ­as', 'ti', 'tiene', 'tienen', 'tienes',\n",
        "    'todo', 'todos', 'tu', 'tus', 'tuve', 'tuviera', 'tuvierais',\n",
        "    'tuvieran', 'tuvieras', 'tuvieron', 'tuviese', 'tuvieseis',\n",
        "    'tuviesen', 'tuvieses', 'tuvimos', 'tuviste', 'tuvisteis',\n",
        "    'tuviÃ©ramos', 'tuviÃ©semos', 'tuvo', 'tÃº', 'un', 'una', 'uno',\n",
        "    'unos', 'vosotras', 'vosotros', 'vuestra', 'vuestras', 'vuestro',\n",
        "    'vuestros', 'y', 'ya', 'yo', 'Ã©l', 'Ã©ramos'\n",
        "])\n",
        "\n",
        "print(\"Iniciando el preprocesamiento del corpus...\")\n",
        "with open('corpus.txt', 'r', encoding='utf-8') as file:\n",
        "    # Procesar el archivo lÃ­nea por lÃ­nea para manejar archivos grandes\n",
        "    word_counter = Counter()\n",
        "    final_tokens = []\n",
        "    for line in file:\n",
        "        tokens = tokenize(line)\n",
        "        stems = [stem(token) for token in tokens]\n",
        "        tokens_filtered = [word for word in stems if word not in stopwords]\n",
        "        word_counter.update(tokens_filtered)\n",
        "        final_tokens.extend(tokens_filtered)\n",
        "\n",
        "# Establecer el umbral de frecuencia\n",
        "threshold = 5\n",
        "\n",
        "# Obtener palabras que cumplen el umbral\n",
        "frequent_words = {word for word, count in word_counter.items() if count >= threshold}\n",
        "\n",
        "# Filtrar las palabras raras\n",
        "final_tokens = [word for word in final_tokens if word in frequent_words]\n",
        "\n",
        "print(f\"TamaÃ±o del vocabulario final: {len(set(final_tokens))} palabras\")\n",
        "print(\"Preprocesamiento completado.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6juCeOIGNWsp"
      },
      "source": [
        "## ImplementaciÃ³n de CBOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "9dBDbvk5NREw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NÃºmero de muestras de entrenamiento: 55805\n",
            "Epoch 1/50, PÃ©rdida: 8.150338530613071\n",
            "Epoch 2/50, PÃ©rdida: 7.4808715852180425\n",
            "Epoch 3/50, PÃ©rdida: 6.980647591927219\n",
            "Epoch 4/50, PÃ©rdida: 6.5984203726249175\n",
            "Epoch 5/50, PÃ©rdida: 6.274850797763866\n",
            "Epoch 6/50, PÃ©rdida: 5.988151156826689\n",
            "Epoch 7/50, PÃ©rdida: 5.728892169815979\n",
            "Epoch 8/50, PÃ©rdida: 5.492144297698888\n",
            "Epoch 9/50, PÃ©rdida: 5.274788892160153\n",
            "Epoch 10/50, PÃ©rdida: 5.074720989628966\n",
            "Epoch 11/50, PÃ©rdida: 4.890112613342857\n",
            "Epoch 12/50, PÃ©rdida: 4.719334432518657\n",
            "Epoch 13/50, PÃ©rdida: 4.5610181967337295\n",
            "Epoch 14/50, PÃ©rdida: 4.413986118020979\n",
            "Epoch 15/50, PÃ©rdida: 4.277204574388703\n",
            "Epoch 16/50, PÃ©rdida: 4.149760320882509\n",
            "Epoch 17/50, PÃ©rdida: 4.0308404865246255\n",
            "Epoch 18/50, PÃ©rdida: 3.919710051012402\n",
            "Epoch 19/50, PÃ©rdida: 3.8156951331735347\n",
            "Epoch 20/50, PÃ©rdida: 3.718175700941419\n",
            "Epoch 21/50, PÃ©rdida: 3.626584087840843\n",
            "Epoch 22/50, PÃ©rdida: 3.5404057369027826\n",
            "Epoch 23/50, PÃ©rdida: 3.4591788343222056\n",
            "Epoch 24/50, PÃ©rdida: 3.3824904597395853\n",
            "Epoch 25/50, PÃ©rdida: 3.309970496216632\n",
            "Epoch 26/50, PÃ©rdida: 3.2412856606095395\n",
            "Epoch 27/50, PÃ©rdida: 3.1761348005778105\n",
            "Epoch 28/50, PÃ©rdida: 3.1142453452679333\n",
            "Epoch 29/50, PÃ©rdida: 3.0553703126484324\n",
            "Epoch 30/50, PÃ©rdida: 2.999285532270458\n",
            "Epoch 31/50, PÃ©rdida: 2.9457871667176234\n",
            "Epoch 32/50, PÃ©rdida: 2.8946897146138317\n",
            "Epoch 33/50, PÃ©rdida: 2.8458244516173976\n",
            "Epoch 34/50, PÃ©rdida: 2.799038086946966\n",
            "Epoch 35/50, PÃ©rdida: 2.7541914551509303\n",
            "Epoch 36/50, PÃ©rdida: 2.7111581672250504\n",
            "Epoch 37/50, PÃ©rdida: 2.6698232018359027\n",
            "Epoch 38/50, PÃ©rdida: 2.630081479873699\n",
            "Epoch 39/50, PÃ©rdida: 2.591836530889666\n",
            "Epoch 40/50, PÃ©rdida: 2.554999349005946\n",
            "Epoch 41/50, PÃ©rdida: 2.5194874814644193\n",
            "Epoch 42/50, PÃ©rdida: 2.4852243415129567\n",
            "Epoch 43/50, PÃ©rdida: 2.452138697169438\n",
            "Epoch 44/50, PÃ©rdida: 2.4201642834249046\n",
            "Epoch 45/50, PÃ©rdida: 2.3892394958047585\n",
            "Epoch 46/50, PÃ©rdida: 2.3593071268914003\n",
            "Epoch 47/50, PÃ©rdida: 2.330314112280626\n",
            "Epoch 48/50, PÃ©rdida: 2.3022112621271233\n",
            "Epoch 49/50, PÃ©rdida: 2.274952971322425\n",
            "Epoch 50/50, PÃ©rdida: 2.248496921293021\n",
            "Tiempo de CBOW inicial: 4116.254072189331 segundos\n",
            "Uso de memoria CBOW inicial: Actual=48.942901 MB; Pico=51.809762 MB\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tracemalloc\n",
        "import time\n",
        "\n",
        "def build_vocabulary(tokens):\n",
        "    \"\"\"\n",
        "    Construye el vocabulario a partir de los tokens y genera mapas de palabras a Ã­ndices y viceversa.\n",
        "\n",
        "    Args:\n",
        "        tokens (list): Lista de tokens del corpus.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (vocab_size, word_to_idx, idx_to_word)\n",
        "            - vocab_size (int): TamaÃ±o del vocabulario.\n",
        "            - word_to_idx (dict): Diccionario que asigna a cada palabra un Ã­ndice.\n",
        "            - idx_to_word (dict): Diccionario que asigna a cada Ã­ndice una palabra.\n",
        "    \"\"\"\n",
        "    vocab = set(tokens)\n",
        "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "    vocab_size = len(vocab)\n",
        "    return vocab_size, word_to_idx, idx_to_word\n",
        "\n",
        "def initialize_weights(vocab_size, embedding_dim):\n",
        "    \"\"\"\n",
        "    Inicializa las matrices de pesos para los embeddings.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): TamaÃ±o del vocabulario.\n",
        "        embedding_dim (int): DimensiÃ³n de los embeddings.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (W1, W2)\n",
        "            - W1 (np.ndarray): Matriz de pesos inicial entre input y capa oculta.\n",
        "            - W2 (np.ndarray): Matriz de pesos inicial entre capa oculta y output.\n",
        "    \"\"\"\n",
        "    W1 = np.random.uniform(-0.8, 0.8, (vocab_size, embedding_dim))\n",
        "    W2 = np.random.uniform(-0.8, 0.8, (embedding_dim, vocab_size))\n",
        "    return W1, W2\n",
        "\n",
        "def generate_cbow_data(tokens, window_size):\n",
        "    \"\"\"\n",
        "    Genera los datos de entrenamiento para el modelo CBOW.\n",
        "\n",
        "    Args:\n",
        "        tokens (list): Lista de tokens del corpus.\n",
        "        window_size (int): TamaÃ±o de la ventana de contexto.\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de tuplas (context, target) para entrenamiento.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for i in range(window_size, len(tokens) - window_size):\n",
        "        context = tokens[i - window_size:i] + tokens[i + 1:i + window_size + 1]\n",
        "        target = tokens[i]\n",
        "        data.append((context, target))\n",
        "    return data\n",
        "\n",
        "def one_hot_encode(word_idx, vocab_size):\n",
        "    \"\"\"\n",
        "    Codifica una palabra en un vector one-hot.\n",
        "\n",
        "    Args:\n",
        "        word_idx (int): Ãndice de la palabra a codificar.\n",
        "        vocab_size (int): TamaÃ±o del vocabulario.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Vector one-hot de longitud vocab_size con un 1 en la posiciÃ³n de word_idx.\n",
        "    \"\"\"\n",
        "    vec = np.zeros(vocab_size)\n",
        "    vec[word_idx] = 1\n",
        "    return vec\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    Calcula la funciÃ³n softmax de un vector.\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): Vector de entrada.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Vector con los valores softmax.\n",
        "    \"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "def train_cbow(training_data, W1, W2, word_to_idx, vocab_size, learning_rate, epochs, window_size):\n",
        "    \"\"\"\n",
        "    Entrena el modelo CBOW utilizando descenso de gradiente y softmax.\n",
        "\n",
        "    Args:\n",
        "        training_data (list): Datos de entrenamiento (context, target).\n",
        "        W1 (np.ndarray): Matriz de pesos entre input y capa oculta.\n",
        "        W2 (np.ndarray): Matriz de pesos entre capa oculta y output.\n",
        "        word_to_idx (dict): Diccionario que asigna Ã­ndices a palabras.\n",
        "        vocab_size (int): TamaÃ±o del vocabulario.\n",
        "        learning_rate (float): Tasa de aprendizaje para el descenso de gradiente.\n",
        "        epochs (int): NÃºmero de Ã©pocas de entrenamiento.\n",
        "        window_size (int): TamaÃ±o de la ventana de contexto.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (W1, W2) Matrices de pesos actualizadas.\n",
        "    \"\"\"\n",
        "    for epoch in range(epochs):\n",
        "        loss = 0\n",
        "        for context_words, target_word in training_data:\n",
        "            # Vector de entrada (promedio de los embeddings de las palabras de contexto)\n",
        "            context_indices = [word_to_idx[word] for word in context_words]\n",
        "            x = np.mean(W1[context_indices], axis=0)  # Shape: (embedding_dim,)\n",
        "\n",
        "            # CÃ¡lculo hacia adelante\n",
        "            z = np.dot(x, W2)  # Shape: (vocab_size,)\n",
        "            y_pred = softmax(z)\n",
        "\n",
        "            # Vector de salida deseado (one-hot encoding)\n",
        "            y_true = one_hot_encode(word_to_idx[target_word], vocab_size)\n",
        "\n",
        "            # CÃ¡lculo de la pÃ©rdida (entropÃ­a cruzada)\n",
        "            loss -= np.log(y_pred[word_to_idx[target_word]] + 1e-9)\n",
        "\n",
        "            # CÃ¡lculo del error\n",
        "            e = y_pred - y_true  # Shape: (vocab_size,)\n",
        "\n",
        "            # CÃ¡lculo hacia atrÃ¡s (gradientes)\n",
        "            dW2 = np.outer(x, e)  # Shape: (embedding_dim, vocab_size)\n",
        "            dW1 = np.dot(W2, e) / (2 * window_size)  # Promedio para las palabras de contexto\n",
        "\n",
        "            # ActualizaciÃ³n de pesos\n",
        "            W1[context_indices] -= learning_rate * dW1\n",
        "            W2 -= learning_rate * dW2\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, PÃ©rdida: {loss / len(training_data)}\")\n",
        "    return W1, W2\n",
        "\n",
        "def measure_memory_time(func, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Mide el tiempo de ejecuciÃ³n y el uso de memoria de una funciÃ³n.\n",
        "\n",
        "    Args:\n",
        "        func (callable): FunciÃ³n a medir.\n",
        "        *args: Argumentos para la funciÃ³n.\n",
        "        **kwargs: Argumentos nombrados para la funciÃ³n.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (result, tiempo_ejecucion, current, peak)\n",
        "            - result: Resultado de la funciÃ³n.\n",
        "            - tiempo_ejecucion (float): Tiempo de ejecuciÃ³n en segundos.\n",
        "            - current (float): Uso de memoria actual en MB.\n",
        "            - peak (float): Uso mÃ¡ximo de memoria en MB.\n",
        "    \"\"\"\n",
        "    tracemalloc.start()\n",
        "    start_time = time.time()\n",
        "    result = func(*args, **kwargs)\n",
        "    end_time = time.time()\n",
        "    current, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "    tiempo_ejecucion = end_time - start_time\n",
        "    return result, tiempo_ejecucion, current / 10**6, peak / 10**6\n",
        "\n",
        "# EjecuciÃ³n del modelo CBOW\n",
        "\n",
        "# Definir los parÃ¡metros\n",
        "embedding_dim = 50\n",
        "window_size = 2\n",
        "learning_rate = 0.05\n",
        "epochs = 50\n",
        "\n",
        "# Construir vocabulario\n",
        "vocab_size, word_to_idx, idx_to_word = build_vocabulary(final_tokens)\n",
        "\n",
        "# Inicializar pesos\n",
        "W1, W2 = initialize_weights(vocab_size, embedding_dim)\n",
        "\n",
        "# Generar datos de entrenamiento\n",
        "training_data = generate_cbow_data(final_tokens, window_size)\n",
        "print(f\"NÃºmero de muestras de entrenamiento: {len(training_data)}\")\n",
        "\n",
        "# Entrenar y medir tiempo y memoria\n",
        "(W1, W2), tiempo_ejecucion, current_mem, peak_mem = measure_memory_time(\n",
        "    train_cbow, training_data, W1, W2, word_to_idx, vocab_size, learning_rate, epochs, window_size\n",
        ")\n",
        "\n",
        "# Resultados de tiempo y memoria\n",
        "print(f\"Tiempo de CBOW inicial: {tiempo_ejecucion} segundos\")\n",
        "print(f\"Uso de memoria CBOW inicial: Actual={current_mem} MB; Pico={peak_mem} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56ctjVjbYZ3p"
      },
      "source": [
        "# ImplementaciÃ³n de skip-gram con negative sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Oq6oTn1pYUw7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NÃºmero de muestras de entrenamiento: 223230\n",
            "Epoch 1/50, PÃ©rdida: 4.15867491761536\n",
            "Epoch 2/50, PÃ©rdida: 4.125658239919439\n",
            "Epoch 3/50, PÃ©rdida: 3.4878207855883163\n",
            "Epoch 4/50, PÃ©rdida: 2.6787751195252234\n",
            "Epoch 5/50, PÃ©rdida: 2.4247308190645045\n",
            "Epoch 6/50, PÃ©rdida: 2.350016609365114\n",
            "Epoch 7/50, PÃ©rdida: 2.320956133911222\n",
            "Epoch 8/50, PÃ©rdida: 2.3017132610258826\n",
            "Epoch 9/50, PÃ©rdida: 2.28391191292416\n",
            "Epoch 10/50, PÃ©rdida: 2.2607671848563826\n",
            "Epoch 11/50, PÃ©rdida: 2.233405705380576\n",
            "Epoch 12/50, PÃ©rdida: 2.205683352797052\n",
            "Epoch 13/50, PÃ©rdida: 2.175820872680901\n",
            "Epoch 14/50, PÃ©rdida: 2.1457642948034277\n",
            "Epoch 15/50, PÃ©rdida: 2.117418745847363\n",
            "Epoch 16/50, PÃ©rdida: 2.0905372548731282\n",
            "Epoch 17/50, PÃ©rdida: 2.061761188686031\n",
            "Epoch 18/50, PÃ©rdida: 2.031669964148663\n",
            "Epoch 19/50, PÃ©rdida: 2.004781745223011\n",
            "Epoch 20/50, PÃ©rdida: 1.9757679620528352\n",
            "Epoch 21/50, PÃ©rdida: 1.9462197657310016\n",
            "Epoch 22/50, PÃ©rdida: 1.9182686195333132\n",
            "Epoch 23/50, PÃ©rdida: 1.89070036226386\n",
            "Epoch 24/50, PÃ©rdida: 1.8624377682786502\n",
            "Epoch 25/50, PÃ©rdida: 1.8329061054491762\n",
            "Epoch 26/50, PÃ©rdida: 1.80454450442776\n",
            "Epoch 27/50, PÃ©rdida: 1.779543514680727\n",
            "Epoch 28/50, PÃ©rdida: 1.7487037625241317\n",
            "Epoch 29/50, PÃ©rdida: 1.7227043426791842\n",
            "Epoch 30/50, PÃ©rdida: 1.6956819259988898\n",
            "Epoch 31/50, PÃ©rdida: 1.6722445377951698\n",
            "Epoch 32/50, PÃ©rdida: 1.6455604981896543\n",
            "Epoch 33/50, PÃ©rdida: 1.6206330394893673\n",
            "Epoch 34/50, PÃ©rdida: 1.5963431718621126\n",
            "Epoch 35/50, PÃ©rdida: 1.5737609146638578\n",
            "Epoch 36/50, PÃ©rdida: 1.5546293979109984\n",
            "Epoch 37/50, PÃ©rdida: 1.5313666843490963\n",
            "Epoch 38/50, PÃ©rdida: 1.5119499750299787\n",
            "Epoch 39/50, PÃ©rdida: 1.4947220204952465\n",
            "Epoch 40/50, PÃ©rdida: 1.4770419023650079\n",
            "Epoch 41/50, PÃ©rdida: 1.4578845913351586\n",
            "Epoch 42/50, PÃ©rdida: 1.4425747491743655\n",
            "Epoch 43/50, PÃ©rdida: 1.4266929332673155\n",
            "Epoch 44/50, PÃ©rdida: 1.4129267394259832\n",
            "Epoch 45/50, PÃ©rdida: 1.3977279221431336\n",
            "Epoch 46/50, PÃ©rdida: 1.3847276774245516\n",
            "Epoch 47/50, PÃ©rdida: 1.3724582708932356\n",
            "Epoch 48/50, PÃ©rdida: 1.3607904948558283\n",
            "Epoch 49/50, PÃ©rdida: 1.3494786615515455\n",
            "Epoch 50/50, PÃ©rdida: 1.3380595917000142\n",
            "Tiempo de entrenamiento Skip-Gram inicial: 4098.213274717331 segundos\n",
            "Uso de memoria CBOW con mejoras: Actual=2.642986 MB; Pico=3.939404 MB\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def build_vocabulary(tokens):\n",
        "    \"\"\"\n",
        "    Construye el vocabulario y mapea palabras a Ã­ndices y viceversa.\n",
        "\n",
        "    Args:\n",
        "        tokens (list): Lista de tokens del corpus.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (vocab_size, word_to_idx, idx_to_word)\n",
        "            - vocab_size (int): TamaÃ±o del vocabulario.\n",
        "            - word_to_idx (dict): Diccionario que asigna Ã­ndices a palabras.\n",
        "            - idx_to_word (dict): Diccionario que asigna palabras a Ã­ndices.\n",
        "    \"\"\"\n",
        "    vocab = set(tokens)\n",
        "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "    vocab_size = len(vocab)\n",
        "    return vocab_size, word_to_idx, idx_to_word\n",
        "\n",
        "def initialize_weights(vocab_size, embedding_dim):\n",
        "    \"\"\"\n",
        "    Inicializa las matrices de pesos para las capas de embeddings.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): TamaÃ±o del vocabulario.\n",
        "        embedding_dim (int): DimensiÃ³n de los embeddings.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (W1, W2) Matrices de pesos inicializadas.\n",
        "    \"\"\"\n",
        "    W1 = np.random.uniform(-0.8, 0.8, (vocab_size, embedding_dim))\n",
        "    W2 = np.random.uniform(-0.8, 0.8, (embedding_dim, vocab_size))\n",
        "    return W1, W2\n",
        "\n",
        "def generate_skipgram_data(tokens, window_size):\n",
        "    \"\"\"\n",
        "    Genera los datos de entrenamiento para el modelo Skip-Gram.\n",
        "\n",
        "    Args:\n",
        "        tokens (list): Lista de tokens del corpus.\n",
        "        window_size (int): TamaÃ±o de la ventana de contexto.\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de tuplas (target, context) para entrenamiento.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for i in range(len(tokens)):\n",
        "        target = tokens[i]\n",
        "        context_indices = list(range(max(0, i - window_size), i)) + list(range(i + 1, min(len(tokens), i + window_size + 1)))\n",
        "        context = [tokens[idx] for idx in context_indices]\n",
        "        for context_word in context:\n",
        "            data.append((target, context_word))\n",
        "    return data\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Calcula la funciÃ³n sigmoide de un valor.\n",
        "\n",
        "    Args:\n",
        "        x (float or np.ndarray): Valor o array de entrada.\n",
        "\n",
        "    Returns:\n",
        "        float or np.ndarray: Valor o array con la funciÃ³n sigmoide aplicada.\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def get_negative_samples(target_idx, num_neg_samples, vocab_size):\n",
        "    \"\"\"\n",
        "    Genera muestras negativas aleatorias excluyendo la palabra objetivo.\n",
        "\n",
        "    Args:\n",
        "        target_idx (int): Ãndice de la palabra objetivo.\n",
        "        num_neg_samples (int): NÃºmero de muestras negativas a generar.\n",
        "        vocab_size (int): TamaÃ±o del vocabulario.\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de Ã­ndices de las muestras negativas.\n",
        "    \"\"\"\n",
        "    negative_samples = []\n",
        "    while len(negative_samples) < num_neg_samples:\n",
        "        neg_idx = np.random.randint(0, vocab_size)\n",
        "        if neg_idx != target_idx:\n",
        "            negative_samples.append(neg_idx)\n",
        "    return negative_samples\n",
        "\n",
        "def train_skipgram(training_data, word_to_idx, vocab_size, embedding_dim, learning_rate, epochs, num_neg_samples):\n",
        "    \"\"\"\n",
        "    Entrena el modelo Skip-Gram con muestreo negativo.\n",
        "\n",
        "    Args:\n",
        "        training_data (list): Datos de entrenamiento (target, context).\n",
        "        word_to_idx (dict): Diccionario que asigna Ã­ndices a palabras.\n",
        "        vocab_size (int): TamaÃ±o del vocabulario.\n",
        "        embedding_dim (int): DimensiÃ³n de los embeddings.\n",
        "        learning_rate (float): Tasa de aprendizaje.\n",
        "        epochs (int): NÃºmero de Ã©pocas de entrenamiento.\n",
        "        num_neg_samples (int): NÃºmero de muestras negativas.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (W1, W2) Matrices de pesos despuÃ©s del entrenamiento.\n",
        "    \"\"\"\n",
        "    # Inicializar pesos\n",
        "    W1 = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
        "    W2 = np.random.randn(embedding_dim, vocab_size) * 0.01\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        loss = 0\n",
        "        for target_word, context_word in training_data:\n",
        "            target_idx = word_to_idx[target_word]\n",
        "            context_idx = word_to_idx[context_word]\n",
        "\n",
        "            # Obtener muestras negativas\n",
        "            negative_indices = get_negative_samples(target_idx, num_neg_samples, vocab_size)\n",
        "\n",
        "            # Vectores para el target y el contexto\n",
        "            w_target = W1[target_idx]\n",
        "            w_context = W2[:, context_idx]\n",
        "\n",
        "            # Producto punto positivo\n",
        "            score = np.dot(w_target, w_context)\n",
        "            loss_pos = -np.log(sigmoid(score) + 1e-9)\n",
        "            grad_pos = sigmoid(score) - 1\n",
        "\n",
        "            # ActualizaciÃ³n de vectores para el par positivo\n",
        "            W1[target_idx] -= learning_rate * grad_pos * w_context\n",
        "            W2[:, context_idx] -= learning_rate * grad_pos * w_target\n",
        "            loss += loss_pos\n",
        "\n",
        "            # ActualizaciÃ³n de vectores para muestras negativas\n",
        "            for neg_idx in negative_indices:\n",
        "                w_negative = W2[:, neg_idx]\n",
        "                score_neg = np.dot(w_target, w_negative)\n",
        "                loss_neg = -np.log(sigmoid(-score_neg) + 1e-9)\n",
        "                grad_neg = (1 - sigmoid(-score_neg))\n",
        "\n",
        "                W1[target_idx] -= learning_rate * grad_neg * w_negative\n",
        "                W2[:, neg_idx] -= learning_rate * grad_neg * w_target\n",
        "                loss += loss_neg\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, PÃ©rdida: {loss / len(training_data)}\")\n",
        "    return W1, W2\n",
        "\n",
        "def measure_training_time_memory(func, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Mide el tiempo de ejecuciÃ³n y el uso de memoria de la funciÃ³n de entrenamiento.\n",
        "\n",
        "    Args:\n",
        "        func (callable): FunciÃ³n de entrenamiento a medir.\n",
        "        *args: Argumentos para la funciÃ³n.\n",
        "        **kwargs: Argumentos nombrados para la funciÃ³n.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (result, tiempo_ejecucion, current_mem, peak_mem)\n",
        "            - result: Resultado de la funciÃ³n de entrenamiento.\n",
        "            - tiempo_ejecucion (float): Tiempo de ejecuciÃ³n en segundos.\n",
        "            - current_mem (float): Uso de memoria actual en MB.\n",
        "            - peak_mem (float): Uso mÃ¡ximo de memoria en MB.\n",
        "    \"\"\"\n",
        "    tracemalloc.start()\n",
        "    start_time = time.time()\n",
        "    result = func(*args, **kwargs)\n",
        "    end_time = time.time()\n",
        "    current_mem, peak_mem = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "    tiempo_ejecucion = end_time - start_time\n",
        "    return result, tiempo_ejecucion, current_mem / 10**6, peak_mem / 10**6\n",
        "\n",
        "# ParÃ¡metros y EjecuciÃ³n del modelo Skip-Gram\n",
        "\n",
        "# Definir los parÃ¡metros\n",
        "embedding_dim = 50\n",
        "window_size = 2\n",
        "learning_rate = 0.01\n",
        "epochs = 50\n",
        "num_neg_samples = 5\n",
        "\n",
        "# Construir vocabulario\n",
        "vocab_size, word_to_idx, idx_to_word = build_vocabulary(final_tokens)\n",
        "\n",
        "# Generar datos de entrenamiento\n",
        "training_data_sg = generate_skipgram_data(final_tokens, window_size)\n",
        "print(f\"NÃºmero de muestras de entrenamiento: {len(training_data_sg)}\")\n",
        "\n",
        "# Entrenar y medir tiempo de ejecuciÃ³n\n",
        "(W1, W2), tiempo_ejecucion, current_mem, peak_mem = measure_training_time_memory(\n",
        "    train_skipgram, training_data_sg, word_to_idx, vocab_size, embedding_dim, learning_rate, epochs, num_neg_samples\n",
        ")\n",
        "\n",
        "\n",
        "# Resultados del tiempo de ejecuciÃ³n\n",
        "print(f\"Tiempo de entrenamiento Skip-Gram inicial: {tiempo_ejecucion} segundos\")\n",
        "print(f\"Uso de memoria CBOW con mejoras: Actual={current_mem} MB; Pico={peak_mem} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6epIEmxLZKXO"
      },
      "source": [
        "# ImplementaciÃ³n con mejoras de CBOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWGiV5bXZJ4m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from multiprocessing import Pool\n",
        "import multiprocessing\n",
        "import time\n",
        "import tracemalloc\n",
        "\n",
        "def build_vocabulary(tokens):\n",
        "    \"\"\"\n",
        "    Construye el vocabulario y mapea palabras a Ã­ndices y viceversa.\n",
        "\n",
        "    Args:\n",
        "        tokens (list): Lista de tokens del corpus.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (vocab_size, word_to_idx, idx_to_word)\n",
        "            - vocab_size (int): TamaÃ±o del vocabulario.\n",
        "            - word_to_idx (dict): Diccionario que asigna Ã­ndices a palabras.\n",
        "            - idx_to_word (dict): Diccionario que asigna palabras a Ã­ndices.\n",
        "    \"\"\"\n",
        "    vocab = set(tokens)\n",
        "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "    vocab_size = len(vocab)\n",
        "    return vocab_size, word_to_idx, idx_to_word\n",
        "\n",
        "def initialize_weights(vocab_size, embedding_dim):\n",
        "    \"\"\"\n",
        "    Inicializa las matrices de pesos utilizando inicializaciÃ³n Xavier/Glorot.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): TamaÃ±o del vocabulario.\n",
        "        embedding_dim (int): DimensiÃ³n de los embeddings.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (W_input, W_output) Matrices de pesos inicializadas.\n",
        "            - W_input (np.ndarray): Matriz de pesos de entrada.\n",
        "            - W_output (np.ndarray): Matriz de pesos de salida.\n",
        "    \"\"\"\n",
        "    limit = np.sqrt(6 / (vocab_size + embedding_dim))\n",
        "    W_input = np.random.uniform(-limit, limit, (vocab_size, embedding_dim))\n",
        "    W_output = np.random.uniform(-limit, limit, (vocab_size, embedding_dim))\n",
        "    return W_input, W_output\n",
        "\n",
        "def split_data(data, num_chunks):\n",
        "    \"\"\"\n",
        "    Divide los datos en partes para distribuir entre mÃºltiples procesos.\n",
        "\n",
        "    Args:\n",
        "        data (list): Lista de datos de entrenamiento.\n",
        "        num_chunks (int): NÃºmero de divisiones.\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de partes divididas.\n",
        "    \"\"\"\n",
        "    if num_chunks > len(data):\n",
        "        num_chunks = len(data)\n",
        "    return np.array_split(data, num_chunks)\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Calcula la funciÃ³n sigmoide.\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): Vector de entrada.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Vector con la funciÃ³n sigmoide aplicada.\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def get_negative_samples(target_idx, num_neg_samples, vocab_size):\n",
        "    \"\"\"\n",
        "    Genera muestras negativas aleatorias excluyendo la palabra objetivo.\n",
        "\n",
        "    Args:\n",
        "        target_idx (int): Ãndice de la palabra objetivo.\n",
        "        num_neg_samples (int): NÃºmero de muestras negativas a generar.\n",
        "        vocab_size (int): TamaÃ±o del vocabulario.\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de Ã­ndices de las muestras negativas.\n",
        "    \"\"\"\n",
        "    negative_samples = []\n",
        "    while len(negative_samples) < num_neg_samples:\n",
        "        neg_idx = np.random.randint(0, vocab_size)\n",
        "        if neg_idx != target_idx and neg_idx not in negative_samples:\n",
        "            negative_samples.append(neg_idx)\n",
        "    return negative_samples\n",
        "\n",
        "def train_cbow_process(args):\n",
        "    \"\"\"\n",
        "    Entrena una porciÃ³n de datos en el modelo CBOW utilizando aprendizaje paralelo.\n",
        "\n",
        "    Args:\n",
        "        args (tuple): Contiene los siguientes elementos:\n",
        "            - data_chunk (list): Parte de los datos de entrenamiento.\n",
        "            - word_to_idx (dict): Diccionario que asigna palabras a Ã­ndices.\n",
        "            - learning_rate (float): Tasa de aprendizaje.\n",
        "            - num_neg_samples (int): NÃºmero de muestras negativas.\n",
        "            - vocab_size (int): TamaÃ±o del vocabulario.\n",
        "            - embedding_dim (int): DimensiÃ³n de los embeddings.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (total_loss, W_input, W_output)\n",
        "            - total_loss (float): PÃ©rdida total para la porciÃ³n de datos.\n",
        "            - W_input (np.ndarray): Gradientes acumulados de pesos de entrada.\n",
        "            - W_output (np.ndarray): Gradientes acumulados de pesos de salida.\n",
        "    \"\"\"\n",
        "    data_chunk, word_to_idx, learning_rate, num_neg_samples, vocab_size, embedding_dim = args\n",
        "    W_input = np.zeros((vocab_size, embedding_dim))\n",
        "    W_output = np.zeros((vocab_size, embedding_dim))\n",
        "    total_loss = 0\n",
        "\n",
        "    for context_words, target_word in data_chunk:\n",
        "        context_indices = [word_to_idx[word] for word in context_words]\n",
        "        target_idx = word_to_idx[target_word]\n",
        "\n",
        "        v_input = np.mean(W_input[context_indices], axis=0)\n",
        "\n",
        "        neg_indices = get_negative_samples(target_idx, num_neg_samples, vocab_size)\n",
        "\n",
        "        indices = [target_idx] + neg_indices\n",
        "        u_output = W_output[indices]\n",
        "\n",
        "        scores = np.dot(u_output, v_input)\n",
        "        labels = np.array([1] + [0] * len(neg_indices))\n",
        "\n",
        "        sigmoid_scores = sigmoid(scores)\n",
        "\n",
        "        loss = -np.sum(np.log(sigmoid_scores + 1e-9) * labels + np.log(1 - sigmoid_scores + 1e-9) * (1 - labels))\n",
        "        total_loss += loss\n",
        "\n",
        "        grad = sigmoid_scores - labels\n",
        "        grad_W_output = np.outer(grad, v_input)\n",
        "        grad_v_input = np.dot(grad, u_output)\n",
        "\n",
        "        for idx in indices:\n",
        "            W_output[idx] -= learning_rate * grad_W_output[indices.index(idx)]\n",
        "\n",
        "        for idx in context_indices:\n",
        "            W_input[idx] -= learning_rate * grad_v_input / len(context_indices)\n",
        "\n",
        "    return total_loss, W_input, W_output\n",
        "\n",
        "def train_cbow_parallel(training_data, word_to_idx, W_input, W_output, learning_rate, num_neg_samples, vocab_size, embedding_dim, num_processes, epochs):\n",
        "    \"\"\"\n",
        "    Entrena el modelo CBOW en paralelo utilizando mÃºltiples procesos.\n",
        "\n",
        "    Args:\n",
        "        training_data (list): Datos de entrenamiento.\n",
        "        word_to_idx (dict): Diccionario que asigna palabras a Ã­ndices.\n",
        "        W_input (np.ndarray): Matriz de pesos de entrada.\n",
        "        W_output (np.ndarray): Matriz de pesos de salida.\n",
        "        learning_rate (float): Tasa de aprendizaje.\n",
        "        num_neg_samples (int): NÃºmero de muestras negativas.\n",
        "        vocab_size (int): TamaÃ±o del vocabulario.\n",
        "        embedding_dim (int): DimensiÃ³n de los embeddings.\n",
        "        num_processes (int): NÃºmero de procesos paralelos.\n",
        "        epochs (int): NÃºmero de Ã©pocas de entrenamiento.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (W_input, W_output) Matrices de pesos entrenadas.\n",
        "    \"\"\"\n",
        "    for epoch in range(epochs):\n",
        "        np.random.shuffle(training_data)\n",
        "        data_chunks = split_data(training_data, num_processes)\n",
        "        args = [\n",
        "            (\n",
        "                chunk,\n",
        "                word_to_idx,\n",
        "                learning_rate,\n",
        "                num_neg_samples,\n",
        "                vocab_size,\n",
        "                embedding_dim\n",
        "            )\n",
        "            for chunk in data_chunks\n",
        "        ]\n",
        "\n",
        "        with Pool(processes=num_processes) as pool:\n",
        "            results = pool.map(train_cbow_process, args)\n",
        "\n",
        "        total_loss = 0\n",
        "        grad_W_input = np.zeros_like(W_input)\n",
        "        grad_W_output = np.zeros_like(W_output)\n",
        "\n",
        "        for loss_chunk, W_input_chunk, W_output_chunk in results:\n",
        "            total_loss += loss_chunk\n",
        "            grad_W_input += W_input_chunk\n",
        "            grad_W_output += W_output_chunk\n",
        "\n",
        "        W_input -= grad_W_input\n",
        "        W_output -= grad_W_output\n",
        "\n",
        "        avg_loss = total_loss / len(training_data)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, PÃ©rdida Promedio: {avg_loss}\")\n",
        "\n",
        "    return W_input, W_output\n",
        "\n",
        "def measure_training_time_memory(func, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Mide el tiempo de ejecuciÃ³n y el uso de memoria de la funciÃ³n de entrenamiento.\n",
        "\n",
        "    Args:\n",
        "        func (callable): FunciÃ³n de entrenamiento a medir.\n",
        "        *args: Argumentos para la funciÃ³n.\n",
        "        **kwargs: Argumentos nombrados para la funciÃ³n.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (result, tiempo_ejecucion, current_mem, peak_mem)\n",
        "            - result: Resultado de la funciÃ³n de entrenamiento.\n",
        "            - tiempo_ejecucion (float): Tiempo de ejecuciÃ³n en segundos.\n",
        "            - current_mem (float): Uso de memoria actual en MB.\n",
        "            - peak_mem (float): Uso mÃ¡ximo de memoria en MB.\n",
        "    \"\"\"\n",
        "    tracemalloc.start()\n",
        "    start_time = time.time()\n",
        "    result = func(*args, **kwargs)\n",
        "    end_time = time.time()\n",
        "    current_mem, peak_mem = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "    tiempo_ejecucion = end_time - start_time\n",
        "    return result, tiempo_ejecucion, current_mem / 10**6, peak_mem / 10**6\n",
        "\n",
        "def generate_cbow_data(tokens, window_size):\n",
        "    \"\"\"\n",
        "    Genera datos para el modelo CBOW.\n",
        "\n",
        "    Args:\n",
        "        tokens (list): Lista de tokens del corpus.\n",
        "        window_size (int): TamaÃ±o de la ventana de contexto.\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de tuplas (context, target) donde context es el contexto\n",
        "        de palabras y target es la palabra objetivo.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for idx in range(window_size, len(tokens) - window_size):\n",
        "        context = tokens[idx - window_size:idx] + tokens[idx + 1:idx + window_size + 1]\n",
        "        target = tokens[idx]\n",
        "        data.append((context, target))\n",
        "    return data\n",
        "\n",
        "# ParÃ¡metros y EjecuciÃ³n del modelo CBOW paralelo\n",
        "\n",
        "embedding_dim = 50\n",
        "window_size = 2\n",
        "learning_rate = 0.01\n",
        "epochs = 5\n",
        "num_neg_samples = 5\n",
        "num_processes = multiprocessing.cpu_count()\n",
        "\n",
        "vocab_size, word_to_idx, idx_to_word = build_vocabulary(final_tokens)\n",
        "\n",
        "W_input, W_output = initialize_weights(vocab_size, embedding_dim)\n",
        "\n",
        "training_data = generate_cbow_data(final_tokens, window_size)\n",
        "\n",
        "(W_input, W_output), tiempo_ejecucion, current_mem, peak_mem = measure_training_time_memory(\n",
        "    train_cbow_parallel, training_data, word_to_idx, W_input, W_output, learning_rate, num_neg_samples, vocab_size, embedding_dim, num_processes, epochs\n",
        ")\n",
        "\n",
        "print(f\"Tiempo de ejecuciÃ³n de CBOW con mejoras: {tiempo_ejecucion} segundos\")\n",
        "print(f\"Uso de memoria CBOW con mejoras: Actual={current_mem} MB; Pico={peak_mem} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3G6Nc_uhaNBn"
      },
      "source": [
        "# ImplementaciÃ³n con mejoras de Skip-gram con negative sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rT6oVq4jmgw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from multiprocessing import Pool, cpu_count\n",
        "import time\n",
        "import tracemalloc\n",
        "\n",
        "def build_vocabulary(tokens):\n",
        "    vocab = set(tokens)\n",
        "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "    vocab_size = len(vocab)\n",
        "    return vocab_size, word_to_idx, idx_to_word\n",
        "\n",
        "def initialize_weights(vocab_size, embedding_dim):\n",
        "    limit = np.sqrt(6 / (vocab_size + embedding_dim))\n",
        "    W_input = np.random.uniform(-limit, limit, (vocab_size, embedding_dim))\n",
        "    W_output = np.random.uniform(-limit, limit, (vocab_size, embedding_dim))\n",
        "    return W_input, W_output\n",
        "\n",
        "def generate_skipgram_data(tokens, window_size):\n",
        "    data = []\n",
        "    for i in range(len(tokens)):\n",
        "        target = tokens[i]\n",
        "        context_indices = list(range(max(0, i - window_size), i)) + list(range(i + 1, min(len(tokens), i + window_size + 1)))\n",
        "        context = [tokens[idx] for idx in context_indices]\n",
        "        for context_word in context:\n",
        "            data.append((target, context_word))\n",
        "    return data\n",
        "\n",
        "def create_alias_table(word_counts, vocab_size, idx_to_word):\n",
        "    total_count = sum(word_counts.values())\n",
        "    probs = np.array([word_counts[idx_to_word[i]] ** 0.75 for i in range(vocab_size)])\n",
        "    probs /= np.sum(probs)\n",
        "\n",
        "    scaled_probs = probs * vocab_size\n",
        "    alias = np.zeros(vocab_size, dtype=np.int32)\n",
        "    prob = np.zeros(vocab_size)\n",
        "\n",
        "    small = [i for i, sp in enumerate(scaled_probs) if sp < 1.0]\n",
        "    large = [i for i, sp in enumerate(scaled_probs) if sp >= 1.0]\n",
        "\n",
        "    while small and large:\n",
        "        s = small.pop()\n",
        "        l = large.pop()\n",
        "        alias[s] = l\n",
        "        prob[s] = scaled_probs[s]\n",
        "        scaled_probs[l] = scaled_probs[l] - (1.0 - scaled_probs[s])\n",
        "\n",
        "        if scaled_probs[l] < 1.0:\n",
        "            small.append(l)\n",
        "        else:\n",
        "            large.append(l)\n",
        "\n",
        "    prob = np.clip(scaled_probs, 0, 1)\n",
        "    return prob, alias\n",
        "\n",
        "def alias_sample(prob_table, alias_table, num_samples):\n",
        "    K = len(prob_table)\n",
        "    kk = np.random.randint(0, K, size=num_samples)\n",
        "    rr = np.random.uniform(0, 1, size=num_samples)\n",
        "    return np.where(rr < prob_table[kk], kk, alias_table[kk])\n",
        "\n",
        "def split_data(data, num_processes):\n",
        "    chunk_size = len(data) // num_processes\n",
        "    return [data[i * chunk_size:(i + 1) * chunk_size] for i in range(num_processes)]\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def train_skipgram_process(args):\n",
        "    \"\"\"\n",
        "    Entrena una porciÃ³n de datos en el modelo Skip-Gram utilizando aprendizaje paralelo.\n",
        "\n",
        "    Args:\n",
        "        args (tuple): Tupla que contiene:\n",
        "            - data_chunk (list): Parte de los datos de entrenamiento.\n",
        "            - W_input (np.ndarray): Matriz de pesos de entrada.\n",
        "            - W_output (np.ndarray): Matriz de pesos de salida.\n",
        "            - word_to_idx (dict)\n",
        "            - prob_table (np.ndarray)\n",
        "            - alias_table (np.ndarray)\n",
        "            - num_neg_samples (int)\n",
        "            - learning_rate (float)\n",
        "    Returns:\n",
        "        tuple: (total_loss, W_input_update, W_output_update) PÃ©rdida total y matrices de pesos actualizadas.\n",
        "    \"\"\"\n",
        "    (data_chunk, W_input, W_output, word_to_idx, prob_table, alias_table, num_neg_samples, learning_rate) = args\n",
        "    total_loss = 0\n",
        "\n",
        "    # Inicializar actualizaciones locales\n",
        "    W_input_update = np.zeros_like(W_input)\n",
        "    W_output_update = np.zeros_like(W_output)\n",
        "\n",
        "    for target_word, context_word in data_chunk:\n",
        "        target_idx = word_to_idx[target_word]\n",
        "        context_idx = word_to_idx[context_word]\n",
        "\n",
        "        neg_indices = alias_sample(prob_table, alias_table, num_neg_samples)\n",
        "        neg_indices = [idx for idx in neg_indices if idx != context_idx]\n",
        "\n",
        "        indices = [context_idx] + neg_indices\n",
        "        u_output = W_output[indices]\n",
        "        v_input = W_input[target_idx]\n",
        "        scores = np.dot(u_output, v_input)\n",
        "        labels = np.array([1] + [0] * len(neg_indices))\n",
        "        sigmoid_scores = sigmoid(scores)\n",
        "\n",
        "        loss = -np.sum(np.log(sigmoid_scores + 1e-9) * labels + np.log(1 - sigmoid_scores + 1e-9) * (1 - labels))\n",
        "        total_loss += loss\n",
        "\n",
        "        grad = sigmoid_scores - labels\n",
        "        grad_W_output = np.outer(grad, v_input)\n",
        "        grad_v_input = np.dot(grad, u_output)\n",
        "\n",
        "        # Acumular gradientes\n",
        "        W_output_update[indices] -= learning_rate * grad_W_output\n",
        "        W_input_update[target_idx] -= learning_rate * grad_v_input\n",
        "\n",
        "    return total_loss, W_input_update, W_output_update\n",
        "\n",
        "def train_skipgram_parallel(training_data_sg, W_input, W_output, word_to_idx, prob_table, alias_table,\n",
        "                            num_neg_samples, learning_rate, num_processes, epochs):\n",
        "    \"\"\"\n",
        "    Entrena el modelo Skip-Gram utilizando aprendizaje paralelo.\n",
        "\n",
        "    Returns:\n",
        "        W_input, W_output, tiempo_ejecucion, current_mem, peak_mem\n",
        "    \"\"\"\n",
        "    # Medir tiempo y memoria\n",
        "    tracemalloc.start()\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Mezclar datos\n",
        "        np.random.shuffle(training_data_sg)\n",
        "        # Dividir datos\n",
        "        data_chunks = split_data(training_data_sg, num_processes)\n",
        "        # Preparar argumentos para cada proceso\n",
        "        args = [ (data_chunk, W_input, W_output, word_to_idx, prob_table, alias_table, num_neg_samples, learning_rate)\n",
        "                 for data_chunk in data_chunks ]\n",
        "\n",
        "        with Pool(processes=num_processes) as pool:\n",
        "            results = pool.map(train_skipgram_process, args)\n",
        "\n",
        "        # Combinar actualizaciones\n",
        "        total_loss = 0\n",
        "        W_input_update = np.zeros_like(W_input)\n",
        "        W_output_update = np.zeros_like(W_output)\n",
        "\n",
        "        for res in results:\n",
        "            loss, W_input_grad, W_output_grad = res\n",
        "            total_loss += loss\n",
        "            W_input_update += W_input_grad\n",
        "            W_output_update += W_output_grad\n",
        "\n",
        "        # Actualizar pesos\n",
        "        W_input += W_input_update\n",
        "        W_output += W_output_update\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    current_mem, peak_mem = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "    tiempo_ejecucion = end_time - start_time\n",
        "    return W_input, W_output, tiempo_ejecucion, current_mem / 10**6, peak_mem / 10**6\n",
        "\n",
        "# ParÃ¡metros y EjecuciÃ³n del modelo Skip-Gram con aprendizaje paralelo\n",
        "embedding_dim = 50\n",
        "window_size = 2\n",
        "learning_rate = 0.01\n",
        "epochs = 10  # Reducido para fines de ejemplo\n",
        "num_neg_samples = 5\n",
        "num_processes = cpu_count()\n",
        "\n",
        "# Construir vocabulario y datos\n",
        "vocab_size, word_to_idx, idx_to_word = build_vocabulary(final_tokens)\n",
        "W_input, W_output = initialize_weights(vocab_size, embedding_dim)\n",
        "training_data_sg = generate_skipgram_data(final_tokens, window_size)\n",
        "\n",
        "# Crear word_counts y tablas Alias\n",
        "word_counts = Counter(final_tokens)\n",
        "prob_table, alias_table = create_alias_table(word_counts, vocab_size, idx_to_word)\n",
        "\n",
        "# Entrenar y medir tiempo y memoria\n",
        "W_input, W_output, tiempo_ejecucion, current_mem, peak_mem = train_skipgram_parallel(\n",
        "    training_data_sg, W_input, W_output, word_to_idx, prob_table, alias_table,\n",
        "    num_neg_samples, learning_rate, num_processes, epochs\n",
        ")\n",
        "\n",
        "# Resultados de tiempo y memoria\n",
        "print(f\"Tiempo de ejecuciÃ³n de Skip-Gram con mejoras: {tiempo_ejecucion} segundos\")\n",
        "print(f\"Uso de memoria Skip-Gram con mejoras: Actual={current_mem} MB; Pico={peak_mem} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU_zGhSb6jgW"
      },
      "source": [
        "# GloVe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4us53dy8LRBm"
      },
      "source": [
        "## ImplementaciÃ³n inicial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dY6VWt7eCYQT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Descargando el artÃ­culo: Inteligencia artificial\n",
            "ArtÃ­culo 'Inteligencia artificial' descargado.\n",
            "Descargando el artÃ­culo: Aprendizaje automÃ¡tico\n",
            "ArtÃ­culo 'Aprendizaje automÃ¡tico' descargado.\n",
            "Descargando el artÃ­culo: BiotecnologÃ­a\n",
            "ArtÃ­culo 'BiotecnologÃ­a' descargado.\n",
            "Descargando el artÃ­culo: NanotecnologÃ­a\n",
            "ArtÃ­culo 'NanotecnologÃ­a' descargado.\n",
            "Descargando el artÃ­culo: GenÃ©tica\n",
            "ArtÃ­culo 'GenÃ©tica' descargado.\n",
            "Descargando el artÃ­culo: TecnologÃ­a de la informaciÃ³n\n",
            "ArtÃ­culo 'TecnologÃ­a de la informaciÃ³n' descargado.\n",
            "Descargando el artÃ­culo: RobÃ³tica\n",
            "ArtÃ­culo 'RobÃ³tica' descargado.\n",
            "Descargando el artÃ­culo: RevoluciÃ³n Industrial\n",
            "ArtÃ­culo 'RevoluciÃ³n Industrial' descargado.\n",
            "Descargando el artÃ­culo: Segunda Guerra Mundial\n",
            "ArtÃ­culo 'Segunda Guerra Mundial' descargado.\n",
            "Descargando el artÃ­culo: Edad Media\n",
            "ArtÃ­culo 'Edad Media' descargado.\n",
            "Descargando el artÃ­culo: Guerra FrÃ­a\n",
            "ArtÃ­culo 'Guerra FrÃ­a' descargado.\n",
            "Descargando el artÃ­culo: RevoluciÃ³n francesa\n",
            "ArtÃ­culo 'RevoluciÃ³n francesa' descargado.\n",
            "Descargando el artÃ­culo: Historia de la ciencia\n",
            "ArtÃ­culo 'Historia de la ciencia' descargado.\n",
            "Descargando el artÃ­culo: Renacimiento\n",
            "ArtÃ­culo 'Renacimiento' descargado.\n",
            "Descargando el artÃ­culo: Literatura en espaÃ±ol\n",
            "ArtÃ­culo 'Literatura en espaÃ±ol' descargado.\n",
            "Descargando el artÃ­culo: Pintura renacentista\n",
            "ArtÃ­culo 'Pintura renacentista' descargado.\n",
            "Descargando el artÃ­culo: MÃºsica clÃ¡sica\n",
            "ArtÃ­culo 'MÃºsica clÃ¡sica' descargado.\n",
            "Descargando el artÃ­culo: FilosofÃ­a griega\n",
            "ArtÃ­culo 'FilosofÃ­a griega' descargado.\n",
            "Descargando el artÃ­culo: Arquitectura moderna\n",
            "ArtÃ­culo 'Arquitectura moderna' descargado.\n",
            "Descargando el artÃ­culo: SociologÃ­a\n",
            "ArtÃ­culo 'SociologÃ­a' descargado.\n",
            "Descargando el artÃ­culo: AntropologÃ­a\n",
            "ArtÃ­culo 'AntropologÃ­a' descargado.\n",
            "Descargando el artÃ­culo: PsicologÃ­a\n",
            "ArtÃ­culo 'PsicologÃ­a' descargado.\n",
            "Descargando el artÃ­culo: Parques nacionales de EspaÃ±a\n",
            "ArtÃ­culo 'Parques nacionales de EspaÃ±a' descargado.\n",
            "Descargando el artÃ­culo: EconomÃ­a de mercado\n",
            "ArtÃ­culo 'EconomÃ­a de mercado' descargado.\n",
            "Descargando el artÃ­culo: GlobalizaciÃ³n\n",
            "ArtÃ­culo 'GlobalizaciÃ³n' descargado.\n",
            "Descargando el artÃ­culo: PolÃ­tica internacional\n",
            "ArtÃ­culo 'PolÃ­tica internacional' descargado.\n",
            "Descargando el artÃ­culo: CorrupciÃ³n polÃ­tica\n",
            "ArtÃ­culo 'CorrupciÃ³n polÃ­tica' descargado.\n",
            "Descargando el artÃ­culo: EconomÃ­a de AmÃ©rica Latina\n",
            "ArtÃ­culo 'EconomÃ­a de AmÃ©rica Latina' descargado.\n",
            "Descargando el artÃ­culo: NutriciÃ³n humana\n",
            "ArtÃ­culo 'NutriciÃ³n humana' descargado.\n",
            "Descargando el artÃ­culo: GenÃ³mica\n",
            "ArtÃ­culo 'GenÃ³mica' descargado.\n",
            "Descargando el artÃ­culo: EpidemiologÃ­a\n",
            "ArtÃ­culo 'EpidemiologÃ­a' descargado.\n",
            "Descargando el artÃ­culo: Neurociencia\n",
            "ArtÃ­culo 'Neurociencia' descargado.\n",
            "Descargando el artÃ­culo: PsicologÃ­a clÃ­nica\n",
            "ArtÃ­culo 'PsicologÃ­a clÃ­nica' descargado.\n",
            "Descargando el artÃ­culo: Medicina tradicional\n",
            "ArtÃ­culo 'Medicina tradicional' descargado.\n",
            "Descargando el artÃ­culo: Salud mental\n",
            "ArtÃ­culo 'Salud mental' descargado.\n",
            "Descargando el artÃ­culo: Juegos OlÃ­mpicos\n",
            "ArtÃ­culo 'Juegos OlÃ­mpicos' descargado.\n",
            "Descargando el artÃ­culo: Arquitectura gÃ³tica\n",
            "ArtÃ­culo 'Arquitectura gÃ³tica' descargado.\n",
            "Descargando el artÃ­culo: IngenierÃ­a estructural\n",
            "ArtÃ­culo 'IngenierÃ­a estructural' descargado.\n",
            "Todos los artÃ­culos han sido descargados y guardados en 'corpus.txt'.\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import json\n",
        "\n",
        "articulos = [\n",
        "    \"Inteligencia artificial\", \"Aprendizaje automÃ¡tico\", \"BiotecnologÃ­a\", \"NanotecnologÃ­a\", \"GenÃ©tica\",\n",
        "    \"TecnologÃ­a de la informaciÃ³n\", \"RobÃ³tica\", \"RevoluciÃ³n Industrial\", \"Segunda Guerra Mundial\", \"Edad Media\",\n",
        "    \"Guerra FrÃ­a\", \"RevoluciÃ³n francesa\", \"Historia de la ciencia\", \"Renacimiento\", \"Literatura en espaÃ±ol\",\n",
        "    \"Pintura renacentista\", \"MÃºsica clÃ¡sica\", \"FilosofÃ­a griega\", \"Arquitectura moderna\", \"SociologÃ­a\",\n",
        "    \"AntropologÃ­a\", \"PsicologÃ­a\", \"Parques nacionales de EspaÃ±a\", \"EconomÃ­a de mercado\", \"GlobalizaciÃ³n\",\n",
        "    \"PolÃ­tica internacional\", \"CorrupciÃ³n polÃ­tica\", \"EconomÃ­a de AmÃ©rica Latina\", \"NutriciÃ³n humana\",\n",
        "    \"GenÃ³mica\", \"EpidemiologÃ­a\", \"Neurociencia\", \"PsicologÃ­a clÃ­nica\", \"Medicina tradicional\", \"Salud mental\",\n",
        "    \"Juegos OlÃ­mpicos\", \"Arquitectura gÃ³tica\", \"IngenierÃ­a estructural\"\n",
        "]\n",
        "\n",
        "# URL de la API de Wikipedia para obtener el extracto en formato JSON\n",
        "base_url = 'https://es.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&explaintext&titles={}'\n",
        "\n",
        "# Archivo para guardar el corpus\n",
        "filename = 'corpus.txt'\n",
        "\n",
        "with open(filename, 'w', encoding='utf-8') as file:\n",
        "    for title in articulos:\n",
        "        \"\"\"\n",
        "        Descarga el contenido de artÃ­culos de Wikipedia y los guarda en un archivo de texto.\n",
        "\n",
        "        Args:\n",
        "            title (str): El tÃ­tulo del artÃ­culo de Wikipedia a descargar.\n",
        "\n",
        "        Funcionalidad:\n",
        "            - Codifica el tÃ­tulo del artÃ­culo para incluirlo en una URL de solicitud a la API de Wikipedia.\n",
        "            - Descarga el contenido del artÃ­culo en formato JSON y extrae el campo 'extract', que contiene el texto del artÃ­culo.\n",
        "            - Si el extracto no estÃ¡ vacÃ­o, lo escribe en el archivo especificado y confirma la descarga en consola.\n",
        "            - Si el extracto estÃ¡ vacÃ­o, indica que el artÃ­culo no se pudo descargar.\n",
        "\n",
        "        Output:\n",
        "            Archivo 'corpus.txt' con el contenido de todos los artÃ­culos descargados.\n",
        "        \"\"\"\n",
        "\n",
        "        # Codificar el tÃ­tulo para URL\n",
        "        encoded_title = urllib.request.quote(title)\n",
        "        url = base_url.format(encoded_title)\n",
        "        print(f\"Descargando el artÃ­culo: {title}\")\n",
        "        response = urllib.request.urlopen(url)\n",
        "        data = response.read().decode('utf-8')\n",
        "\n",
        "        # Extraer el texto del JSON\n",
        "        json_data = json.loads(data)\n",
        "        pages = json_data['query']['pages']\n",
        "        page = next(iter(pages.values()))\n",
        "        extract = page.get('extract', '')\n",
        "\n",
        "        # Escribir el texto en el archivo si el extracto no estÃ¡ vacÃ­o\n",
        "        if extract:\n",
        "            file.write(extract + '\\n')\n",
        "            print(f\"ArtÃ­culo '{title}' descargado.\")\n",
        "        else:\n",
        "            print(f\"No se pudo descargar el artÃ­culo '{title}'.\")\n",
        "\n",
        "print(f\"Todos los artÃ­culos han sido descargados y guardados en '{filename}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "M7vl-DxQDigc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iniciando el preprocesamiento del corpus\n",
            "TamaÃ±o del vocabulario final: 4888 palabras\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Convierte el texto en minÃºsculas, remueve la puntuaciÃ³n y lo tokeniza en palabras.\n",
        "\n",
        "    Args:\n",
        "        text (str): Texto a procesar.\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de palabras (tokens) extraÃ­das del texto.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "def stem(word):\n",
        "    \"\"\"\n",
        "    Aplica una reducciÃ³n simple de sufijos comunes en espaÃ±ol para el stemming.\n",
        "\n",
        "    Args:\n",
        "        word (str): Palabra a la que se le aplicarÃ¡ el stemming.\n",
        "\n",
        "    Returns:\n",
        "        str: La palabra sin sufijos comunes.\n",
        "    \"\"\"\n",
        "    suffixes = ['osos', 'osas', 'ismo', 'ismos', 'able', 'ables', 'ible', 'ibles',\n",
        "                'ente', 'entes', 'mente', 'aciones', 'imientos', 'amiento', 'iciÃ³n', 'adora', 'aciÃ³n', 'adoras', 'adores', 'ante',\n",
        "                'ancia', 'mente', 'idad', 'ivas', 'ivos', 'anza', 'icos', 'icas', 'ico', 'ica',\n",
        "                'oso', 'osa']\n",
        "    for suffix in suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            return word[:-len(suffix)]\n",
        "    return word\n",
        "\n",
        "stopwords = set([\n",
        "     'vuestro', 'vuestros', 'y', 'ya', 'yo', 'Ã©l', 'Ã©ramos','a', 'al', 'algo', 'algunas', 'algunos',\n",
        "     'ante', 'antes', 'como', 'con', 'contra', 'de', 'del', 'desde', 'donde', 'durante', 'e', 'el',\n",
        "     'ella', 'ellas', 'ellos', 'en', 'entre', 'era', 'erais', 'eran', 'eras', 'eres', 'es',\n",
        "     'esa', 'esas', 'ese', 'eso', 'esos', 'esta', 'estaba', 'estabais', 'estaban', 'estabas',\n",
        "    'vosotras', 'vosotros', 'vuestra', 'vuestras'\n",
        "])\n",
        "\n",
        "\n",
        "def preprocess_corpus(filename):\n",
        "    \"\"\"\n",
        "    Lee un archivo de texto, realiza tokenizaciÃ³n, stemming, remueve stopwords y filtra palabras raras.\n",
        "\n",
        "    Args:\n",
        "        filename (str): Ruta del archivo de texto a procesar.\n",
        "\n",
        "    Returns:\n",
        "        list: Lista de tokens procesados que cumplen con un umbral de frecuencia.\n",
        "    \"\"\"\n",
        "    print(\"Iniciando el preprocesamiento del corpus\")\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        word_counter = Counter()\n",
        "        final_tokens = []\n",
        "        for line in file:\n",
        "            tokens = tokenize(line)\n",
        "            stems = [stem(token) for token in tokens]\n",
        "            tokens_filtered = [word for word in stems if word not in stopwords]\n",
        "            word_counter.update(tokens_filtered)\n",
        "            final_tokens.extend(tokens_filtered)\n",
        "\n",
        "    # Filtrado de palabras raras\n",
        "    threshold = 5\n",
        "    frequent_words = {word for word, count in word_counter.items() if count >= threshold}\n",
        "    final_tokens = [word for word in final_tokens if word in frequent_words]\n",
        "\n",
        "    print(f\"TamaÃ±o del vocabulario final: {len(set(final_tokens))} palabras\")\n",
        "    return final_tokens\n",
        "\n",
        "final_tokens = preprocess_corpus('corpus.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OAuSF_cNJz64"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "def build_vocab(tokens):\n",
        "    \"\"\"\n",
        "    Construye el vocabulario a partir de una lista de tokens.\n",
        "\n",
        "    Args:\n",
        "        tokens (list): Lista de palabras tokenizadas del corpus.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Un conjunto de palabras Ãºnicas (vocabulario), un diccionario que asigna un ID a cada palabra (word_to_id),\n",
        "               y un diccionario inverso de IDs a palabras (id_to_word).\n",
        "    \"\"\"\n",
        "    vocab = set(tokens)\n",
        "    word_to_id = {word: idx for idx, word in enumerate(vocab)}\n",
        "    id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
        "    return vocab, word_to_id, id_to_word\n",
        "\n",
        "def build_cooccurrence_matrix(tokens, vocab, word_to_id, window_size=5):\n",
        "    \"\"\"\n",
        "    Construye una matriz de co-ocurrencia de palabras utilizando una ventana de contexto.\n",
        "\n",
        "    Args:\n",
        "        tokens (list): Lista de palabras tokenizadas.\n",
        "        vocab (set): Conjunto de palabras Ãºnicas en el corpus.\n",
        "        word_to_id (dict): Diccionario que asigna un ID Ãºnico a cada palabra del vocabulario.\n",
        "        window_size (int): TamaÃ±o de la ventana de contexto para contar co-ocurrencias.\n",
        "\n",
        "    Returns:\n",
        "        defaultdict: Matriz de co-ocurrencia en formato de diccionario, donde las claves son pares (word_id, context_id)\n",
        "                     y los valores son las frecuencias ponderadas de co-ocurrencia.\n",
        "    \"\"\"\n",
        "    cooccurrences = defaultdict(float)\n",
        "    for i, word in enumerate(tokens):\n",
        "        word_id = word_to_id[word]\n",
        "        start = max(0, i - window_size)\n",
        "        end = min(len(tokens), i + window_size + 1)\n",
        "        for j in range(start, end):\n",
        "            if i != j:\n",
        "                context_word = tokens[j]\n",
        "                context_id = word_to_id[context_word]\n",
        "                distance = abs(i - j)\n",
        "                weight = 1.0 / distance  # PonderaciÃ³n por distancia inversa\n",
        "                cooccurrences[(word_id, context_id)] += weight\n",
        "    return cooccurrences\n",
        "\n",
        "window_size = 5  # TamaÃ±o de la ventana de contexto\n",
        "\n",
        "# ConstrucciÃ³n del vocabulario\n",
        "vocab, word_to_id, id_to_word = build_vocab(final_tokens)\n",
        "\n",
        "# ConstrucciÃ³n de la matriz de co-ocurrencia\n",
        "cooccurrence= build_cooccurrence_matrix(final_tokens, vocab, word_to_id, window_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JpgqNTnEKbv4"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def initialize_embeddings(vocab_size, embedding_dim=50):\n",
        "    \"\"\"\n",
        "    Inicializa matrices de embeddings y vectores de sesgo para el modelo.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): NÃºmero total de palabras en el vocabulario.\n",
        "        embedding_dim (int, opcional): Dimensionalidad de los vectores de embeddings.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Contiene las matrices de embeddings (W y W_tilde) y los vectores de sesgo (b y b_tilde).\n",
        "               - W (np.array): Matriz de embeddings para las palabras del vocabulario.\n",
        "               - W_tilde (np.array): Matriz de embeddings para las palabras en el contexto.\n",
        "               - b (np.array): Vector de sesgo para las palabras del vocabulario.\n",
        "               - b_tilde (np.array): Vector de sesgo para las palabras en el contexto.\n",
        "    \"\"\"\n",
        "    W = np.random.uniform(-0.5, 0.5, (vocab_size, embedding_dim))\n",
        "    W_tilde = np.random.uniform(-0.5, 0.5, (vocab_size, embedding_dim))\n",
        "    b = np.zeros(vocab_size)\n",
        "    b_tilde = np.zeros(vocab_size)\n",
        "    return W, W_tilde, b, b_tilde\n",
        "\n",
        "\n",
        "embedding_dim = 50  # DimensiÃ³n de los embeddings\n",
        "\n",
        "# InicializaciÃ³n de embeddings y sesgos\n",
        "W, W_tilde, b, b_tilde = initialize_embeddings(len(vocab), embedding_dim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WA1jlJK3FM3i"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# HiperparÃ¡metros\n",
        "X_max = 100\n",
        "alpha = 0.75\n",
        "learning_rate = 0.05\n",
        "epochs = 100 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ITOsdQyjFP1-"
      },
      "outputs": [],
      "source": [
        "def weighting_function(x_ij):\n",
        "    \"\"\"\n",
        "    Calcula un peso basado en la frecuencia de co-ocurrencia entre palabras,\n",
        "    aplicando una ponderaciÃ³n mÃ¡xima para valores altos de frecuencia.\n",
        "\n",
        "    Args:\n",
        "        x_ij (float): Frecuencia de co-ocurrencia entre dos palabras.\n",
        "\n",
        "    Returns:\n",
        "        float: Peso ajustado en funciÃ³n de la frecuencia; es menor que 1 si\n",
        "               x_ij es menor que X_max y 1 en caso contrario.\n",
        "    \"\"\"\n",
        "    if x_ij < X_max:\n",
        "        return (x_ij / X_max) ** alpha\n",
        "    return 1.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pmLUl7eBLFkZ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import tracemalloc\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def train_embeddings(cooccurrences, W, W_tilde, b, b_tilde, epochs, learning_rate):\n",
        "    \"\"\"\n",
        "    Entrena los embeddings ajustando vectores de palabras y sesgos,\n",
        "    utilizando una funciÃ³n de ponderaciÃ³n en funciÃ³n de la frecuencia de co-ocurrencia.\n",
        "\n",
        "    Args:\n",
        "        cooccurrences (dict): Diccionario de co-ocurrencias con pares de palabras como claves y frecuencias como valores.\n",
        "        W (np.array): Matriz de embeddings de palabras.\n",
        "        W_tilde (np.array): Matriz de embeddings de contexto.\n",
        "        b (np.array): Vector de sesgos para las palabras.\n",
        "        b_tilde (np.array): Vector de sesgos para el contexto.\n",
        "        epochs (int): NÃºmero de iteraciones de entrenamiento.\n",
        "        learning_rate (float): Tasa de aprendizaje para la actualizaciÃ³n de los parÃ¡metros.\n",
        "    \"\"\"\n",
        "    cooccurrence_items = list(cooccurrences.items())\n",
        "    inicio = time.time()\n",
        "    tracemalloc.start()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_cost = 0\n",
        "        random.shuffle(cooccurrence_items)\n",
        "\n",
        "        for (i, j), x_ij in cooccurrence_items:\n",
        "            # Calcula el peso f(x_ij)\n",
        "            f_ij = weighting_function(x_ij)\n",
        "\n",
        "            # CÃ¡lculo del producto punto y el costo para el par de palabras\n",
        "            w_i = W[i]\n",
        "            w_j = W_tilde[j]\n",
        "            bi = b[i]\n",
        "            bj = b_tilde[j]\n",
        "\n",
        "            dot_product = np.dot(w_i, w_j)\n",
        "            log_x_ij = math.log(x_ij)\n",
        "\n",
        "            # Costo de error para cada par\n",
        "            error = dot_product + bi + bj - log_x_ij\n",
        "            cost = f_ij * np.power(error, 2)\n",
        "            total_cost += 0.5 * cost\n",
        "\n",
        "            # Calcular gradiente\n",
        "            grad = f_ij * error\n",
        "            # ActualizaciÃ³n de parÃ¡metros\n",
        "            W[i] -= learning_rate * grad * w_j\n",
        "            W_tilde[j] -= learning_rate * grad * w_i\n",
        "            b[i] -= learning_rate * grad\n",
        "            b_tilde[j] -= learning_rate * grad\n",
        "\n",
        "        average_cost = total_cost / len(cooccurrence_items)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Costo promedio: {average_cost}\")\n",
        "\n",
        "    fin = time.time()\n",
        "    current, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "\n",
        "    tiempo_ejecucion = fin - inicio\n",
        "    uso_memoria = peak / 10**6\n",
        "    print(f\"Tiempo total de ejecuciÃ³n: {tiempo_ejecucion} segundos\")\n",
        "    print(f\"Uso de memoria pico: {uso_memoria} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ke6GNoOTL_AP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100, Costo promedio: 0.01959478085628495\n",
            "Epoch 2/100, Costo promedio: 0.015277750936697995\n",
            "Epoch 3/100, Costo promedio: 0.013130331901944082\n",
            "Epoch 4/100, Costo promedio: 0.01173686852196658\n",
            "Epoch 5/100, Costo promedio: 0.010777448850583113\n",
            "Epoch 6/100, Costo promedio: 0.010068601194500822\n",
            "Epoch 7/100, Costo promedio: 0.009515412717139486\n",
            "Epoch 8/100, Costo promedio: 0.009074825120611512\n",
            "Epoch 9/100, Costo promedio: 0.008711276628052609\n",
            "Epoch 10/100, Costo promedio: 0.008401825756057324\n",
            "Epoch 11/100, Costo promedio: 0.008131627730565763\n",
            "Epoch 12/100, Costo promedio: 0.007894824078514167\n",
            "Epoch 13/100, Costo promedio: 0.00768236994657698\n",
            "Epoch 14/100, Costo promedio: 0.007493112190585492\n",
            "Epoch 15/100, Costo promedio: 0.007313902181939885\n",
            "Epoch 16/100, Costo promedio: 0.007151933590402577\n",
            "Epoch 17/100, Costo promedio: 0.0070050872181588635\n",
            "Epoch 18/100, Costo promedio: 0.006865083594179574\n",
            "Epoch 19/100, Costo promedio: 0.006736626340488856\n",
            "Epoch 20/100, Costo promedio: 0.006612259375701852\n",
            "Epoch 21/100, Costo promedio: 0.00649734832646753\n",
            "Epoch 22/100, Costo promedio: 0.006388195749342445\n",
            "Epoch 23/100, Costo promedio: 0.00628521883013055\n",
            "Epoch 24/100, Costo promedio: 0.006188116707640533\n",
            "Epoch 25/100, Costo promedio: 0.006091501801416796\n",
            "Epoch 26/100, Costo promedio: 0.0060049151384353175\n",
            "Epoch 27/100, Costo promedio: 0.0059201036288496515\n",
            "Epoch 28/100, Costo promedio: 0.005835599619374395\n",
            "Epoch 29/100, Costo promedio: 0.005758308265599935\n",
            "Epoch 30/100, Costo promedio: 0.0056836798574991364\n",
            "Epoch 31/100, Costo promedio: 0.005607951910016088\n",
            "Epoch 32/100, Costo promedio: 0.00553781694259304\n",
            "Epoch 33/100, Costo promedio: 0.005474135625980086\n",
            "Epoch 34/100, Costo promedio: 0.005405913262066156\n",
            "Epoch 35/100, Costo promedio: 0.005344483923565236\n",
            "Epoch 36/100, Costo promedio: 0.005283790894289462\n",
            "Epoch 37/100, Costo promedio: 0.005223597765065467\n",
            "Epoch 38/100, Costo promedio: 0.005166272754804746\n",
            "Epoch 39/100, Costo promedio: 0.005110113129620913\n",
            "Epoch 40/100, Costo promedio: 0.005058281927016528\n",
            "Epoch 41/100, Costo promedio: 0.005001992704619174\n",
            "Epoch 42/100, Costo promedio: 0.00495382386952118\n",
            "Epoch 43/100, Costo promedio: 0.0049056595174491\n",
            "Epoch 44/100, Costo promedio: 0.004857263544040072\n",
            "Epoch 45/100, Costo promedio: 0.004809375680045441\n",
            "Epoch 46/100, Costo promedio: 0.004765610512608936\n",
            "Epoch 47/100, Costo promedio: 0.004721015031779601\n",
            "Epoch 48/100, Costo promedio: 0.004678277439065487\n",
            "Epoch 49/100, Costo promedio: 0.004635279208738019\n",
            "Epoch 50/100, Costo promedio: 0.004596332617800761\n",
            "Epoch 51/100, Costo promedio: 0.004555670379562181\n",
            "Epoch 52/100, Costo promedio: 0.004516180751985817\n",
            "Epoch 53/100, Costo promedio: 0.004478700023423795\n",
            "Epoch 54/100, Costo promedio: 0.004442300931115147\n",
            "Epoch 55/100, Costo promedio: 0.004406112465281384\n",
            "Epoch 56/100, Costo promedio: 0.004369730939948138\n",
            "Epoch 57/100, Costo promedio: 0.0043343760612265456\n",
            "Epoch 58/100, Costo promedio: 0.004300889495633915\n",
            "Epoch 59/100, Costo promedio: 0.0042699447836071665\n",
            "Epoch 60/100, Costo promedio: 0.004236100123027184\n",
            "Epoch 61/100, Costo promedio: 0.0042030303021848535\n",
            "Epoch 62/100, Costo promedio: 0.004173574629933811\n",
            "Epoch 63/100, Costo promedio: 0.004142974423130118\n",
            "Epoch 64/100, Costo promedio: 0.004110021098903165\n",
            "Epoch 65/100, Costo promedio: 0.0040852113918486005\n",
            "Epoch 66/100, Costo promedio: 0.004056978919743715\n",
            "Epoch 67/100, Costo promedio: 0.004026993031269851\n",
            "Epoch 68/100, Costo promedio: 0.003998715738295528\n",
            "Epoch 69/100, Costo promedio: 0.00397434967930784\n",
            "Epoch 70/100, Costo promedio: 0.0039446912976117705\n",
            "Epoch 71/100, Costo promedio: 0.003920008845458232\n",
            "Epoch 72/100, Costo promedio: 0.0038964763134981565\n",
            "Epoch 73/100, Costo promedio: 0.003870589825351623\n",
            "Epoch 74/100, Costo promedio: 0.0038468927814222314\n",
            "Epoch 75/100, Costo promedio: 0.003821414829091157\n",
            "Epoch 76/100, Costo promedio: 0.0037971480388782446\n",
            "Epoch 77/100, Costo promedio: 0.003776436552584365\n",
            "Epoch 78/100, Costo promedio: 0.003753679143331174\n",
            "Epoch 79/100, Costo promedio: 0.0037312587856531453\n",
            "Epoch 80/100, Costo promedio: 0.0037092651007237425\n",
            "Epoch 81/100, Costo promedio: 0.00368734377643445\n",
            "Epoch 82/100, Costo promedio: 0.003667072247007625\n",
            "Epoch 83/100, Costo promedio: 0.003646406521619282\n",
            "Epoch 84/100, Costo promedio: 0.003625614299281573\n",
            "Epoch 85/100, Costo promedio: 0.0036060400686049265\n",
            "Epoch 86/100, Costo promedio: 0.003585517236316815\n",
            "Epoch 87/100, Costo promedio: 0.003565065658096188\n",
            "Epoch 88/100, Costo promedio: 0.003546686619440654\n",
            "Epoch 89/100, Costo promedio: 0.0035270319158931447\n",
            "Epoch 90/100, Costo promedio: 0.0035071691575885426\n",
            "Epoch 91/100, Costo promedio: 0.0034916654188086155\n",
            "Epoch 92/100, Costo promedio: 0.0034747323257895866\n",
            "Epoch 93/100, Costo promedio: 0.003454293741648521\n",
            "Epoch 94/100, Costo promedio: 0.0034358596637339687\n",
            "Epoch 95/100, Costo promedio: 0.0034232481977564885\n",
            "Epoch 96/100, Costo promedio: 0.003404269962849978\n",
            "Epoch 97/100, Costo promedio: 0.003386800993200849\n",
            "Epoch 98/100, Costo promedio: 0.003370242416208531\n",
            "Epoch 99/100, Costo promedio: 0.003355902630675994\n",
            "Epoch 100/100, Costo promedio: 0.003337323796176787\n",
            "Tiempo total de ejecuciÃ³n: 2011.3149185180664 segundos\n",
            "Uso de memoria pico: 0.025134 MB\n"
          ]
        }
      ],
      "source": [
        "train_embeddings(cooccurrence, W, W_tilde, b, b_tilde, epochs, learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he_ArqgMFx01"
      },
      "source": [
        "## Tareas de evaluaciÃ³n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0_SbPxojFxhC"
      },
      "outputs": [],
      "source": [
        "embeddings_glove_initial = W + W_tilde"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZzB5fRdUF4d3"
      },
      "outputs": [],
      "source": [
        "analogias = [\n",
        "    ('hombre', 'mujer', 'rey', 'reina')\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ROsQCln7F5Ub"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Todas las palabras de las analogÃ­as estÃ¡n en el vocabulario.\n"
          ]
        }
      ],
      "source": [
        "missing_words = set()\n",
        "for analogia in analogias:\n",
        "    for word in analogia:\n",
        "        if word not in word_to_id:\n",
        "            missing_words.add(word)\n",
        "\n",
        "if missing_words:\n",
        "    print(\"Las siguientes palabras no estÃ¡n en el vocabulario:\", missing_words)\n",
        "else:\n",
        "    print(\"Todas las palabras de las analogÃ­as estÃ¡n en el vocabulario.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oIHVDrTvGKOu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_analogies(analogies, embeddings, word_to_idx, idx_to_word, top_n=1):\n",
        "    correct = 0\n",
        "    total = len(analogies)\n",
        "\n",
        "    for word_a, word_b, word_c, word_d in analogies:\n",
        "        if word_a not in word_to_idx or word_b not in word_to_idx or word_c not in word_to_idx:\n",
        "            continue  # Saltar si alguna palabra no estÃ¡ en el vocabulario\n",
        "\n",
        "        vec_a = embeddings[word_to_idx[word_a]]\n",
        "        vec_b = embeddings[word_to_idx[word_b]]\n",
        "        vec_c = embeddings[word_to_idx[word_c]]\n",
        "\n",
        "        # Calcular el vector de analogÃ­a\n",
        "        analogy_vector = vec_b - vec_a + vec_c\n",
        "\n",
        "        # Calcular similitudes\n",
        "        similarities = embeddings @ analogy_vector\n",
        "        best_indices = np.argsort(-similarities)\n",
        "\n",
        "        # Excluir las palabras originales\n",
        "        best_indices = [idx for idx in best_indices if idx not in [word_to_idx[word_a], word_to_idx[word_b], word_to_idx[word_c]]]\n",
        "\n",
        "        predicted_word = idx_to_word[best_indices[0]]\n",
        "\n",
        "        if predicted_word == word_d:\n",
        "            correct += 1\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Exactitud en analogÃ­as: {accuracy * 100:.2f}% ({correct}/{total})\")\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "lqGFKgfzGR_u"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exactitud en analogÃ­as: 0.00% (0/1)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_analogies(analogias, embeddings_glove_initial, word_to_id, id_to_word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El valor de la exactitud de 0 en las analogÃ­as puede deberse a la falta de robustez de los corpus asÃ­ como al ajuste de hiperparÃ¡metros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBzMO4iyHWv4"
      },
      "source": [
        "# ImplementaciÃ³n mejorada de GloVe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xfbunpeSjCbJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "def build_vocab(tokens):\n",
        "    \"\"\"\n",
        "    Construye el vocabulario a partir de una lista de tokens.\n",
        "\n",
        "    Args:\n",
        "        tokens (list): Lista de palabras tokenizadas del corpus.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Un conjunto de palabras Ãºnicas (vocabulario), un diccionario que asigna un ID a cada palabra (word_to_id),\n",
        "               y un diccionario inverso de IDs a palabras (id_to_word).\n",
        "    \"\"\"\n",
        "    vocab = set(tokens)\n",
        "    word_to_id = {word: idx for idx, word in enumerate(vocab)}\n",
        "    id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
        "    return vocab, word_to_id, id_to_word\n",
        "\n",
        "def build_cooccurrence_matrix(tokens, vocab, word_to_id, window_size=5):\n",
        "    \"\"\"\n",
        "    Construye una matriz de co-ocurrencia de palabras utilizando una ventana de contexto.\n",
        "\n",
        "    Args:\n",
        "        tokens (list): Lista de palabras tokenizadas.\n",
        "        vocab (set): Conjunto de palabras Ãºnicas en el corpus.\n",
        "        word_to_id (dict): Diccionario que asigna un ID Ãºnico a cada palabra del vocabulario.\n",
        "        window_size (int): TamaÃ±o de la ventana de contexto para contar co-ocurrencias.\n",
        "\n",
        "    Returns:\n",
        "        defaultdict: Matriz de co-ocurrencia en formato de diccionario, donde las claves son pares (word_id, context_id)\n",
        "                     y los valores son las frecuencias ponderadas de co-ocurrencia.\n",
        "    \"\"\"\n",
        "    cooccurrences = defaultdict(float)\n",
        "    for i, word in enumerate(tokens):\n",
        "        word_id = word_to_id[word]\n",
        "        start = max(0, i - window_size)\n",
        "        end = min(len(tokens), i + window_size + 1)\n",
        "        for j in range(start, end):\n",
        "            if i != j:\n",
        "                context_word = tokens[j]\n",
        "                context_id = word_to_id[context_word]\n",
        "                distance = abs(i - j)\n",
        "                weight = 1.0 / distance  # PonderaciÃ³n por distancia inversa\n",
        "                cooccurrences[(word_id, context_id)] += weight\n",
        "    return cooccurrences\n",
        "\n",
        "window_size = 5  # TamaÃ±o de la ventana de contexto\n",
        "\n",
        "# ConstrucciÃ³n del vocabulario\n",
        "vocab, word_to_id, id_to_word = build_vocab(final_tokens)\n",
        "\n",
        "# ConstrucciÃ³n de la matriz de co-ocurrencia\n",
        "cooccurrence= build_cooccurrence_matrix(final_tokens, vocab, word_to_id, window_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "FGTwFaCsR9hQ"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import coo_matrix\n",
        "vocab_size = len(vocab)\n",
        "# Obtener listas de Ã­ndices y valores para construir la matriz dispersa\n",
        "rows = []\n",
        "cols = []\n",
        "data = []\n",
        "\n",
        "for (i, j), x_ij in cooccurrence.items():\n",
        "    rows.append(i)\n",
        "    cols.append(j)\n",
        "    data.append(x_ij)\n",
        "\n",
        "# Crear la matriz de co-ocurrencia dispersa\n",
        "X = coo_matrix((data, (rows, cols)), shape=(vocab_size, vocab_size), dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "bITxV2_DSH8M"
      },
      "outputs": [],
      "source": [
        "def precompute_weights_and_logs(X, weighting_function):\n",
        "    \"\"\"\n",
        "    Precomputa los valores ponderados y los logaritmos de las co-ocurrencias,\n",
        "    optimizando el proceso de entrenamiento.\n",
        "\n",
        "    Args:\n",
        "        X (scipy.sparse.coo_matrix): Matriz de co-ocurrencias en formato disperso.\n",
        "        weighting_function (function): FunciÃ³n para calcular el peso de cada co-ocurrencia.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Dos arreglos de numpy, `f_X_data` y `log_X_data`, que contienen los valores\n",
        "               ponderados y logarÃ­tmicos de las co-ocurrencias en `X`.\n",
        "               - f_X_data (np.array): Peso f(X_{ij}) de cada co-ocurrencia.\n",
        "               - log_X_data (np.array): Logaritmo log(X_{ij}) de cada co-ocurrencia.\n",
        "    \"\"\"\n",
        "    f_X_data = np.zeros_like(X.data)\n",
        "    log_X_data = np.zeros_like(X.data)\n",
        "\n",
        "    for idx in range(len(X.data)):\n",
        "        x_ij = X.data[idx]\n",
        "        f_X_data[idx] = weighting_function(x_ij)\n",
        "        log_X_data[idx] = np.log(x_ij)\n",
        "\n",
        "    return f_X_data, log_X_data\n",
        "\n",
        "# EjecuciÃ³n de precomputaciÃ³n\n",
        "f_X_data, log_X_data = precompute_weights_and_logs(X, weighting_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "9b5CVO_6Sh9H"
      },
      "outputs": [],
      "source": [
        "def initialize_vectors_and_biases(vocab_size, embedding_dim):\n",
        "    \"\"\"\n",
        "    Inicializa las matrices de embeddings y los vectores de sesgos para el modelo,\n",
        "    asignando valores aleatorios a los embeddings y ceros a los sesgos.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): NÃºmero de palabras en el vocabulario.\n",
        "        embedding_dim (int): Dimensionalidad de los vectores de embeddings.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Matrices de embeddings y vectores de sesgos inicializados.\n",
        "               - W (np.array): Matriz de embeddings para palabras, con valores aleatorios.\n",
        "               - W_tilde (np.array): Matriz de embeddings para contexto, con valores aleatorios.\n",
        "               - b (np.array): Vector de sesgo para palabras, inicializado en ceros.\n",
        "               - b_tilde (np.array): Vector de sesgo para contexto, inicializado en ceros.\n",
        "    \"\"\"\n",
        "    W = np.random.uniform(-0.5, 0.5, (vocab_size, embedding_dim)).astype(np.float32)\n",
        "    W_tilde = np.random.uniform(-0.5, 0.5, (vocab_size, embedding_dim)).astype(np.float32)\n",
        "    b = np.zeros(vocab_size, dtype=np.float32)\n",
        "    b_tilde = np.zeros(vocab_size, dtype=np.float32)\n",
        "\n",
        "    return W, W_tilde, b, b_tilde\n",
        "\n",
        "# EjecuciÃ³n de la inicializaciÃ³n\n",
        "W, W_tilde, b, b_tilde = initialize_vectors_and_biases(vocab_size, embedding_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "mZoJ_4KeSrKq"
      },
      "outputs": [],
      "source": [
        "def initialize_gradient_squares(vocab_size, embedding_dim):\n",
        "    \"\"\"\n",
        "    Inicializa los acumuladores de cuadrados de gradiente para las matrices de embeddings\n",
        "    y los vectores de sesgos, necesarios para optimizaciÃ³n adaptativa.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): NÃºmero de palabras en el vocabulario.\n",
        "        embedding_dim (int): Dimensionalidad de los vectores de embeddings.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Acumuladores de cuadrados de gradiente inicializados con unos.\n",
        "               - gradsq_W (np.array): Acumulador para los gradientes de embeddings de palabras.\n",
        "               - gradsq_W_tilde (np.array): Acumulador para los gradientes de embeddings de contexto.\n",
        "               - gradsq_b (np.array): Acumulador para los gradientes de sesgo de palabras.\n",
        "               - gradsq_b_tilde (np.array): Acumulador para los gradientes de sesgo de contexto.\n",
        "    \"\"\"\n",
        "    gradsq_W = np.ones((vocab_size, embedding_dim), dtype=np.float32)\n",
        "    gradsq_W_tilde = np.ones((vocab_size, embedding_dim), dtype=np.float32)\n",
        "    gradsq_b = np.ones(vocab_size, dtype=np.float32)\n",
        "    gradsq_b_tilde = np.ones(vocab_size, dtype=np.float32)\n",
        "\n",
        "    return gradsq_W, gradsq_W_tilde, gradsq_b, gradsq_b_tilde\n",
        "\n",
        "# EjecuciÃ³n de la inicializaciÃ³n de acumuladores de gradiente\n",
        "gradsq_W, gradsq_W_tilde, gradsq_b, gradsq_b_tilde = initialize_gradient_squares(vocab_size, embedding_dim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ASpOFEiYS3Y3"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import tracemalloc\n",
        "\n",
        "def train_model_with_batches(X, f_X_data, log_X_data, W, W_tilde, b, b_tilde, gradsq_W, gradsq_W_tilde, gradsq_b, gradsq_b_tilde, epochs, learning_rate, batch_size=2048):\n",
        "    \"\"\"\n",
        "    Entrena el modelo de embeddings en mÃºltiples Ã©pocas, dividiendo los datos en lotes para optimizar la actualizaciÃ³n\n",
        "    de parÃ¡metros. Utiliza AdaGrad para una optimizaciÃ³n adaptativa si los acumuladores de gradiente estÃ¡n definidos.\n",
        "\n",
        "    Args:\n",
        "        X (scipy.sparse.coo_matrix): Matriz dispersa de co-ocurrencias.\n",
        "        f_X_data (np.array): Pesos precomputados para cada co-ocurrencia.\n",
        "        log_X_data (np.array): Logaritmos precomputados de cada co-ocurrencia.\n",
        "        W (np.array): Matriz de embeddings de palabras.\n",
        "        W_tilde (np.array): Matriz de embeddings de contexto.\n",
        "        b (np.array): Vector de sesgos para palabras.\n",
        "        b_tilde (np.array): Vector de sesgos para contexto.\n",
        "        gradsq_W (np.array): Acumulador de gradientes de `W` para AdaGrad.\n",
        "        gradsq_W_tilde (np.array): Acumulador de gradientes de `W_tilde` para AdaGrad.\n",
        "        gradsq_b (np.array): Acumulador de gradientes de `b` para AdaGrad.\n",
        "        gradsq_b_tilde (np.array): Acumulador de gradientes de `b_tilde` para AdaGrad.\n",
        "        epochs (int): NÃºmero total de Ã©pocas para el entrenamiento.\n",
        "        learning_rate (float): Tasa de aprendizaje.\n",
        "        batch_size (int, opcional): TamaÃ±o de cada lote para la actualizaciÃ³n.\n",
        "    \"\"\"\n",
        "    inicio = time.time()\n",
        "    tracemalloc.start()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_cost = 0\n",
        "        permutation = np.random.permutation(len(X.data))\n",
        "        X_data_shuffled = X.data[permutation]\n",
        "        f_X_data_shuffled = f_X_data[permutation]\n",
        "        log_X_data_shuffled = log_X_data[permutation]\n",
        "        row_indices = X.row[permutation]\n",
        "        col_indices = X.col[permutation]\n",
        "\n",
        "        num_batches = int(np.ceil(len(X_data_shuffled) / batch_size))\n",
        "\n",
        "        for batch_idx in range(num_batches):\n",
        "            start = batch_idx * batch_size\n",
        "            end = min((batch_idx + 1) * batch_size, len(X_data_shuffled))\n",
        "\n",
        "            i_s = row_indices[start:end]\n",
        "            j_s = col_indices[start:end]\n",
        "            x_ijs = X_data_shuffled[start:end]\n",
        "            f_ijs = f_X_data_shuffled[start:end]\n",
        "            log_x_ijs = log_X_data_shuffled[start:end]\n",
        "\n",
        "            w_i = W[i_s]\n",
        "            w_j = W_tilde[j_s]\n",
        "            b_i = b[i_s]\n",
        "            b_j = b_tilde[j_s]\n",
        "\n",
        "            dot_products = np.sum(w_i * w_j, axis=1)\n",
        "            errors = (dot_products + b_i + b_j - log_x_ijs)\n",
        "            costs = f_ijs * errors ** 2\n",
        "            total_cost += 0.5 * np.sum(costs)\n",
        "\n",
        "            grad = f_ijs * errors\n",
        "\n",
        "            grad_w_i = grad[:, np.newaxis] * w_j\n",
        "            grad_w_j = grad[:, np.newaxis] * w_i\n",
        "\n",
        "            if gradsq_W is not None:\n",
        "                gradsq_W[i_s] += grad_w_i ** 2\n",
        "                gradsq_W_tilde[j_s] += grad_w_j ** 2\n",
        "                gradsq_b[i_s] += grad ** 2\n",
        "                gradsq_b_tilde[j_s] += grad ** 2\n",
        "\n",
        "                W[i_s] -= (learning_rate / np.sqrt(gradsq_W[i_s])) * grad_w_i\n",
        "                W_tilde[j_s] -= (learning_rate / np.sqrt(gradsq_W_tilde[j_s])) * grad_w_j\n",
        "                b[i_s] -= (learning_rate / np.sqrt(gradsq_b[i_s])) * grad\n",
        "                b_tilde[j_s] -= (learning_rate / np.sqrt(gradsq_b_tilde[j_s])) * grad\n",
        "            else:\n",
        "                W[i_s] -= learning_rate * grad_w_i\n",
        "                W_tilde[j_s] -= learning_rate * grad_w_j\n",
        "                b[i_s] -= learning_rate * grad\n",
        "                b_tilde[j_s] -= learning_rate * grad\n",
        "\n",
        "        average_cost = total_cost / len(X.data)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Costo promedio: {average_cost}\")\n",
        "\n",
        "    current, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "    fin = time.time()\n",
        "    tiempo_ejecucion = fin - inicio\n",
        "    uso_memoria = peak / 10**6\n",
        "    print(f\"Tiempo total de ejecuciÃ³n: {tiempo_ejecucion} segundos\")\n",
        "    print(f\"Uso de memoria pico: {uso_memoria} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "uta0yrL5jyHK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100, Costo promedio: 0.02945303899735189\n",
            "Epoch 2/100, Costo promedio: 0.02456833467961881\n",
            "Epoch 3/100, Costo promedio: 0.02178615036928054\n",
            "Epoch 4/100, Costo promedio: 0.019753966248337813\n",
            "Epoch 5/100, Costo promedio: 0.018119247065061544\n",
            "Epoch 6/100, Costo promedio: 0.016720278399914468\n",
            "Epoch 7/100, Costo promedio: 0.015510732433338783\n",
            "Epoch 8/100, Costo promedio: 0.014544588550946272\n",
            "Epoch 9/100, Costo promedio: 0.013661633414290605\n",
            "Epoch 10/100, Costo promedio: 0.012914699356060837\n",
            "Epoch 11/100, Costo promedio: 0.012259993639432278\n",
            "Epoch 12/100, Costo promedio: 0.011670634573155854\n",
            "Epoch 13/100, Costo promedio: 0.01114636960560641\n",
            "Epoch 14/100, Costo promedio: 0.01067463420399928\n",
            "Epoch 15/100, Costo promedio: 0.01026386065948475\n",
            "Epoch 16/100, Costo promedio: 0.009885646283670637\n",
            "Epoch 17/100, Costo promedio: 0.009554518201733786\n",
            "Epoch 18/100, Costo promedio: 0.009258546024394377\n",
            "Epoch 19/100, Costo promedio: 0.008972534924792594\n",
            "Epoch 20/100, Costo promedio: 0.008706704087350855\n",
            "Epoch 21/100, Costo promedio: 0.00848071172659279\n",
            "Epoch 22/100, Costo promedio: 0.008276553681904142\n",
            "Epoch 23/100, Costo promedio: 0.00808475535007957\n",
            "Epoch 24/100, Costo promedio: 0.007915252779822926\n",
            "Epoch 25/100, Costo promedio: 0.007751624228881405\n",
            "Epoch 26/100, Costo promedio: 0.007596926127788814\n",
            "Epoch 27/100, Costo promedio: 0.00745942792192184\n",
            "Epoch 28/100, Costo promedio: 0.007328789271298114\n",
            "Epoch 29/100, Costo promedio: 0.007207227408899134\n",
            "Epoch 30/100, Costo promedio: 0.007091942056425582\n",
            "Epoch 31/100, Costo promedio: 0.006986930419573796\n",
            "Epoch 32/100, Costo promedio: 0.006890537483205501\n",
            "Epoch 33/100, Costo promedio: 0.006794736584226047\n",
            "Epoch 34/100, Costo promedio: 0.0067050607645370115\n",
            "Epoch 35/100, Costo promedio: 0.006617987490653912\n",
            "Epoch 36/100, Costo promedio: 0.006538327101761853\n",
            "Epoch 37/100, Costo promedio: 0.006460614685057547\n",
            "Epoch 38/100, Costo promedio: 0.006386963611357634\n",
            "Epoch 39/100, Costo promedio: 0.006314494387866686\n",
            "Epoch 40/100, Costo promedio: 0.006244355554067572\n",
            "Epoch 41/100, Costo promedio: 0.0061785820984491206\n",
            "Epoch 42/100, Costo promedio: 0.006114433895864391\n",
            "Epoch 43/100, Costo promedio: 0.006054071640203229\n",
            "Epoch 44/100, Costo promedio: 0.005994288223574544\n",
            "Epoch 45/100, Costo promedio: 0.005936618593301319\n",
            "Epoch 46/100, Costo promedio: 0.005880747750235513\n",
            "Epoch 47/100, Costo promedio: 0.00582640022994871\n",
            "Epoch 48/100, Costo promedio: 0.00577461823942971\n",
            "Epoch 49/100, Costo promedio: 0.005724351946934093\n",
            "Epoch 50/100, Costo promedio: 0.005675444668878109\n",
            "Epoch 51/100, Costo promedio: 0.005628236222220435\n",
            "Epoch 52/100, Costo promedio: 0.005581256215029811\n",
            "Epoch 53/100, Costo promedio: 0.00553544329678594\n",
            "Epoch 54/100, Costo promedio: 0.0054922191101362035\n",
            "Epoch 55/100, Costo promedio: 0.005449022734412679\n",
            "Epoch 56/100, Costo promedio: 0.0054074548087905796\n",
            "Epoch 57/100, Costo promedio: 0.005366012458418232\n",
            "Epoch 58/100, Costo promedio: 0.005325021947435698\n",
            "Epoch 59/100, Costo promedio: 0.00528547766896469\n",
            "Epoch 60/100, Costo promedio: 0.005246261971544419\n",
            "Epoch 61/100, Costo promedio: 0.0052078567933175005\n",
            "Epoch 62/100, Costo promedio: 0.005173004859184243\n",
            "Epoch 63/100, Costo promedio: 0.005136846092907539\n",
            "Epoch 64/100, Costo promedio: 0.005101437210603147\n",
            "Epoch 65/100, Costo promedio: 0.005066841964215035\n",
            "Epoch 66/100, Costo promedio: 0.005033149273233885\n",
            "Epoch 67/100, Costo promedio: 0.005000441076970728\n",
            "Epoch 68/100, Costo promedio: 0.004967845488455857\n",
            "Epoch 69/100, Costo promedio: 0.004935124860928995\n",
            "Epoch 70/100, Costo promedio: 0.004904877977806532\n",
            "Epoch 71/100, Costo promedio: 0.0048743077749437485\n",
            "Epoch 72/100, Costo promedio: 0.004843457920271857\n",
            "Epoch 73/100, Costo promedio: 0.004814242555010996\n",
            "Epoch 74/100, Costo promedio: 0.004785265015377517\n",
            "Epoch 75/100, Costo promedio: 0.004757326527109954\n",
            "Epoch 76/100, Costo promedio: 0.00472909104692188\n",
            "Epoch 77/100, Costo promedio: 0.004701653085160776\n",
            "Epoch 78/100, Costo promedio: 0.004674941695571467\n",
            "Epoch 79/100, Costo promedio: 0.004648147122054198\n",
            "Epoch 80/100, Costo promedio: 0.004622423703275994\n",
            "Epoch 81/100, Costo promedio: 0.004596650471827175\n",
            "Epoch 82/100, Costo promedio: 0.004571996047386076\n",
            "Epoch 83/100, Costo promedio: 0.004547221246111314\n",
            "Epoch 84/100, Costo promedio: 0.00452273065666615\n",
            "Epoch 85/100, Costo promedio: 0.0044991362023168715\n",
            "Epoch 86/100, Costo promedio: 0.004475961267489622\n",
            "Epoch 87/100, Costo promedio: 0.004452382483098342\n",
            "Epoch 88/100, Costo promedio: 0.0044295445986064766\n",
            "Epoch 89/100, Costo promedio: 0.004407009440143644\n",
            "Epoch 90/100, Costo promedio: 0.004385488048981904\n",
            "Epoch 91/100, Costo promedio: 0.00436380869308261\n",
            "Epoch 92/100, Costo promedio: 0.004341962315662658\n",
            "Epoch 93/100, Costo promedio: 0.004321016642084305\n",
            "Epoch 94/100, Costo promedio: 0.004300367457090069\n",
            "Epoch 95/100, Costo promedio: 0.004278880381509457\n",
            "Epoch 96/100, Costo promedio: 0.0042586775404677615\n",
            "Epoch 97/100, Costo promedio: 0.0042382350613150545\n",
            "Epoch 98/100, Costo promedio: 0.004218892483244406\n",
            "Epoch 99/100, Costo promedio: 0.004199313440863186\n",
            "Epoch 100/100, Costo promedio: 0.004180667001926363\n",
            "Tiempo total de ejecuciÃ³n: 46.91589140892029 segundos\n",
            "Uso de memoria pico: 29.312214 MB\n"
          ]
        }
      ],
      "source": [
        "train_model_with_batches(X, f_X_data, log_X_data, W, W_tilde, b, b_tilde, gradsq_W, gradsq_W_tilde, gradsq_b, gradsq_b_tilde, epochs, learning_rate, batch_size=2048)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
